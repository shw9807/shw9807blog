{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shw9807/shw9807blog/blob/master/%EC%8B%A0%EC%9A%A9%EC%B9%B4%EB%93%9C_%EC%97%B0%EC%B2%B4%EC%98%88%EC%B8%A1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fAoYy8o7heHz"
      },
      "source": [
        "# \"[Colab] 신용카드 사용자 연체 예측\"\n",
        "\n",
        "- toc:true\n",
        "- branch: master\n",
        "- badges: true\n",
        "- author: shw9807\n",
        "- categories: [데이콘]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ytP8y8J8hI7Q",
        "outputId": "704c44f0-d324-4166-a60a-b347e9f3f46e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33mWARNING: Skipping typing as it is not installed.\u001b[0m\n",
            "Collecting pytorch_tabnet\n",
            "  Cloning https://github.com/dreamquark-ai/tabnet.git (to revision develop) to /tmp/pip-install-vk88_98j/pytorch-tabnet_c577cf56909f451bb57123369d3b06a6\n",
            "  Running command git clone -q https://github.com/dreamquark-ai/tabnet.git /tmp/pip-install-vk88_98j/pytorch-tabnet_c577cf56909f451bb57123369d3b06a6\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy<2.0,>=1.17 in /usr/local/lib/python3.7/dist-packages (from pytorch_tabnet) (1.19.5)\n",
            "Requirement already satisfied: tqdm<5.0,>=4.36 in /usr/local/lib/python3.7/dist-packages (from pytorch_tabnet) (4.62.3)\n",
            "Requirement already satisfied: torch<2.0,>=1.2 in /usr/local/lib/python3.7/dist-packages (from pytorch_tabnet) (1.10.0+cu111)\n",
            "Requirement already satisfied: scipy>1.4 in /usr/local/lib/python3.7/dist-packages (from pytorch_tabnet) (1.4.1)\n",
            "Requirement already satisfied: scikit_learn>0.21 in /usr/local/lib/python3.7/dist-packages (from pytorch_tabnet) (1.0.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit_learn>0.21->pytorch_tabnet) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit_learn>0.21->pytorch_tabnet) (1.1.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch<2.0,>=1.2->pytorch_tabnet) (3.10.0.2)\n",
            "Building wheels for collected packages: pytorch-tabnet\n",
            "  Building wheel for pytorch-tabnet (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pytorch-tabnet: filename=pytorch_tabnet-3.1.1-py3-none-any.whl size=40626 sha256=af83d1a6c0e84932cedc83c69a1c85784f8a4b338d964fd21f6b3cb160c9779d\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-83a06ce3/wheels/a6/8e/aa/6f5ef6a2e389c8b5f7ea1c74bbb03ece8773b03c2b8955c334\n",
            "Successfully built pytorch-tabnet\n",
            "Installing collected packages: pytorch-tabnet\n",
            "Successfully installed pytorch-tabnet-3.1.1\n"
          ]
        }
      ],
      "source": [
        "!pip uninstall -y typing # this should avoid  AttributeError: type object 'Callable' has no attribute '_abc_registry'\n",
        "!pip install  \"git+https://github.com/dreamquark-ai/tabnet.git@develop#egg=pytorch_tabnet\" --upgrade"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2mS8E-WjQN7"
      },
      "source": [
        "TabNet : 정형 데이터에 적합한 딥러닝 모델  \n",
        ">장점은 크게 4가지라고 함\n",
        "1. TabNet은 전처리 과정이 필요하지 않다.\n",
        "2. Decision step을 통해 feature selection을 진행한다.\n",
        "3. 위의 과정으로 인해 decision step별이나(local interpretability) 모델 전체의(global interpretability) feature importance를 수치화 할 수 있다.\n",
        "4. 무작위로 가려진 feature 값을 예측하는 unsupervised pretrain 단계를 적용하여 상당한 성능 향상을 보여준다.\n",
        "\n",
        "TabNet에 대한 자세한 설명: https://themore-dont-know.tistory.com/2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z0nvzJIFiNbl",
        "outputId": "8856af15-25f4-492d-ad0d-3fa854d3b467"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting catboost\n",
            "  Downloading catboost-1.0.4-cp37-none-manylinux1_x86_64.whl (76.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 76.1 MB 1.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: graphviz in /usr/local/lib/python3.7/dist-packages (from catboost) (0.10.1)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from catboost) (1.19.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from catboost) (1.15.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from catboost) (3.2.2)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.7/dist-packages (from catboost) (5.5.0)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.7/dist-packages (from catboost) (1.3.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from catboost) (1.4.1)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->catboost) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->catboost) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->catboost) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->catboost) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->catboost) (3.0.7)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from plotly->catboost) (8.0.1)\n",
            "Installing collected packages: catboost\n",
            "Successfully installed catboost-1.0.4\n",
            "Collecting optuna\n",
            "  Downloading optuna-2.10.0-py3-none-any.whl (308 kB)\n",
            "\u001b[K     |████████████████████████████████| 308 kB 7.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from optuna) (3.13)\n",
            "Collecting alembic\n",
            "  Downloading alembic-1.7.6-py3-none-any.whl (210 kB)\n",
            "\u001b[K     |████████████████████████████████| 210 kB 51.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from optuna) (4.62.3)\n",
            "Requirement already satisfied: scipy!=1.4.0 in /usr/local/lib/python3.7/dist-packages (from optuna) (1.4.1)\n",
            "Requirement already satisfied: sqlalchemy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from optuna) (1.4.31)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from optuna) (21.3)\n",
            "Collecting cmaes>=0.8.2\n",
            "  Downloading cmaes-0.8.2-py3-none-any.whl (15 kB)\n",
            "Collecting colorlog\n",
            "  Downloading colorlog-6.6.0-py2.py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from optuna) (1.19.5)\n",
            "Collecting cliff\n",
            "  Downloading cliff-3.10.0-py3-none-any.whl (80 kB)\n",
            "\u001b[K     |████████████████████████████████| 80 kB 9.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->optuna) (3.0.7)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from sqlalchemy>=1.1.0->optuna) (4.10.1)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.7/dist-packages (from sqlalchemy>=1.1.0->optuna) (1.1.2)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from alembic->optuna) (5.4.0)\n",
            "Collecting Mako\n",
            "  Downloading Mako-1.1.6-py2.py3-none-any.whl (75 kB)\n",
            "\u001b[K     |████████████████████████████████| 75 kB 4.7 MB/s \n",
            "\u001b[?25hCollecting pbr!=2.1.0,>=2.0.0\n",
            "  Downloading pbr-5.8.1-py2.py3-none-any.whl (113 kB)\n",
            "\u001b[K     |████████████████████████████████| 113 kB 56.1 MB/s \n",
            "\u001b[?25hCollecting cmd2>=1.0.0\n",
            "  Downloading cmd2-2.3.3-py3-none-any.whl (149 kB)\n",
            "\u001b[K     |████████████████████████████████| 149 kB 67.7 MB/s \n",
            "\u001b[?25hCollecting autopage>=0.4.0\n",
            "  Downloading autopage-0.5.0-py3-none-any.whl (29 kB)\n",
            "Collecting stevedore>=2.0.1\n",
            "  Downloading stevedore-3.5.0-py3-none-any.whl (49 kB)\n",
            "\u001b[K     |████████████████████████████████| 49 kB 6.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: PrettyTable>=0.7.2 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna) (3.0.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna) (3.10.0.2)\n",
            "Collecting pyperclip>=1.6\n",
            "  Downloading pyperclip-1.8.2.tar.gz (20 kB)\n",
            "Requirement already satisfied: wcwidth>=0.1.7 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna) (0.2.5)\n",
            "Requirement already satisfied: attrs>=16.3.0 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna) (21.4.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->sqlalchemy>=1.1.0->optuna) (3.7.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.7/dist-packages (from Mako->alembic->optuna) (2.0.1)\n",
            "Building wheels for collected packages: pyperclip\n",
            "  Building wheel for pyperclip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyperclip: filename=pyperclip-1.8.2-py3-none-any.whl size=11137 sha256=155c92cf927017f0697a47c617051424d4c7d9cb9bc502efdc02f27dd7fca2ab\n",
            "  Stored in directory: /root/.cache/pip/wheels/9f/18/84/8f69f8b08169c7bae2dde6bd7daf0c19fca8c8e500ee620a28\n",
            "Successfully built pyperclip\n",
            "Installing collected packages: pyperclip, pbr, stevedore, Mako, cmd2, autopage, colorlog, cmaes, cliff, alembic, optuna\n",
            "Successfully installed Mako-1.1.6 alembic-1.7.6 autopage-0.5.0 cliff-3.10.0 cmaes-0.8.2 cmd2-2.3.3 colorlog-6.6.0 optuna-2.10.0 pbr-5.8.1 pyperclip-1.8.2 stevedore-3.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install catboost\n",
        "!pip install optuna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wfwgtdtFh3s5"
      },
      "outputs": [],
      "source": [
        "from typing import Dict, Tuple, Union, List\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import catboost\n",
        "\n",
        "from catboost import CatBoostClassifier, Pool\n",
        "from lightgbm import LGBMClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from pytorch_tabnet.multitask import TabNetMultiTaskClassifier\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.ensemble import RandomForestClassifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q4a8D6AjnIq8"
      },
      "source": [
        "## LOAD DATASET"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zUwZyMmcnRZo"
      },
      "source": [
        "- catboost가 이 대회의 핵심 알고리즘이라고 생각하여 categorical한 변수를 만드는게 중요하다고 생각하여 따로 함수를 만듦\n",
        "- 다른 알고리즘들은 categorical한 변수보다 numeric한 변수 처리에 초점을 맞춤"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5KVpJFwpB90"
      },
      "source": [
        "index  \n",
        "gender: 성별  \n",
        "\n",
        "car: 차량 소유 여부  \n",
        "\n",
        "reality: 부동산 소유 여부  \n",
        "\n",
        "child_num: 자녀 수  \n",
        "\n",
        "income_total: 연간 소득  \n",
        "\n",
        "income_type: 소득 분류   \n",
        "['Commercial associate', 'Working', 'State servant', 'Pensioner', 'Student']  \n",
        "\n",
        "edu_type: 교육 수준  \n",
        "['Higher education' ,'Secondary / secondary special', 'Incomplete higher', 'Lower secondary', 'Academic degree']  \n",
        "\n",
        "family_type: 결혼 여부  \n",
        "['Married', 'Civil marriage', 'Separated', 'Single / not married', 'Widow']  \n",
        "\n",
        "house_type: 생활 방식  \n",
        "['Municipal apartment', 'House / apartment', 'With parents', 'Co-op apartment', 'Rented apartment', 'Office apartment']  \n",
        "\n",
        "DAYS_BIRTH: 출생일  \n",
        "데이터 수집 당시 (0)부터 역으로 셈, 즉, -1은 데이터 수집일 하루 전에 태어났음을 의미\n",
        "\n",
        "DAYS_EMPLOYED: 업무 시작일\n",
        "\t\t\t\t\t\t\t데이터 수집 당시 (0)부터 역으로 셈, 즉, -1은 데이터 수집일 하루 전부터 일을 시작함을 의미\n",
        "\t\t\t\t\t\t\t양수 값은 고용되지 않은 상태를 의미함\n",
        "\n",
        "FLAG_MOBIL: 핸드폰 소유 여부\n",
        "\n",
        "work_phone: 업무용 전화 소유 여부\n",
        "\n",
        "phone: 전화 소유 여부\n",
        "\n",
        "email: 이메일 소유 여부\n",
        "\n",
        "occyp_type: 직업 유형\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
        "\n",
        "family_size: 가족 규모\n",
        "\n",
        "begin_month: 신용카드 발급 월  \n",
        "데이터 수집 당시 (0)부터 역으로 셈, 즉, -1은 데이터 수집일 한 달 전에 신용카드를 발급함을 의미\n",
        "\n",
        "credit: 사용자의 신용카드 대금 연체를 기준으로 한 신용도  \n",
        "=> 낮을 수록 높은 신용의 신용카드 사용자를 의미함"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FmzpS8D2nUQL"
      },
      "outputs": [],
      "source": [
        "def category_income(data: pd.DataFrame) -> pd.DataFrame:\n",
        "    data[\"income_total\"] = data[\"income_total\"] / 10000\n",
        "    conditions = [\n",
        "        (data[\"income_total\"].le(18)),\n",
        "        (data[\"income_total\"].gt(18) & data[\"income_total\"].le(33)),\n",
        "        (data[\"income_total\"].gt(33) & data[\"income_total\"].le(49)),\n",
        "        (data[\"income_total\"].gt(49) & data[\"income_total\"].le(64)),\n",
        "        (data[\"income_total\"].gt(64) & data[\"income_total\"].le(80)),\n",
        "        (data[\"income_total\"].gt(80) & data[\"income_total\"].le(95)),\n",
        "        (data[\"income_total\"].gt(95) & data[\"income_total\"].le(111)),\n",
        "        (data[\"income_total\"].gt(111) & data[\"income_total\"].le(126)),\n",
        "        (data[\"income_total\"].gt(126) & data[\"income_total\"].le(142)),\n",
        "        (data[\"income_total\"].gt(142)),\n",
        "    ]\n",
        "    choices = [i for i in range(10)]\n",
        "\n",
        "    data[\"income_total\"] = np.select(conditions, choices)\n",
        "    return data\n",
        "\n",
        "\n",
        "def load_dataset() -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
        "    path = \"/content/drive/MyDrive/Colab Notebooks/신용카드 연체예측/\"\n",
        "    train = pd.read_csv(path + \"train.csv\")\n",
        "    train = train.drop([\"index\"], axis=1)\n",
        "    train.fillna(\"NAN\", inplace=True)\n",
        "\n",
        "    test = pd.read_csv(path + \"test.csv\")\n",
        "    test = test.drop([\"index\"], axis=1)\n",
        "    test.fillna(\"NAN\", inplace=True)\n",
        "\n",
        "    # absolute\n",
        "    train[\"DAYS_EMPLOYED\"] = train[\"DAYS_EMPLOYED\"].map(lambda x: 0 if x > 0 else x)\n",
        "    train[\"DAYS_EMPLOYED\"] = np.abs(train[\"DAYS_EMPLOYED\"])\n",
        "    test[\"DAYS_EMPLOYED\"] = test[\"DAYS_EMPLOYED\"].map(lambda x: 0 if x > 0 else x)\n",
        "    test[\"DAYS_EMPLOYED\"] = np.abs(test[\"DAYS_EMPLOYED\"])\n",
        "    train[\"DAYS_BIRTH\"] = np.abs(train[\"DAYS_BIRTH\"])\n",
        "    test[\"DAYS_BIRTH\"] = np.abs(test[\"DAYS_BIRTH\"])\n",
        "    train[\"begin_month\"] = np.abs(train[\"begin_month\"]).astype(int)\n",
        "    test[\"begin_month\"] = np.abs(test[\"begin_month\"]).astype(int)\n",
        "\n",
        "    # DAYS_BIRTH\n",
        "    train[\"DAYS_BIRTH_month\"] = np.floor(train[\"DAYS_BIRTH\"] / 30) - (\n",
        "        (np.floor(train[\"DAYS_BIRTH\"] / 30) / 12).astype(int) * 12\n",
        "    )\n",
        "    train[\"DAYS_BIRTH_month\"] = train[\"DAYS_BIRTH_month\"].astype(int)\n",
        "    train[\"DAYS_BIRTH_week\"] = np.floor(train[\"DAYS_BIRTH\"] / 7) - (\n",
        "        (np.floor(train[\"DAYS_BIRTH\"] / 7) / 4).astype(int) * 4\n",
        "    )\n",
        "    train[\"DAYS_BIRTH_week\"] = train[\"DAYS_BIRTH_week\"].astype(int)\n",
        "    test[\"DAYS_BIRTH_month\"] = np.floor(test[\"DAYS_BIRTH\"] / 30) - (\n",
        "        (np.floor(test[\"DAYS_BIRTH\"] / 30) / 12).astype(int) * 12\n",
        "    )\n",
        "    test[\"DAYS_BIRTH_month\"] = test[\"DAYS_BIRTH_month\"].astype(int)\n",
        "    test[\"DAYS_BIRTH_week\"] = np.floor(test[\"DAYS_BIRTH\"] / 7) - (\n",
        "        (np.floor(test[\"DAYS_BIRTH\"] / 7) / 4).astype(int) * 4\n",
        "    )\n",
        "    test[\"DAYS_BIRTH_week\"] = test[\"DAYS_BIRTH_week\"].astype(int)\n",
        "\n",
        "    # Age\n",
        "    train[\"Age\"] = np.abs(train[\"DAYS_BIRTH\"]) // 360\n",
        "    test[\"Age\"] = np.abs(test[\"DAYS_BIRTH\"]) // 360\n",
        "\n",
        "    # DAYS_EMPLOYED\n",
        "    train[\"DAYS_EMPLOYED_month\"] = np.floor(train[\"DAYS_EMPLOYED\"] / 30) - (\n",
        "        (np.floor(train[\"DAYS_EMPLOYED\"] / 30) / 12).astype(int) * 12\n",
        "    )\n",
        "    train[\"DAYS_EMPLOYED_month\"] = train[\"DAYS_EMPLOYED_month\"].astype(int)\n",
        "    train[\"DAYS_EMPLOYED_week\"] = np.floor(train[\"DAYS_EMPLOYED\"] / 7) - (\n",
        "        (np.floor(train[\"DAYS_EMPLOYED\"] / 7) / 4).astype(int) * 4\n",
        "    )\n",
        "    train[\"DAYS_EMPLOYED_week\"] = train[\"DAYS_EMPLOYED_week\"].astype(int)\n",
        "    test[\"DAYS_EMPLOYED_month\"] = np.floor(test[\"DAYS_EMPLOYED\"] / 30) - (\n",
        "        (np.floor(test[\"DAYS_EMPLOYED\"] / 30) / 12).astype(int) * 12\n",
        "    )\n",
        "    test[\"DAYS_EMPLOYED_month\"] = test[\"DAYS_EMPLOYED_month\"].astype(int)\n",
        "    test[\"DAYS_EMPLOYED_week\"] = np.floor(test[\"DAYS_EMPLOYED\"] / 7) - (\n",
        "        (np.floor(test[\"DAYS_EMPLOYED\"] / 7) / 4).astype(int) * 4\n",
        "    )\n",
        "    test[\"DAYS_EMPLOYED_week\"] = test[\"DAYS_EMPLOYED_week\"].astype(int)\n",
        "\n",
        "    # EMPLOYED\n",
        "    train[\"EMPLOYED\"] = train[\"DAYS_EMPLOYED\"] / 360\n",
        "    test[\"EMPLOYED\"] = test[\"DAYS_EMPLOYED\"] / 360\n",
        "\n",
        "    # before_EMPLOYED\n",
        "    train[\"before_EMPLOYED\"] = train[\"DAYS_BIRTH\"] - train[\"DAYS_EMPLOYED\"]\n",
        "    train[\"before_EMPLOYED_month\"] = np.floor(train[\"before_EMPLOYED\"] / 30) - (\n",
        "        (np.floor(train[\"before_EMPLOYED\"] / 30) / 12).astype(int) * 12\n",
        "    )\n",
        "    train[\"before_EMPLOYED_month\"] = train[\"before_EMPLOYED_month\"].astype(int)\n",
        "    train[\"before_EMPLOYED_week\"] = np.floor(train[\"before_EMPLOYED\"] / 7) - (\n",
        "        (np.floor(train[\"before_EMPLOYED\"] / 7) / 4).astype(int) * 4\n",
        "    )\n",
        "    train[\"before_EMPLOYED_week\"] = train[\"before_EMPLOYED_week\"].astype(int)\n",
        "    test[\"before_EMPLOYED\"] = test[\"DAYS_BIRTH\"] - test[\"DAYS_EMPLOYED\"]\n",
        "    test[\"before_EMPLOYED_month\"] = np.floor(test[\"before_EMPLOYED\"] / 30) - (\n",
        "        (np.floor(test[\"before_EMPLOYED\"] / 30) / 12).astype(int) * 12\n",
        "    )\n",
        "    test[\"before_EMPLOYED_month\"] = test[\"before_EMPLOYED_month\"].astype(int)\n",
        "    test[\"before_EMPLOYED_week\"] = np.floor(test[\"before_EMPLOYED\"] / 7) - (\n",
        "        (np.floor(test[\"before_EMPLOYED\"] / 7) / 4).astype(int) * 4\n",
        "    )\n",
        "    test[\"before_EMPLOYED_week\"] = test[\"before_EMPLOYED_week\"].astype(int)\n",
        "\n",
        "    # gender_car_reality\n",
        "    train[\"user_code\"] = (\n",
        "        train[\"gender\"].astype(str)\n",
        "        + \"_\"\n",
        "        + train[\"car\"].astype(str)\n",
        "        + \"_\"\n",
        "        + train[\"reality\"].astype(str)\n",
        "    )\n",
        "    test[\"user_code\"] = (\n",
        "        test[\"gender\"].astype(str)\n",
        "        + \"_\"\n",
        "        + test[\"car\"].astype(str)\n",
        "        + \"_\"\n",
        "        + test[\"reality\"].astype(str)\n",
        "    )\n",
        "\n",
        "    del_cols = [\n",
        "        \"gender\",\n",
        "        \"car\",\n",
        "        \"reality\",\n",
        "        \"email\",\n",
        "        \"child_num\",\n",
        "        \"DAYS_BIRTH\",\n",
        "        \"DAYS_EMPLOYED\",\n",
        "    ]\n",
        "    train.drop(train.loc[train[\"family_size\"] > 7, \"family_size\"].index, inplace=True)\n",
        "    train.drop(del_cols, axis=1, inplace=True)\n",
        "    test.drop(del_cols, axis=1, inplace=True)\n",
        "\n",
        "    cat_cols = [\n",
        "        \"income_type\",\n",
        "        \"edu_type\",\n",
        "        \"family_type\",\n",
        "        \"house_type\",\n",
        "        \"occyp_type\",\n",
        "        \"user_code\",\n",
        "    ]\n",
        "\n",
        "    for col in cat_cols:\n",
        "        label_encoder = LabelEncoder()\n",
        "        label_encoder = label_encoder.fit(train[col])\n",
        "        train[col] = label_encoder.transform(train[col])\n",
        "        test[col] = label_encoder.transform(test[col])\n",
        "\n",
        "    return train, test\n",
        "\n",
        "\n",
        "def cat_load_dataset() -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
        "    path = \"/content/drive/MyDrive/Colab Notebooks/신용카드 연체예측/\"\n",
        "    train = pd.read_csv(path + \"train.csv\")\n",
        "    train = train.drop([\"index\"], axis=1)\n",
        "    train.fillna(\"NAN\", inplace=True)\n",
        "\n",
        "    test = pd.read_csv(path + \"test.csv\")\n",
        "    test = test.drop([\"index\"], axis=1)\n",
        "    test.fillna(\"NAN\", inplace=True)\n",
        "\n",
        "    # absolute\n",
        "    train[\"DAYS_EMPLOYED\"] = train[\"DAYS_EMPLOYED\"].map(lambda x: 0 if x > 0 else x)\n",
        "    train[\"DAYS_EMPLOYED\"] = np.abs(train[\"DAYS_EMPLOYED\"])\n",
        "    test[\"DAYS_EMPLOYED\"] = test[\"DAYS_EMPLOYED\"].map(lambda x: 0 if x > 0 else x)\n",
        "    test[\"DAYS_EMPLOYED\"] = np.abs(test[\"DAYS_EMPLOYED\"])\n",
        "    train[\"DAYS_BIRTH\"] = np.abs(train[\"DAYS_BIRTH\"])\n",
        "    test[\"DAYS_BIRTH\"] = np.abs(test[\"DAYS_BIRTH\"])\n",
        "    train[\"begin_month\"] = np.abs(train[\"begin_month\"]).astype(int)\n",
        "    test[\"begin_month\"] = np.abs(test[\"begin_month\"]).astype(int)\n",
        "\n",
        "    # income_total\n",
        "    train = category_income(train)\n",
        "    test = category_income(test)\n",
        "\n",
        "    # DAYS_BIRTH\n",
        "    train[\"DAYS_BIRTH_month\"] = np.floor(train[\"DAYS_BIRTH\"] / 30) - (\n",
        "        (np.floor(train[\"DAYS_BIRTH\"] / 30) / 12).astype(int) * 12\n",
        "    )\n",
        "    train[\"DAYS_BIRTH_month\"] = train[\"DAYS_BIRTH_month\"].astype(int)\n",
        "    train[\"DAYS_BIRTH_week\"] = np.floor(train[\"DAYS_BIRTH\"] / 7) - (\n",
        "        (np.floor(train[\"DAYS_BIRTH\"] / 7) / 4).astype(int) * 4\n",
        "    )\n",
        "    train[\"DAYS_BIRTH_week\"] = train[\"DAYS_BIRTH_week\"].astype(int)\n",
        "    test[\"DAYS_BIRTH_month\"] = np.floor(test[\"DAYS_BIRTH\"] / 30) - (\n",
        "        (np.floor(test[\"DAYS_BIRTH\"] / 30) / 12).astype(int) * 12\n",
        "    )\n",
        "    test[\"DAYS_BIRTH_month\"] = test[\"DAYS_BIRTH_month\"].astype(int)\n",
        "    test[\"DAYS_BIRTH_week\"] = np.floor(test[\"DAYS_BIRTH\"] / 7) - (\n",
        "        (np.floor(test[\"DAYS_BIRTH\"] / 7) / 4).astype(int) * 4\n",
        "    )\n",
        "    test[\"DAYS_BIRTH_week\"] = test[\"DAYS_BIRTH_week\"].astype(int)\n",
        "\n",
        "    # Age\n",
        "    train[\"Age\"] = np.abs(train[\"DAYS_BIRTH\"]) // 360\n",
        "    test[\"Age\"] = np.abs(test[\"DAYS_BIRTH\"]) // 360\n",
        "\n",
        "    # DAYS_EMPLOYED\n",
        "    train[\"DAYS_EMPLOYED_month\"] = np.floor(train[\"DAYS_EMPLOYED\"] / 30) - (\n",
        "        (np.floor(train[\"DAYS_EMPLOYED\"] / 30) / 12).astype(int) * 12\n",
        "    )\n",
        "    train[\"DAYS_EMPLOYED_month\"] = train[\"DAYS_EMPLOYED_month\"].astype(int)\n",
        "    train[\"DAYS_EMPLOYED_week\"] = np.floor(train[\"DAYS_EMPLOYED\"] / 7) - (\n",
        "        (np.floor(train[\"DAYS_EMPLOYED\"] / 7) / 4).astype(int) * 4\n",
        "    )\n",
        "    train[\"DAYS_EMPLOYED_week\"] = train[\"DAYS_EMPLOYED_week\"].astype(int)\n",
        "    test[\"DAYS_EMPLOYED_month\"] = np.floor(test[\"DAYS_EMPLOYED\"] / 30) - (\n",
        "        (np.floor(test[\"DAYS_EMPLOYED\"] / 30) / 12).astype(int) * 12\n",
        "    )\n",
        "    test[\"DAYS_EMPLOYED_month\"] = test[\"DAYS_EMPLOYED_month\"].astype(int)\n",
        "    test[\"DAYS_EMPLOYED_week\"] = np.floor(test[\"DAYS_EMPLOYED\"] / 7) - (\n",
        "        (np.floor(test[\"DAYS_EMPLOYED\"] / 7) / 4).astype(int) * 4\n",
        "    )\n",
        "    test[\"DAYS_EMPLOYED_week\"] = test[\"DAYS_EMPLOYED_week\"].astype(int)\n",
        "\n",
        "    # EMPLOYED\n",
        "    train[\"EMPLOYED\"] = train[\"DAYS_EMPLOYED\"] / 360\n",
        "    test[\"EMPLOYED\"] = test[\"DAYS_EMPLOYED\"] / 360\n",
        "\n",
        "    # before_EMPLOYED\n",
        "    train[\"before_EMPLOYED\"] = train[\"DAYS_BIRTH\"] - train[\"DAYS_EMPLOYED\"]\n",
        "    train[\"before_EMPLOYED_month\"] = np.floor(train[\"before_EMPLOYED\"] / 30) - (\n",
        "        (np.floor(train[\"before_EMPLOYED\"] / 30) / 12).astype(int) * 12\n",
        "    )\n",
        "    train[\"before_EMPLOYED_month\"] = train[\"before_EMPLOYED_month\"].astype(int)\n",
        "    train[\"before_EMPLOYED_week\"] = np.floor(train[\"before_EMPLOYED\"] / 7) - (\n",
        "        (np.floor(train[\"before_EMPLOYED\"] / 7) / 4).astype(int) * 4\n",
        "    )\n",
        "    train[\"before_EMPLOYED_week\"] = train[\"before_EMPLOYED_week\"].astype(int)\n",
        "    test[\"before_EMPLOYED\"] = test[\"DAYS_BIRTH\"] - test[\"DAYS_EMPLOYED\"]\n",
        "    test[\"before_EMPLOYED_month\"] = np.floor(test[\"before_EMPLOYED\"] / 30) - (\n",
        "        (np.floor(test[\"before_EMPLOYED\"] / 30) / 12).astype(int) * 12\n",
        "    )\n",
        "    test[\"before_EMPLOYED_month\"] = test[\"before_EMPLOYED_month\"].astype(int)\n",
        "    test[\"before_EMPLOYED_week\"] = np.floor(test[\"before_EMPLOYED\"] / 7) - (\n",
        "        (np.floor(test[\"before_EMPLOYED\"] / 7) / 4).astype(int) * 4\n",
        "    )\n",
        "    test[\"before_EMPLOYED_week\"] = test[\"before_EMPLOYED_week\"].astype(int)\n",
        "\n",
        "    # gender_car_reality\n",
        "    train[\"user_code\"] = (\n",
        "        train[\"gender\"].astype(str)\n",
        "        + \"_\"\n",
        "        + train[\"car\"].astype(str)\n",
        "        + \"_\"\n",
        "        + train[\"reality\"].astype(str)\n",
        "    )\n",
        "    test[\"user_code\"] = (\n",
        "        test[\"gender\"].astype(str)\n",
        "        + \"_\"\n",
        "        + test[\"car\"].astype(str)\n",
        "        + \"_\"\n",
        "        + test[\"reality\"].astype(str)\n",
        "    )\n",
        "\n",
        "    del_cols = [\n",
        "        \"gender\",\n",
        "        \"car\",\n",
        "        \"reality\",\n",
        "        \"email\",\n",
        "        \"child_num\",\n",
        "        \"DAYS_BIRTH\",\n",
        "        \"DAYS_EMPLOYED\",\n",
        "    ]\n",
        "    train.drop(train.loc[train[\"family_size\"] > 7, \"family_size\"].index, inplace=True)\n",
        "    train.drop(del_cols, axis=1, inplace=True)\n",
        "    test.drop(del_cols, axis=1, inplace=True)\n",
        "\n",
        "    cat_cols = [\n",
        "        \"income_type\",\n",
        "        \"edu_type\",\n",
        "        \"family_type\",\n",
        "        \"house_type\",\n",
        "        \"occyp_type\",\n",
        "        \"user_code\",\n",
        "    ]\n",
        "\n",
        "    for col in cat_cols:\n",
        "        label_encoder = LabelEncoder()\n",
        "        label_encoder = label_encoder.fit(train[col])\n",
        "        train[col] = label_encoder.transform(train[col])\n",
        "        test[col] = label_encoder.transform(test[col])\n",
        "\n",
        "    return train, test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "usky5gQ3vXnn"
      },
      "source": [
        "## MODELS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qLymASbrvY-j"
      },
      "outputs": [],
      "source": [
        "# Catboost\n",
        "def stratified_kfold_cat(\n",
        "    params: Dict[str, Union[int, float, str, List[str]]],\n",
        "    n_fold: int,\n",
        "    X: pd.DataFrame,\n",
        "    y: pd.DataFrame,\n",
        "    X_test: pd.DataFrame,\n",
        ") -> Tuple[np.ndarray, np.ndarray]:\n",
        "    folds = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=42)\n",
        "    splits = folds.split(X, y)\n",
        "    cat_oof = np.zeros((X.shape[0], 3))\n",
        "    cat_preds = np.zeros((X_test.shape[0], 3))\n",
        "    cat_cols = [c for c in X.columns if X[c].dtypes == \"int64\"]\n",
        "\n",
        "    for fold, (train_idx, valid_idx) in enumerate(splits):\n",
        "        print(f\"============ Fold {fold} ============\\n\")\n",
        "        X_train, X_valid = X.iloc[train_idx], X.iloc[valid_idx]\n",
        "        y_train, y_valid = y.iloc[train_idx], y.iloc[valid_idx]\n",
        "        train_data = Pool(data=X_train, label=y_train, cat_features=cat_cols)\n",
        "        valid_data = Pool(data=X_valid, label=y_valid, cat_features=cat_cols)\n",
        "\n",
        "        model = CatBoostClassifier(**params)\n",
        "\n",
        "        model.fit(\n",
        "            train_data,\n",
        "            eval_set=valid_data,\n",
        "            early_stopping_rounds=100,\n",
        "            use_best_model=True,\n",
        "            verbose=100,\n",
        "        )\n",
        "\n",
        "        cat_oof[valid_idx] = model.predict_proba(X_valid)\n",
        "        cat_preds += model.predict_proba(X_test) / n_fold\n",
        "\n",
        "    log_score = log_loss(y, cat_oof)\n",
        "    print(f\"Log Loss Score: {log_score:.5f}\\n\")\n",
        "    return cat_oof, cat_preds\n",
        "\n",
        "\n",
        "# Light GBM\n",
        "def stratified_kfold_lgbm(\n",
        "    params: Dict[str, Union[int, float, str]],\n",
        "    n_fold: int,\n",
        "    X: pd.DataFrame,\n",
        "    y: pd.DataFrame,\n",
        "    X_test: pd.DataFrame,\n",
        ") -> Tuple[np.ndarray, np.ndarray]:\n",
        "    folds = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=42)\n",
        "    splits = folds.split(X, y)\n",
        "    lgb_oof = np.zeros((X.shape[0], 3))\n",
        "    lgb_preds = np.zeros((X_test.shape[0], 3))\n",
        "\n",
        "    for fold, (train_idx, valid_idx) in enumerate(splits):\n",
        "        print(f\"============ Fold {fold} ============\\n\")\n",
        "        X_train, X_valid = X.iloc[train_idx], X.iloc[valid_idx]\n",
        "        y_train, y_valid = y.iloc[train_idx], y.iloc[valid_idx]\n",
        "        pre_model = LGBMClassifier(**params)\n",
        "\n",
        "        pre_model.fit(\n",
        "            X_train,\n",
        "            y_train,\n",
        "            eval_set=[(X_train, y_train), (X_valid, y_valid)],\n",
        "            early_stopping_rounds=100,\n",
        "            verbose=100,\n",
        "        )\n",
        "        params2 = params.copy()\n",
        "        params2[\"learning_rate\"] = params[\"learning_rate\"] * 0.1\n",
        "\n",
        "        model = LGBMClassifier(**params2)\n",
        "        model.fit(\n",
        "            X_train,\n",
        "            y_train,\n",
        "            eval_set=[(X_train, y_train), (X_valid, y_valid)],\n",
        "            early_stopping_rounds=100,\n",
        "            verbose=100,\n",
        "            init_model=pre_model,\n",
        "        )\n",
        "        lgb_oof[valid_idx] = model.predict_proba(X_valid)\n",
        "        lgb_preds += model.predict_proba(X_test) / n_fold\n",
        "\n",
        "    log_score = log_loss(y, lgb_oof)\n",
        "    print(f\"Log Loss Score: {log_score:.5f}\")\n",
        "\n",
        "    return lgb_oof, lgb_preds\n",
        "\n",
        "\n",
        "# XGB\n",
        "def stratified_kfold_xgb(\n",
        "    params: Dict[str, Union[int, float, str]],\n",
        "    n_fold: int,\n",
        "    X: pd.DataFrame,\n",
        "    y: pd.DataFrame,\n",
        "    X_test: pd.DataFrame,\n",
        ") -> Tuple[np.ndarray, np.ndarray]:\n",
        "\n",
        "    folds = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=42)\n",
        "    splits = folds.split(X, y)\n",
        "    xgb_oof = np.zeros((X.shape[0], 3))\n",
        "    xgb_preds = np.zeros((X_test.shape[0], 3))\n",
        "\n",
        "    for fold, (train_idx, valid_idx) in enumerate(splits):\n",
        "        print(f\"============ Fold {fold} ============\\n\")\n",
        "        X_train, X_valid = X.iloc[train_idx], X.iloc[valid_idx]\n",
        "        y_train, y_valid = y.iloc[train_idx], y.iloc[valid_idx]\n",
        "\n",
        "        model = XGBClassifier(**params)\n",
        "        model.fit(\n",
        "            X_train,\n",
        "            y_train,\n",
        "            eval_set=[(X_train, y_train), (X_valid, y_valid)],\n",
        "            early_stopping_rounds=100,\n",
        "            verbose=100,\n",
        "        )\n",
        "\n",
        "        xgb_oof[valid_idx] = model.predict_proba(X_valid)\n",
        "        xgb_preds += model.predict_proba(X_test) / n_fold\n",
        "\n",
        "    log_score = log_loss(y, xgb_oof)\n",
        "    print(f\"Log Loss Score: {log_score:.5f}\")\n",
        "\n",
        "    return xgb_oof, xgb_preds\n",
        "\n",
        "\n",
        "# Random Foreset\n",
        "def stratified_kfold_rf(\n",
        "    params: Dict[str, Union[int, float, str, bool]],\n",
        "    n_fold: int,\n",
        "    X: pd.DataFrame,\n",
        "    y: pd.DataFrame,\n",
        "    X_test: pd.DataFrame,\n",
        ") -> Tuple[np.ndarray, np.ndarray]:\n",
        "\n",
        "    folds = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=42)\n",
        "    splits = folds.split(X, y)\n",
        "    rf_oof = np.zeros((X.shape[0], 3))\n",
        "    rf_preds = np.zeros((X_test.shape[0], 3))\n",
        "\n",
        "    for fold, (train_idx, valid_idx) in enumerate(splits):\n",
        "        print(f\"============ Fold {fold} ============\\n\")\n",
        "        X_train, X_valid = X.iloc[train_idx], X.iloc[valid_idx]\n",
        "        y_train, y_valid = y.iloc[train_idx], y.iloc[valid_idx]\n",
        "        model = RandomForestClassifier(**params)\n",
        "        model.fit(\n",
        "            X_train,\n",
        "            y_train,\n",
        "        )\n",
        "\n",
        "        rf_oof[valid_idx] = model.predict_proba(X_valid)\n",
        "        rf_preds += model.predict_proba(X_test) / n_fold\n",
        "        print(f\"Log Loss Score: {log_loss(y_valid, rf_oof[valid_idx]):.5f}\")\n",
        "\n",
        "    log_score = log_loss(y, rf_oof)\n",
        "    print(f\"Log Loss Score: {log_score:.5f}\")\n",
        "\n",
        "    return rf_oof, rf_preds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SV5QcElPwrhW"
      },
      "source": [
        "## CAT TRAIN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WyMqVjqxwvsS"
      },
      "source": [
        "- catboost 같은 경우 categorical한 feature를 고정시켜주는게 핵심이라고 생각하여 하이퍼파라미터튜닝에 신경을 씀\n",
        "- 하이퍼파라미터튜닝은 optuna 라이브러리로 했음"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sBxITPJwwswu"
      },
      "outputs": [],
      "source": [
        "train_cat, test_cat = cat_load_dataset()\n",
        "X = train_cat.drop(\"credit\", axis=1)\n",
        "y = train_cat[\"credit\"]\n",
        "\n",
        "X_test = test_cat.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "xkY2QWDtyf0_",
        "outputId": "4fc9eeb8-a0d7-461a-8b91-be82e87e74e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============ Fold 0 ============\n",
            "\n",
            "0:\tlearn: 1.0832640\ttest: 1.0829576\tbest: 1.0829576 (0)\ttotal: 612ms\tremaining: 1h 42m 1s\n",
            "100:\tlearn: 0.7323907\ttest: 0.6929501\tbest: 0.6929501 (100)\ttotal: 44.8s\tremaining: 1h 13m 10s\n",
            "200:\tlearn: 0.7058060\ttest: 0.6735975\tbest: 0.6735975 (200)\ttotal: 1m 43s\tremaining: 1h 23m 43s\n",
            "300:\tlearn: 0.6885144\ttest: 0.6706689\tbest: 0.6706689 (300)\ttotal: 2m 45s\tremaining: 1h 28m 45s\n",
            "400:\tlearn: 0.6701975\ttest: 0.6682080\tbest: 0.6681988 (388)\ttotal: 3m 47s\tremaining: 1h 30m 43s\n",
            "500:\tlearn: 0.6494751\ttest: 0.6664689\tbest: 0.6662605 (486)\ttotal: 4m 50s\tremaining: 1h 31m 54s\n",
            "600:\tlearn: 0.6290599\ttest: 0.6657848\tbest: 0.6657848 (600)\ttotal: 5m 54s\tremaining: 1h 32m 16s\n",
            "700:\tlearn: 0.6080674\ttest: 0.6648291\tbest: 0.6648201 (698)\ttotal: 6m 57s\tremaining: 1h 32m 14s\n",
            "800:\tlearn: 0.5895904\ttest: 0.6646412\tbest: 0.6643652 (761)\ttotal: 8m 1s\tremaining: 1h 32m 7s\n",
            "Stopped by overfitting detector  (100 iterations wait)\n",
            "\n",
            "bestTest = 0.6643651535\n",
            "bestIteration = 761\n",
            "\n",
            "Shrink model to first 762 iterations.\n",
            "============ Fold 1 ============\n",
            "\n",
            "0:\tlearn: 1.0831624\ttest: 1.0831592\tbest: 1.0831592 (0)\ttotal: 300ms\tremaining: 50m 1s\n",
            "100:\tlearn: 0.7348941\ttest: 0.6976274\tbest: 0.6976274 (100)\ttotal: 43.6s\tremaining: 1h 11m 13s\n",
            "200:\tlearn: 0.7091219\ttest: 0.6726237\tbest: 0.6726237 (200)\ttotal: 1m 41s\tremaining: 1h 22m 48s\n",
            "300:\tlearn: 0.6922515\ttest: 0.6687022\tbest: 0.6687022 (300)\ttotal: 2m 42s\tremaining: 1h 27m 30s\n",
            "400:\tlearn: 0.6724169\ttest: 0.6655868\tbest: 0.6655868 (400)\ttotal: 3m 46s\tremaining: 1h 30m 15s\n",
            "500:\tlearn: 0.6510442\ttest: 0.6630204\tbest: 0.6630204 (500)\ttotal: 4m 50s\tremaining: 1h 31m 49s\n",
            "600:\tlearn: 0.6316988\ttest: 0.6614526\tbest: 0.6614526 (600)\ttotal: 5m 53s\tremaining: 1h 32m 6s\n",
            "700:\tlearn: 0.6129803\ttest: 0.6607424\tbest: 0.6606660 (687)\ttotal: 6m 57s\tremaining: 1h 32m 12s\n",
            "800:\tlearn: 0.5930475\ttest: 0.6602049\tbest: 0.6601429 (793)\ttotal: 8m 1s\tremaining: 1h 32m 10s\n",
            "900:\tlearn: 0.5758447\ttest: 0.6597748\tbest: 0.6596636 (885)\ttotal: 9m 5s\tremaining: 1h 31m 47s\n",
            "Stopped by overfitting detector  (100 iterations wait)\n",
            "\n",
            "bestTest = 0.6596635705\n",
            "bestIteration = 885\n",
            "\n",
            "Shrink model to first 886 iterations.\n",
            "============ Fold 2 ============\n",
            "\n",
            "0:\tlearn: 1.0831179\ttest: 1.0832346\tbest: 1.0832346 (0)\ttotal: 296ms\tremaining: 49m 16s\n",
            "100:\tlearn: 0.7276329\ttest: 0.6945366\tbest: 0.6945366 (100)\ttotal: 46.9s\tremaining: 1h 16m 32s\n",
            "200:\tlearn: 0.7020572\ttest: 0.6763344\tbest: 0.6763344 (200)\ttotal: 1m 49s\tremaining: 1h 28m 43s\n",
            "300:\tlearn: 0.6849021\ttest: 0.6734732\tbest: 0.6734732 (300)\ttotal: 2m 50s\tremaining: 1h 31m 25s\n",
            "400:\tlearn: 0.6678492\ttest: 0.6713501\tbest: 0.6713501 (400)\ttotal: 3m 52s\tremaining: 1h 32m 47s\n",
            "500:\tlearn: 0.6489240\ttest: 0.6697702\tbest: 0.6697702 (500)\ttotal: 4m 56s\tremaining: 1h 33m 37s\n",
            "600:\tlearn: 0.6286190\ttest: 0.6682808\tbest: 0.6682423 (598)\ttotal: 5m 59s\tremaining: 1h 33m 43s\n",
            "700:\tlearn: 0.6096438\ttest: 0.6678665\tbest: 0.6678645 (698)\ttotal: 7m 3s\tremaining: 1h 33m 38s\n",
            "800:\tlearn: 0.5927210\ttest: 0.6675491\tbest: 0.6673454 (766)\ttotal: 8m 7s\tremaining: 1h 33m 15s\n",
            "900:\tlearn: 0.5757733\ttest: 0.6672852\tbest: 0.6670491 (844)\ttotal: 9m 11s\tremaining: 1h 32m 46s\n",
            "Stopped by overfitting detector  (100 iterations wait)\n",
            "\n",
            "bestTest = 0.6670490964\n",
            "bestIteration = 844\n",
            "\n",
            "Shrink model to first 845 iterations.\n",
            "============ Fold 3 ============\n",
            "\n",
            "0:\tlearn: 1.0831514\ttest: 1.0830642\tbest: 1.0830642 (0)\ttotal: 310ms\tremaining: 51m 42s\n",
            "100:\tlearn: 0.7303778\ttest: 0.7092536\tbest: 0.7092536 (100)\ttotal: 49.1s\tremaining: 1h 20m 16s\n",
            "200:\tlearn: 0.7023113\ttest: 0.6888512\tbest: 0.6888512 (200)\ttotal: 2m 1s\tremaining: 1h 38m 47s\n",
            "300:\tlearn: 0.6853219\ttest: 0.6845243\tbest: 0.6845243 (300)\ttotal: 3m 10s\tremaining: 1h 42m 11s\n",
            "400:\tlearn: 0.6675973\ttest: 0.6826500\tbest: 0.6826052 (388)\ttotal: 4m 26s\tremaining: 1h 46m 19s\n",
            "500:\tlearn: 0.6481532\ttest: 0.6808902\tbest: 0.6807720 (498)\ttotal: 5m 35s\tremaining: 1h 46m 7s\n",
            "600:\tlearn: 0.6285784\ttest: 0.6800921\tbest: 0.6800914 (595)\ttotal: 6m 41s\tremaining: 1h 44m 44s\n",
            "700:\tlearn: 0.6083968\ttest: 0.6796023\tbest: 0.6794399 (663)\ttotal: 7m 46s\tremaining: 1h 43m 12s\n",
            "800:\tlearn: 0.5899674\ttest: 0.6789752\tbest: 0.6788943 (792)\ttotal: 8m 51s\tremaining: 1h 41m 38s\n",
            "900:\tlearn: 0.5721327\ttest: 0.6783820\tbest: 0.6783820 (900)\ttotal: 9m 55s\tremaining: 1h 40m 14s\n",
            "1000:\tlearn: 0.5544633\ttest: 0.6780686\tbest: 0.6779982 (942)\ttotal: 10m 59s\tremaining: 1h 38m 45s\n",
            "1100:\tlearn: 0.5367893\ttest: 0.6780652\tbest: 0.6776773 (1037)\ttotal: 12m 9s\tremaining: 1h 38m 14s\n",
            "Stopped by overfitting detector  (100 iterations wait)\n",
            "\n",
            "bestTest = 0.6776773442\n",
            "bestIteration = 1037\n",
            "\n",
            "Shrink model to first 1038 iterations.\n",
            "============ Fold 4 ============\n",
            "\n",
            "0:\tlearn: 1.0831773\ttest: 1.0829793\tbest: 1.0829793 (0)\ttotal: 296ms\tremaining: 49m 23s\n",
            "100:\tlearn: 0.7273797\ttest: 0.6978671\tbest: 0.6978671 (100)\ttotal: 52.4s\tremaining: 1h 25m 35s\n",
            "200:\tlearn: 0.7017828\ttest: 0.6804129\tbest: 0.6804129 (200)\ttotal: 1m 52s\tremaining: 1h 31m 20s\n",
            "300:\tlearn: 0.6860989\ttest: 0.6769156\tbest: 0.6768962 (299)\ttotal: 2m 54s\tremaining: 1h 33m 37s\n",
            "400:\tlearn: 0.6681431\ttest: 0.6751760\tbest: 0.6751278 (397)\ttotal: 3m 57s\tremaining: 1h 34m 36s\n",
            "500:\tlearn: 0.6485897\ttest: 0.6737932\tbest: 0.6737814 (499)\ttotal: 5m\tremaining: 1h 35m 3s\n",
            "600:\tlearn: 0.6285877\ttest: 0.6730592\tbest: 0.6728910 (598)\ttotal: 6m 4s\tremaining: 1h 35m 3s\n",
            "700:\tlearn: 0.6097368\ttest: 0.6721781\tbest: 0.6721426 (699)\ttotal: 7m 14s\tremaining: 1h 35m 58s\n",
            "800:\tlearn: 0.5926144\ttest: 0.6721474\tbest: 0.6719096 (776)\ttotal: 8m 19s\tremaining: 1h 35m 39s\n",
            "Stopped by overfitting detector  (100 iterations wait)\n",
            "\n",
            "bestTest = 0.6719096303\n",
            "bestIteration = 776\n",
            "\n",
            "Shrink model to first 777 iterations.\n",
            "============ Fold 5 ============\n",
            "\n",
            "0:\tlearn: 1.0832186\ttest: 1.0833499\tbest: 1.0833499 (0)\ttotal: 298ms\tremaining: 49m 34s\n",
            "100:\tlearn: 0.7249238\ttest: 0.7056807\tbest: 0.7056807 (100)\ttotal: 45.7s\tremaining: 1h 14m 35s\n",
            "200:\tlearn: 0.6995494\ttest: 0.6906425\tbest: 0.6906425 (200)\ttotal: 1m 45s\tremaining: 1h 25m 49s\n",
            "300:\tlearn: 0.6821473\ttest: 0.6874748\tbest: 0.6874748 (300)\ttotal: 2m 47s\tremaining: 1h 30m 6s\n",
            "400:\tlearn: 0.6642446\ttest: 0.6860589\tbest: 0.6860589 (400)\ttotal: 3m 50s\tremaining: 1h 31m 59s\n",
            "500:\tlearn: 0.6449000\ttest: 0.6855384\tbest: 0.6855013 (499)\ttotal: 4m 57s\tremaining: 1h 34m 5s\n",
            "600:\tlearn: 0.6241052\ttest: 0.6848923\tbest: 0.6848844 (589)\ttotal: 6m 1s\tremaining: 1h 34m 17s\n",
            "700:\tlearn: 0.6055032\ttest: 0.6847242\tbest: 0.6844358 (666)\ttotal: 7m 8s\tremaining: 1h 34m 49s\n",
            "Stopped by overfitting detector  (100 iterations wait)\n",
            "\n",
            "bestTest = 0.6844358101\n",
            "bestIteration = 666\n",
            "\n",
            "Shrink model to first 667 iterations.\n",
            "============ Fold 6 ============\n",
            "\n",
            "0:\tlearn: 1.0831277\ttest: 1.0830831\tbest: 1.0830831 (0)\ttotal: 307ms\tremaining: 51m 6s\n",
            "100:\tlearn: 0.7299032\ttest: 0.7012839\tbest: 0.7012839 (100)\ttotal: 47.2s\tremaining: 1h 17m 8s\n",
            "200:\tlearn: 0.7043960\ttest: 0.6834182\tbest: 0.6833724 (188)\ttotal: 1m 50s\tremaining: 1h 29m 28s\n",
            "300:\tlearn: 0.6855325\ttest: 0.6793321\tbest: 0.6793321 (300)\ttotal: 2m 51s\tremaining: 1h 32m 8s\n",
            "400:\tlearn: 0.6663620\ttest: 0.6766051\tbest: 0.6766051 (400)\ttotal: 3m 57s\tremaining: 1h 34m 33s\n",
            "500:\tlearn: 0.6459442\ttest: 0.6751095\tbest: 0.6750316 (451)\ttotal: 5m\tremaining: 1h 35m 3s\n",
            "600:\tlearn: 0.6271073\ttest: 0.6742075\tbest: 0.6741912 (599)\ttotal: 6m 4s\tremaining: 1h 35m 3s\n",
            "700:\tlearn: 0.6080896\ttest: 0.6741347\tbest: 0.6738342 (628)\ttotal: 7m 8s\tremaining: 1h 34m 50s\n",
            "Stopped by overfitting detector  (100 iterations wait)\n",
            "\n",
            "bestTest = 0.6738341764\n",
            "bestIteration = 628\n",
            "\n",
            "Shrink model to first 629 iterations.\n",
            "============ Fold 7 ============\n",
            "\n",
            "0:\tlearn: 1.0832149\ttest: 1.0831962\tbest: 1.0831962 (0)\ttotal: 307ms\tremaining: 51m 7s\n",
            "100:\tlearn: 0.7276994\ttest: 0.7009006\tbest: 0.7009006 (100)\ttotal: 46.4s\tremaining: 1h 15m 49s\n",
            "200:\tlearn: 0.7030936\ttest: 0.6848146\tbest: 0.6847949 (198)\ttotal: 1m 47s\tremaining: 1h 27m 26s\n",
            "300:\tlearn: 0.6856515\ttest: 0.6815006\tbest: 0.6815006 (300)\ttotal: 2m 49s\tremaining: 1h 30m 58s\n",
            "400:\tlearn: 0.6681527\ttest: 0.6796503\tbest: 0.6795603 (384)\ttotal: 3m 52s\tremaining: 1h 32m 50s\n",
            "500:\tlearn: 0.6503799\ttest: 0.6787021\tbest: 0.6785460 (493)\ttotal: 4m 56s\tremaining: 1h 33m 43s\n",
            "600:\tlearn: 0.6319088\ttest: 0.6780125\tbest: 0.6779053 (597)\ttotal: 6m\tremaining: 1h 34m 4s\n",
            "700:\tlearn: 0.6122103\ttest: 0.6774097\tbest: 0.6773104 (698)\ttotal: 7m 5s\tremaining: 1h 34m\n",
            "800:\tlearn: 0.5939343\ttest: 0.6764882\tbest: 0.6764710 (799)\ttotal: 8m 11s\tremaining: 1h 34m 10s\n",
            "900:\tlearn: 0.5762685\ttest: 0.6764979\tbest: 0.6762442 (823)\ttotal: 9m 15s\tremaining: 1h 33m 32s\n",
            "1000:\tlearn: 0.5586911\ttest: 0.6759459\tbest: 0.6759226 (999)\ttotal: 10m 19s\tremaining: 1h 32m 49s\n",
            "1100:\tlearn: 0.5415433\ttest: 0.6763338\tbest: 0.6757223 (1032)\ttotal: 11m 23s\tremaining: 1h 32m 4s\n",
            "Stopped by overfitting detector  (100 iterations wait)\n",
            "\n",
            "bestTest = 0.6757223098\n",
            "bestIteration = 1032\n",
            "\n",
            "Shrink model to first 1033 iterations.\n",
            "============ Fold 8 ============\n",
            "\n",
            "0:\tlearn: 1.0831574\ttest: 1.0832749\tbest: 1.0832749 (0)\ttotal: 294ms\tremaining: 48m 55s\n",
            "100:\tlearn: 0.7315047\ttest: 0.7031190\tbest: 0.7031190 (100)\ttotal: 41.5s\tremaining: 1h 7m 51s\n",
            "200:\tlearn: 0.7056228\ttest: 0.6837846\tbest: 0.6837846 (200)\ttotal: 1m 41s\tremaining: 1h 22m 21s\n",
            "300:\tlearn: 0.6886137\ttest: 0.6786870\tbest: 0.6786859 (299)\ttotal: 2m 41s\tremaining: 1h 26m 42s\n",
            "400:\tlearn: 0.6690231\ttest: 0.6763612\tbest: 0.6762533 (394)\ttotal: 3m 43s\tremaining: 1h 29m 21s\n",
            "500:\tlearn: 0.6483094\ttest: 0.6741831\tbest: 0.6741430 (497)\ttotal: 4m 47s\tremaining: 1h 30m 45s\n",
            "600:\tlearn: 0.6292264\ttest: 0.6731320\tbest: 0.6731320 (600)\ttotal: 5m 50s\tremaining: 1h 31m 24s\n",
            "700:\tlearn: 0.6111467\ttest: 0.6726734\tbest: 0.6726604 (662)\ttotal: 6m 54s\tremaining: 1h 31m 35s\n",
            "800:\tlearn: 0.5925022\ttest: 0.6727895\tbest: 0.6724140 (727)\ttotal: 7m 57s\tremaining: 1h 31m 21s\n",
            "Stopped by overfitting detector  (100 iterations wait)\n",
            "\n",
            "bestTest = 0.672413971\n",
            "bestIteration = 727\n",
            "\n",
            "Shrink model to first 728 iterations.\n",
            "============ Fold 9 ============\n",
            "\n",
            "0:\tlearn: 1.0832009\ttest: 1.0827439\tbest: 1.0827439 (0)\ttotal: 295ms\tremaining: 49m 9s\n",
            "100:\tlearn: 0.7324903\ttest: 0.7048082\tbest: 0.7048082 (100)\ttotal: 41s\tremaining: 1h 6m 55s\n",
            "200:\tlearn: 0.7065402\ttest: 0.6859564\tbest: 0.6859419 (198)\ttotal: 1m 37s\tremaining: 1h 19m 28s\n",
            "300:\tlearn: 0.6876098\ttest: 0.6816683\tbest: 0.6816683 (300)\ttotal: 2m 38s\tremaining: 1h 25m 10s\n",
            "400:\tlearn: 0.6702645\ttest: 0.6800155\tbest: 0.6800155 (400)\ttotal: 3m 41s\tremaining: 1h 28m 33s\n",
            "500:\tlearn: 0.6496183\ttest: 0.6780545\tbest: 0.6779703 (494)\ttotal: 4m 44s\tremaining: 1h 30m 1s\n",
            "600:\tlearn: 0.6291321\ttest: 0.6770233\tbest: 0.6769609 (596)\ttotal: 5m 48s\tremaining: 1h 30m 49s\n",
            "700:\tlearn: 0.6095260\ttest: 0.6765122\tbest: 0.6765086 (674)\ttotal: 6m 51s\tremaining: 1h 31m 2s\n",
            "800:\tlearn: 0.5920527\ttest: 0.6764972\tbest: 0.6763723 (712)\ttotal: 7m 54s\tremaining: 1h 30m 52s\n",
            "Stopped by overfitting detector  (100 iterations wait)\n",
            "\n",
            "bestTest = 0.6763722857\n",
            "bestIteration = 712\n",
            "\n",
            "Shrink model to first 713 iterations.\n",
            "Log Loss Score: 0.67234\n",
            "\n"
          ]
        }
      ],
      "source": [
        "cat_params = {\n",
        "    \"learning_rate\": 0.026612467217016746,\n",
        "    \"l2_leaf_reg\": 0.3753065117824262,\n",
        "    \"max_depth\": 8,\n",
        "    \"bagging_temperature\": 1,\n",
        "    \"min_data_in_leaf\": 57,\n",
        "    \"max_bin\": 494,\n",
        "    \"random_state\": 42,\n",
        "    \"eval_metric\": \"MultiClass\",\n",
        "    \"loss_function\": \"MultiClass\",\n",
        "    \"od_type\": \"Iter\",\n",
        "    \"od_wait\": 500,\n",
        "    \"iterations\": 10000,\n",
        "    \"cat_features\": [\n",
        "        \"income_total\",\n",
        "        \"income_type\",\n",
        "        \"edu_type\",\n",
        "        \"family_type\",\n",
        "        \"house_type\",\n",
        "        \"FLAG_MOBIL\",\n",
        "        \"work_phone\",\n",
        "        \"phone\",\n",
        "        \"occyp_type\",\n",
        "        \"begin_month\",\n",
        "        \"DAYS_BIRTH_month\",\n",
        "        \"DAYS_BIRTH_week\",\n",
        "        \"Age\",\n",
        "        \"DAYS_EMPLOYED_month\",\n",
        "        \"DAYS_EMPLOYED_week\",\n",
        "        \"before_EMPLOYED\",\n",
        "        \"before_EMPLOYED_month\",\n",
        "        \"before_EMPLOYED_week\",\n",
        "        \"user_code\",\n",
        "    ],\n",
        "}\n",
        "\n",
        "cat_oof, cat_preds = stratified_kfold_cat(cat_params, 10, X, y, X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8eDIkI655IF2"
      },
      "source": [
        "## LGBM TRAIN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aLoEoVsL5MeP"
      },
      "source": [
        "하이퍼파라미터 튜닝은 역시 optuna를 사용"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jRb22yGK5J4w",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "outputId": "f1f49edb-6adf-4097-ca0d-d7b06f5504cd"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-a46e421ec841>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"credit\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"credit\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'load_dataset' is not defined"
          ]
        }
      ],
      "source": [
        "train, test = load_dataset()\n",
        "X = train.drop(\"credit\", axis=1)\n",
        "y = train[\"credit\"]\n",
        "X_test = test.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "quBpJ_0o5T1L",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        },
        "outputId": "83967bd7-31a9-410e-c9b0-5705ce542958"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-00803de4ed59>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;34m\"metric\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"multi_logloss\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m }\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mlgbm_oof\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlgbm_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstratified_kfold_lgbm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlgb_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'stratified_kfold_lgbm' is not defined"
          ]
        }
      ],
      "source": [
        "lgb_params = {\n",
        "    \"reg_alpha\": 5.998770177220496e-05,\n",
        "    \"reg_lambda\": 0.07127674208132959,\n",
        "    \"max_depth\": 18,\n",
        "    \"num_leaves\": 125,\n",
        "    \"colsample_bytree\": 0.4241631237880101,\n",
        "    \"subsample\": 0.8876057928391585,\n",
        "    \"subsample_freq\": 5,\n",
        "    \"min_child_samples\": 5,\n",
        "    \"max_bin\": 449,\n",
        "    \"random_state\": 42,\n",
        "    \"boosting_type\": \"gbdt\",\n",
        "    \"learning_rate\": 0.05,\n",
        "    \"n_estimators\": 10000,\n",
        "    \"objective\": \"multiclass\",\n",
        "    \"metric\": \"multi_logloss\",\n",
        "}\n",
        "lgbm_oof, lgbm_preds = stratified_kfold_lgbm(lgb_params, 10, X, y, X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SfUGzW6Q5Xis"
      },
      "source": [
        "## XGB TRAIN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eMeypggB5ZWS"
      },
      "outputs": [],
      "source": [
        "xgb_params = {\n",
        "    \"eta\": 0.023839252347297356,\n",
        "    \"reg_alpha\": 6.99554614267605e-06,\n",
        "    \"reg_lambda\": 0.010419988953061583,\n",
        "    \"max_depth\": 15,\n",
        "    \"max_leaves\": 159,\n",
        "    \"colsample_bytree\": 0.4515469593932409,\n",
        "    \"subsample\": 0.7732694309118915,\n",
        "    \"min_child_weight\": 5,\n",
        "    \"gamma\": 0.6847131315687576,\n",
        "    \"random_state\": 42,\n",
        "    \"n_estimators\": 10000,\n",
        "    \"objective\": \"multi:softmax\",\n",
        "    \"eval_metric\": \"mlogloss\",\n",
        "}\n",
        "xgb_oof, xgb_preds = stratified_kfold_xgb(xgb_params, 10, X, y, X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKEvKBE-5fTf"
      },
      "source": [
        "## RANDOM FOREST TRAIN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "06enE9EC5hky"
      },
      "outputs": [],
      "source": [
        "rf_params = {\n",
        "        \"criterion\": \"gini\",\n",
        "        \"n_estimators\": 300,\n",
        "        \"min_samples_split\": 10,\n",
        "        \"min_samples_leaf\": 2,\n",
        "        \"max_features\": \"auto\",\n",
        "        \"oob_score\": True,\n",
        "        \"random_state\": 42,\n",
        "        \"n_jobs\": -1,\n",
        "    }\n",
        "rf_oof, rf_preds = stratified_kfold_rf(rf_params, 10, X, y, X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VPgaWoc_5qSd"
      },
      "outputs": [],
      "source": [
        "train, test = load_dataset()\n",
        "train_x = train.drop(\"credit\", axis = 1)\n",
        "train_y = train['credit'].values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HUAKuwAS7pyw"
      },
      "source": [
        "## 배열 합치기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vMEmWxY7skm"
      },
      "source": [
        "catboost와 lgbm, xgb, randomforest를 통해 생성한 배열들을 합쳐준다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SGiT6yaz5usZ"
      },
      "outputs": [],
      "source": [
        "train_pred = np.concatenate([cat_oof, lgbm_oof, xgb_oof, rf_oof], axis=1)\n",
        "train_pred.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PZZe2tUh5wTN"
      },
      "outputs": [],
      "source": [
        "test_pred = np.concatenate([cat_preds, lgbm_preds, xgb_preds, rf_preds], axis=1)\n",
        "test_pred.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tgnW8cJ85yjG"
      },
      "source": [
        "## PYTORCH TABULAR"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9RAa-KiP531n"
      },
      "source": [
        "- Stacking Ensemble을 사용하며 학습 진행"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DdK6yu669YKK"
      },
      "source": [
        "TabNet은 입력으로 Categorical변수를 Embedding 하기 때문에, Categorical 변수라는 것을 지정해주어야 한다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ohKnDTl255fE"
      },
      "outputs": [],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZpdCCEue9fLu"
      },
      "source": [
        "이후 TabNet의 Classifier를 정의해준다.  \n",
        "Approximate Range of Model Hyperparameters:  \n",
        "n_d, n_a: 8 to 512  \n",
        "batch size: 256 to 32768  \n",
        "virtual batch size: 128 to 2048  \n",
        "sparsity regularization constant: 0 to 0.00001  \n",
        "number of shared GLU Blocks: 2 to 10  \n",
        "number of independent decision Blocks: 2 to 10  \n",
        "relaxation constant: 1 to 2.5  \n",
        "number of decision steps: 2 to 10  \n",
        "batch normalization momentum: 0.5 to 0.98"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D0lc7FBs_BgB"
      },
      "source": [
        "첫 번째 공유 GLU 블록(또는 블록이 공유되지 않는 경우 첫 번째 독립 블록)은 입력 피쳐의 차원을 n_a+n_d와 같은 차원으로 줄이기 때문에 독특하다. n_a는 다음 단계의 주의 변압기에 입력된 형상의 치수이고 n_d는 최종 결과를 계산하는 데 사용되는 형상의 치수이다. 이러한 기능은 스플리터에 도달할 때까지 함께 처리됩니다. ReLU 활성화는 n_d 차원 벡터에 적용됩니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gP1hd5m8_mFx"
      },
      "source": [
        "mask_type을 entmax로 설정했는데, 이는 softmax함수의 한 종류이다.  \n",
        "softmax 함수는 하나의 샘플 데이터에 대한 예측값으로 모든 가능한 정답지에 대해서 정답일 확률의 합이 1이 되도록 하는것  \n",
        "즉 정답지의 총 개수가 k일때, k차원의 벡터를 입력받아 각 클래스에 대한 확률을 추정하는 것  \n",
        "\n",
        "entmax는 sparsemax를 더 일반화시킨 것으로 좀더 훈련이 용이하다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nh3gsrSJ57jO"
      },
      "outputs": [],
      "source": [
        "n_fold = 10\n",
        "folds = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=42)\n",
        "splits = folds.split(train_pred, train_y)\n",
        "net_oof = np.zeros((train_pred.shape[0], 3))\n",
        "net_preds = np.zeros((test_pred.shape[0], 3))\n",
        "for fold, (train_idx, valid_idx) in enumerate(splits):\n",
        "    print(f\"============ Fold {fold} ============\\n\")\n",
        "    X_train, X_valid = train_pred[train_idx], train_pred[valid_idx]\n",
        "    y_train, y_valid = train_y[train_idx], train_y[valid_idx]\n",
        "    model = TabNetMultiTaskClassifier(\n",
        "            n_d=64, n_a=64, n_steps=1,\n",
        "            lambda_sparse=1e-4,\n",
        "            optimizer_fn=torch.optim.Adam,\n",
        "            optimizer_params=dict(lr=2e-2),\n",
        "            scheduler_params = {\"gamma\": 0.9, \"step_size\": 50},\n",
        "            scheduler_fn=torch.optim.lr_scheduler.StepLR,\n",
        "            mask_type=\"entmax\", \n",
        "            device_name=device\n",
        "    )\n",
        "\n",
        "    model.fit(\n",
        "        X_train, y_train.reshape(-1,1),\n",
        "        eval_set=[(X_valid, y_valid.reshape(-1,1))],\n",
        "        max_epochs=100,\n",
        "        batch_size=1024,\n",
        "        eval_metric=[\"logloss\"],\n",
        "        virtual_batch_size=128,\n",
        "        num_workers=1,\n",
        "        drop_last=False\n",
        "    )\n",
        "    net_oof[valid_idx] = model.predict_proba(X_valid)\n",
        "    net_preds += model.predict_proba(test_pred)[0] / n_fold\n",
        "log_score = log_loss(train_y, net_oof)\n",
        "print(f\"Log Loss Score: {log_score:.5f}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "신용카드 연체예측.ipynb",
      "provenance": [],
      "mount_file_id": "12auMWQxnaqPe15673jwzNcQXn28PVu5p",
      "authorship_tag": "ABX9TyOn5xF153sDkmjY+pLyRs6u",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
