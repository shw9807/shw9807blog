<h1 id="모두를-위한-머신러닝-강의-정리-">“모두를 위한 머신러닝 강의 정리 “</h1>

<ul>
  <li>toc:true</li>
  <li>branch: master</li>
  <li>badges: true</li>
  <li>author: shw9807</li>
</ul>

<h1 id="모두를-위한-머신러닝">모두를 위한 머신러닝</h1>

<h2 id="lecture-1--기본적인-machine-learning의-용어와-개념-설명">Lecture 1 : 기본적인 Machine Learning의 용어와 개념 설명</h2>
<h3 id="machine-learning-이란">Machine Learning 이란?</h3>
<p>: 학습을 통해서 뭔가를 배우는 능력을 갖는 프로그램</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>1.Supervised learning
- 이미 label들이 정해져 있는 데이터를 통해서 학습
2.Unsupervised learning
- label을 정해주는 것이 아니라 데이터만을 보고 스스로 학습
</code></pre></div></div>

<h3 id="supervised-learning">Supervised learning</h3>
<h4 id="일반적으로-사용하는-방법">일반적으로 사용하는 방법</h4>
<ul>
  <li>이미지 라벨링</li>
  <li>이메일 스팸 필터</li>
  <li>시험 성적 예측</li>
</ul>

<h4 id="supervised-learning의-종류">Supervised learning의 종류</h4>
<ul>
  <li>회귀모형</li>
  <li>이항 분류</li>
  <li>multi-label 분류</li>
</ul>

<h2 id="lecture-2--linear-regression">Lecture 2 : Linear Regression</h2>
<p>세상에서 관측되는 여러 현상들은 선형 모델로 설명되는 경우가 굉장히 많음.</p>

<h3 id="linear-hypothesis-가설">(Linear) Hypothesis (가설)</h3>
<p>$ H(x) = Wx + b $</p>

<h3 id="cost-function--loss-function">Cost function (= Loss function)</h3>
<p>가설이 실제 데이터와 얼마나 일치하는가<br />
$ (H(x) - y)^2 $<br />
$ cost(W,b) = \frac{1}{m} \displaystyle\sum_{i=1}^{m}{(H(x^{(i)} )-y^{(i)} )^2} $</p>

<h3 id="목표--minimize-cost">목표 : Minimize cost</h3>
<p>$ minimize\,cost(W,b) $</p>

<h2 id="lecture-3--how-to-minimize-cost">Lecture 3 : How to minimize cost</h2>

<h3 id="gradient-descent-algorithm">Gradient descent algorithm</h3>
<p>$cost(W,b)$에서 뿐만 아니라 많은 값을 가지는 cost function도 minimize 할 수 있다.</p>
<blockquote>
  <ol>
    <li>아무 값에서든 시작한다.
2.cost(W,b)를 감소시키는 방향으로 W와 b를 조금 변화시킨다.</li>
    <li>더 이상 변화시킬 수 없을 때까지 반복한다.
      <ul>
        <li>
          <dl>
            <dt>이 알고리즘의 장점</dt>
            <dd>어떤 점에서 시작하든 같은 최솟값을 얻을 수 있다.(몇몇 예외의 경우를 제외하고)</dd>
          </dl>
        </li>
      </ul>
    </li>
  </ol>
</blockquote>

<h3 id="formal-definition">Formal definition</h3>
<p>$ cost(W,b) = \frac1m \displaystyle\sum_{i=1}^{m}{(Wx^{(i)}-y^{(i)} )^2} $
$\to cost(W,b) = \frac{1}{2m} \displaystyle\sum_{i=1}^{m}{(Wx^{(i)}-y^{(i)} )^2} $
미분을 쉽게 하기 위해 분모를 2m으로 변화시킨다.</p>

<p>$ W := W - \alpha \frac{d}{dW} cost(W)$<br />
$ W := W - \alpha\frac{d}{dW}\frac{1}{2m} \displaystyle\sum_{i=1}^{m}{(Wx^{(i)}-y^{(i)} )^2}$<br />
$ W := W - \alpha\frac{1}{2m} \displaystyle\sum_{i=1}^{m}{2(Wx^{(i)}-y^{(i)} )x^{(i)}}$<br />
$ W := W - \alpha\frac{1}{m} \displaystyle\sum_{i=1}^{m}{(Wx^{(i)}-y^{(i)} )x^{(i)}}$</p>

<h3 id="gradient-descent-algorithm-1">Gradient descent algorithm</h3>
<p>$ W := W - \alpha\frac{1}{m} \displaystyle\sum_{i=1}^{m}{(Wx^{(i)}-y^{(i)} )x^{(i)}}$</p>

<p>Gradient descent algorithm을 적용하기 전 cost function의 모양을 확인해봐야함<br />
-&gt; 최소제곱법???</p>

<h2 id="lecture-4--multivariable-linear-regression">Lecture 4 : Multivariable linear regression</h2>

<h3 id="hypothesis">Hypothesis</h3>

<p>$ H(x_1,x_2,x_3) = w_1x_1 + w_2x_2 + w_3x_3 + b$</p>

<h3 id="cost-function">cost function</h3>
<p>$ cost(W,b) = \frac1m \displaystyle\sum_{i=1}^{m}{(H(x_1^{(i)},x_2^{(i)},x_3^{(i)})-y^{(i)} )^2} $</p>

<h3 id="hypothesis-using-matrix">Hypothesis using matrix</h3>
<p>$w_1x_1 + w_2x_2 + w_3x_3 +… +w_nx_n$</p>

<p>$ (x_1\,x_2\,x_3) ·\begin{pmatrix}w_1\w_2\w_3\ \end{pmatrix} = (x_1w_1 + x_2w_2 + x_3w_3)  \ H(X) = XW $</p>

<h3 id="wx-vs-xw">WX vs XW</h3>

<ul>
  <li>Lecture (theory) :<br />
$ H(x) = Wx + b $</li>
  <li>Implementation (TensorFlow) :<br />
$ H(X) = XW$</li>
</ul>

<h2 id="lecture-5-1--logisticregression-classification">Lecture 5-1 : Logistic(regression) classification</h2>
<ul>
  <li>classification algorithm 중에서 굉장히 정확도가 높은 알고리즘</li>
  <li>실제 문제에도 바로 적용할 수 있음.</li>
  <li>수업의 궁극적 목표인 뉴럴 네트워크와 deep learning 에 굉장히 중요한 component</li>
</ul>

<h3 id="regression">Regression</h3>
<ul>
  <li>Hypothesis: $H(X) = WX$</li>
  <li>Cost: $cost(W) = \frac1m \sum(WX-y)^2$</li>
  <li>Gradient decent: $W := W - \alpha \frac{d}{dW}cost(W)$</li>
</ul>

<h3 id="binary-classification">(Binary) Classification</h3>
<ul>
  <li>Spam Detection: Spam or Ham</li>
  <li>Facebook feed: show or hide</li>
  <li>Credit Cart Fraudulent Transaction detection</li>
</ul>

<h3 id="linear-regression">Linear regression</h3>
<ul>
  <li>We know Y is 0 or 1<br />
$ H(x) = Wx + b $</li>
  <li>Hypothesis 는 1보다 크거나 0보다 작은 값을 가질 수도 있다.</li>
</ul>

<h3 id="logistic-hypothesis">Logistic Hypothesis</h3>
<p>$ H(X) = \frac1{1+e^{-W^TX}} $</p>

<h2 id="lecture-5-2--logisticregression-classification-cost-function--gradient-decent">Lecture 5-2 : Logistic(regression) classification: <br />cost function &amp; gradient decent</h2>

<h3 id="new-cost-function-for-logistic">New cost function for logistic</h3>

<p>$ cost(W) = \frac1m \sum c(H(x),y) $<br />
$ c(H(x),y) = \begin{cases} -log(H(x))\quad\;\;:y=1 \ -log(1-H(x))\, :y=0\end{cases}$<br />
$C(H(x),y) = -ylog(H(x)) - (1-y)log(1-H(x))$</p>

<h3 id="gradient-decent-algorithm">Gradient decent algorithm</h3>
<p>$W:= W-\alpha\frac d{dW}cost(W)$</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># cost function
</span><span class="n">cost</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="o">-</span><span class="n">tf</span><span class="p">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">Y</span><span class="o">*</span><span class="n">tf</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="n">hypothesis</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">Y</span><span class="p">)</span><span class="o">*</span><span class="n">tf</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">hypothesis</span><span class="p">)))</span>

<span class="c1">#Minimize
</span><span class="n">a</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">Variable</span><span class="p">(</span><span class="mf">0.1</span><span class="p">)</span> <span class="c1"># learning rate, alpha
</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">train</span><span class="p">.</span><span class="n">GradientDescentOptimizer</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="n">train</span> <span class="o">=</span> <span class="n">optimizer</span><span class="p">.</span><span class="n">minimize</span><span class="p">(</span><span class="n">cost</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>---------------------------------------------------------------------------

NameError                                 Traceback (most recent call last)

~\AppData\Local\Temp/ipykernel_8704/1539947199.py in &lt;module&gt;
      1 # cost function
----&gt; 2 cost = tf.reduce_mean(-tf.reduce_sum(Y*tf.log(hypothesis) + (1-Y)*tf.log(1-hypothesis)))
      3 
      4 #Minimize
      5 a = tf.Variable(0.1) # learning rate, alpha


NameError: name 'tf' is not defined
</code></pre></div></div>

<h2 id="lecture-6-1--softmax-classification--multinomial-classification">Lecture 6-1 : Softmax classification :<br /> Multinomial classification</h2>

<h3 id="multinomial-classification">Multinomial classification</h3>
<p><img src="attachment:image.png" alt="image.png" /></p>

<p><img src="attachment:image-2.png" alt="image-2.png" /></p>

<h2 id="lecture-6-2--softmax-classifier의-cost-function">Lecture 6-2 : Softmax classifier의 cost function</h2>
<p><img src="attachment:image.png" alt="image.png" />
<img src="attachment:image-2.png" alt="image-2.png" />
<img src="attachment:image-3.png" alt="image-3.png" />
<img src="attachment:image-4.png" alt="image-4.png" />
<img src="attachment:image-5.png" alt="image-5.png" /></p>
