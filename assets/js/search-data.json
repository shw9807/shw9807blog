{
  
    
        "post0": {
            "title": "[Colab] 식수인원 예측",
            "content": ". from google.colab import drive drive.mount(&#39;/content/drive&#39;) . Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(&#34;/content/drive&#34;, force_remount=True). . !pip install pandas numpy seaborn matplotlib pycaret opencv-python . Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.3.5) Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.21.5) Requirement already satisfied: seaborn in /usr/local/lib/python3.7/dist-packages (0.11.2) Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (3.2.2) Collecting pycaret Downloading pycaret-2.3.6-py3-none-any.whl (301 kB) |████████████████████████████████| 301 kB 3.2 MB/s Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (4.1.2.30) Requirement already satisfied: pytz&gt;=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2018.9) Requirement already satisfied: python-dateutil&gt;=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.2) Requirement already satisfied: six&gt;=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil&gt;=2.7.3-&gt;pandas) (1.15.0) Requirement already satisfied: scipy&gt;=1.0 in /usr/local/lib/python3.7/dist-packages (from seaborn) (1.4.1) Requirement already satisfied: kiwisolver&gt;=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.3.2) Requirement already satisfied: cycler&gt;=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (0.11.0) Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,&gt;=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (3.0.7) Requirement already satisfied: spacy&lt;2.4.0 in /usr/local/lib/python3.7/dist-packages (from pycaret) (2.2.4) Requirement already satisfied: gensim&lt;4.0.0 in /usr/local/lib/python3.7/dist-packages (from pycaret) (3.6.0) Collecting lightgbm&gt;=2.3.1 Downloading lightgbm-3.3.2-py3-none-manylinux1_x86_64.whl (2.0 MB) |████████████████████████████████| 2.0 MB 51.5 MB/s Collecting scikit-learn==0.23.2 Downloading scikit_learn-0.23.2-cp37-cp37m-manylinux1_x86_64.whl (6.8 MB) |████████████████████████████████| 6.8 MB 47.0 MB/s Collecting umap-learn Downloading umap-learn-0.5.2.tar.gz (86 kB) |████████████████████████████████| 86 kB 5.4 MB/s Collecting pandas-profiling&gt;=2.8.0 Downloading pandas_profiling-3.1.0-py2.py3-none-any.whl (261 kB) |████████████████████████████████| 261 kB 41.3 MB/s Requirement already satisfied: cufflinks&gt;=0.17.0 in /usr/local/lib/python3.7/dist-packages (from pycaret) (0.17.3) Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from pycaret) (3.2.5) Collecting mlflow Downloading mlflow-1.23.1-py3-none-any.whl (15.6 MB) |████████████████████████████████| 15.6 MB 33.3 MB/s Collecting pyLDAvis Downloading pyLDAvis-3.3.1.tar.gz (1.7 MB) |████████████████████████████████| 1.7 MB 35.1 MB/s Installing build dependencies ... done Getting requirements to build wheel ... done Installing backend dependencies ... done Preparing wheel metadata ... done Collecting yellowbrick&gt;=1.0.1 Downloading yellowbrick-1.3.post1-py3-none-any.whl (271 kB) |████████████████████████████████| 271 kB 61.0 MB/s Collecting mlxtend&gt;=0.17.0 Downloading mlxtend-0.19.0-py2.py3-none-any.whl (1.3 MB) |████████████████████████████████| 1.3 MB 52.5 MB/s Requirement already satisfied: IPython in /usr/local/lib/python3.7/dist-packages (from pycaret) (5.5.0) Requirement already satisfied: textblob in /usr/local/lib/python3.7/dist-packages (from pycaret) (0.15.3) Requirement already satisfied: ipywidgets in /usr/local/lib/python3.7/dist-packages (from pycaret) (7.6.5) Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from pycaret) (1.1.0) Requirement already satisfied: pyyaml&lt;6.0.0 in /usr/local/lib/python3.7/dist-packages (from pycaret) (3.13) Collecting pyod Downloading pyod-0.9.7.tar.gz (114 kB) |████████████████████████████████| 114 kB 51.4 MB/s Collecting imbalanced-learn==0.7.0 Downloading imbalanced_learn-0.7.0-py3-none-any.whl (167 kB) |████████████████████████████████| 167 kB 41.7 MB/s Collecting scikit-plot Downloading scikit_plot-0.3.7-py3-none-any.whl (33 kB) Requirement already satisfied: wordcloud in /usr/local/lib/python3.7/dist-packages (from pycaret) (1.5.0) Collecting Boruta Downloading Boruta-0.3-py3-none-any.whl (56 kB) |████████████████████████████████| 56 kB 3.6 MB/s Collecting kmodes&gt;=0.10.1 Downloading kmodes-0.11.1-py2.py3-none-any.whl (19 kB) Requirement already satisfied: plotly&gt;=4.4.1 in /usr/local/lib/python3.7/dist-packages (from pycaret) (5.5.0) Requirement already satisfied: threadpoolctl&gt;=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.23.2-&gt;pycaret) (3.1.0) Requirement already satisfied: colorlover&gt;=0.2.1 in /usr/local/lib/python3.7/dist-packages (from cufflinks&gt;=0.17.0-&gt;pycaret) (0.3.0) Requirement already satisfied: setuptools&gt;=34.4.1 in /usr/local/lib/python3.7/dist-packages (from cufflinks&gt;=0.17.0-&gt;pycaret) (57.4.0) Requirement already satisfied: smart-open&gt;=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim&lt;4.0.0-&gt;pycaret) (5.2.1) Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from IPython-&gt;pycaret) (2.6.1) Requirement already satisfied: simplegeneric&gt;0.8 in /usr/local/lib/python3.7/dist-packages (from IPython-&gt;pycaret) (0.8.1) Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from IPython-&gt;pycaret) (0.7.5) Requirement already satisfied: traitlets&gt;=4.2 in /usr/local/lib/python3.7/dist-packages (from IPython-&gt;pycaret) (5.1.1) Requirement already satisfied: prompt-toolkit&lt;2.0.0,&gt;=1.0.4 in /usr/local/lib/python3.7/dist-packages (from IPython-&gt;pycaret) (1.0.18) Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from IPython-&gt;pycaret) (4.8.0) Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from IPython-&gt;pycaret) (4.4.2) Requirement already satisfied: ipykernel&gt;=4.5.1 in /usr/local/lib/python3.7/dist-packages (from ipywidgets-&gt;pycaret) (4.10.1) Requirement already satisfied: nbformat&gt;=4.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets-&gt;pycaret) (5.1.3) Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets-&gt;pycaret) (0.2.0) Requirement already satisfied: jupyterlab-widgets&gt;=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets-&gt;pycaret) (1.0.2) Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets-&gt;pycaret) (3.5.2) Requirement already satisfied: jupyter-client in /usr/local/lib/python3.7/dist-packages (from ipykernel&gt;=4.5.1-&gt;ipywidgets-&gt;pycaret) (5.3.5) Requirement already satisfied: tornado&gt;=4.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel&gt;=4.5.1-&gt;ipywidgets-&gt;pycaret) (5.1.1) Requirement already satisfied: wheel in /usr/local/lib/python3.7/dist-packages (from lightgbm&gt;=2.3.1-&gt;pycaret) (0.37.1) Requirement already satisfied: jupyter-core in /usr/local/lib/python3.7/dist-packages (from nbformat&gt;=4.2.0-&gt;ipywidgets-&gt;pycaret) (4.9.1) Requirement already satisfied: jsonschema!=2.5.0,&gt;=2.4 in /usr/local/lib/python3.7/dist-packages (from nbformat&gt;=4.2.0-&gt;ipywidgets-&gt;pycaret) (4.3.3) Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,&gt;=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema!=2.5.0,&gt;=2.4-&gt;nbformat&gt;=4.2.0-&gt;ipywidgets-&gt;pycaret) (0.18.1) Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from jsonschema!=2.5.0,&gt;=2.4-&gt;nbformat&gt;=4.2.0-&gt;ipywidgets-&gt;pycaret) (4.11.0) Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from jsonschema!=2.5.0,&gt;=2.4-&gt;nbformat&gt;=4.2.0-&gt;ipywidgets-&gt;pycaret) (3.10.0.2) Requirement already satisfied: importlib-resources&gt;=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema!=2.5.0,&gt;=2.4-&gt;nbformat&gt;=4.2.0-&gt;ipywidgets-&gt;pycaret) (5.4.0) Requirement already satisfied: attrs&gt;=17.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema!=2.5.0,&gt;=2.4-&gt;nbformat&gt;=4.2.0-&gt;ipywidgets-&gt;pycaret) (21.4.0) Requirement already satisfied: zipp&gt;=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources&gt;=1.4.0-&gt;jsonschema!=2.5.0,&gt;=2.4-&gt;nbformat&gt;=4.2.0-&gt;ipywidgets-&gt;pycaret) (3.7.0) Collecting pydantic&gt;=1.8.1 Downloading pydantic-1.9.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.9 MB) |████████████████████████████████| 10.9 MB 43.7 MB/s Collecting htmlmin&gt;=0.1.12 Downloading htmlmin-0.1.12.tar.gz (19 kB) Collecting joblib Downloading joblib-1.0.1-py3-none-any.whl (303 kB) |████████████████████████████████| 303 kB 47.9 MB/s Collecting visions[type_image_path]==0.7.4 Downloading visions-0.7.4-py3-none-any.whl (102 kB) |████████████████████████████████| 102 kB 11.5 MB/s Requirement already satisfied: missingno&gt;=0.4.2 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling&gt;=2.8.0-&gt;pycaret) (0.5.0) Collecting pyyaml&lt;6.0.0 Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB) |████████████████████████████████| 636 kB 58.3 MB/s Collecting tangled-up-in-unicode==0.1.0 Downloading tangled_up_in_unicode-0.1.0-py3-none-any.whl (3.1 MB) |████████████████████████████████| 3.1 MB 57.8 MB/s Collecting multimethod&gt;=1.4 Downloading multimethod-1.7-py3-none-any.whl (9.5 kB) Requirement already satisfied: jinja2&gt;=2.11.1 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling&gt;=2.8.0-&gt;pycaret) (2.11.3) Collecting requests&gt;=2.24.0 Downloading requests-2.27.1-py2.py3-none-any.whl (63 kB) |████████████████████████████████| 63 kB 1.5 MB/s Requirement already satisfied: tqdm&gt;=4.48.2 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling&gt;=2.8.0-&gt;pycaret) (4.62.3) Collecting phik&gt;=0.11.1 Downloading phik-0.12.0-cp37-cp37m-manylinux2010_x86_64.whl (675 kB) |████████████████████████████████| 675 kB 56.2 MB/s Requirement already satisfied: markupsafe~=2.0.1 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling&gt;=2.8.0-&gt;pycaret) (2.0.1) Requirement already satisfied: networkx&gt;=2.4 in /usr/local/lib/python3.7/dist-packages (from visions[type_image_path]==0.7.4-&gt;pandas-profiling&gt;=2.8.0-&gt;pycaret) (2.6.3) Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from visions[type_image_path]==0.7.4-&gt;pandas-profiling&gt;=2.8.0-&gt;pycaret) (7.1.2) Collecting imagehash Downloading ImageHash-4.2.1.tar.gz (812 kB) |████████████████████████████████| 812 kB 57.0 MB/s Collecting scipy&gt;=1.0 Downloading scipy-1.5.4-cp37-cp37m-manylinux1_x86_64.whl (25.9 MB) |████████████████████████████████| 25.9 MB 1.6 MB/s Requirement already satisfied: tenacity&gt;=6.2.0 in /usr/local/lib/python3.7/dist-packages (from plotly&gt;=4.4.1-&gt;pycaret) (8.0.1) Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit&lt;2.0.0,&gt;=1.0.4-&gt;IPython-&gt;pycaret) (0.2.5) Requirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests&gt;=2.24.0-&gt;pandas-profiling&gt;=2.8.0-&gt;pycaret) (2021.10.8) Requirement already satisfied: urllib3&lt;1.27,&gt;=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests&gt;=2.24.0-&gt;pandas-profiling&gt;=2.8.0-&gt;pycaret) (1.24.3) Requirement already satisfied: idna&lt;4,&gt;=2.5 in /usr/local/lib/python3.7/dist-packages (from requests&gt;=2.24.0-&gt;pandas-profiling&gt;=2.8.0-&gt;pycaret) (2.10) Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests&gt;=2.24.0-&gt;pandas-profiling&gt;=2.8.0-&gt;pycaret) (2.0.11) Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy&lt;2.4.0-&gt;pycaret) (7.4.0) Requirement already satisfied: cymem&lt;2.1.0,&gt;=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy&lt;2.4.0-&gt;pycaret) (2.0.6) Requirement already satisfied: catalogue&lt;1.1.0,&gt;=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy&lt;2.4.0-&gt;pycaret) (1.0.0) Requirement already satisfied: wasabi&lt;1.1.0,&gt;=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy&lt;2.4.0-&gt;pycaret) (0.9.0) Requirement already satisfied: murmurhash&lt;1.1.0,&gt;=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy&lt;2.4.0-&gt;pycaret) (1.0.6) Requirement already satisfied: srsly&lt;1.1.0,&gt;=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy&lt;2.4.0-&gt;pycaret) (1.0.5) Requirement already satisfied: blis&lt;0.5.0,&gt;=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy&lt;2.4.0-&gt;pycaret) (0.4.1) Requirement already satisfied: preshed&lt;3.1.0,&gt;=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy&lt;2.4.0-&gt;pycaret) (3.0.6) Requirement already satisfied: plac&lt;1.2.0,&gt;=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy&lt;2.4.0-&gt;pycaret) (1.1.3) Requirement already satisfied: notebook&gt;=4.4.1 in /usr/local/lib/python3.7/dist-packages (from widgetsnbextension~=3.5.0-&gt;ipywidgets-&gt;pycaret) (5.3.1) Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.5.0-&gt;ipywidgets-&gt;pycaret) (5.6.1) Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.5.0-&gt;ipywidgets-&gt;pycaret) (1.8.0) Requirement already satisfied: terminado&gt;=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.5.0-&gt;ipywidgets-&gt;pycaret) (0.13.1) Requirement already satisfied: pyzmq&gt;=13 in /usr/local/lib/python3.7/dist-packages (from jupyter-client-&gt;ipykernel&gt;=4.5.1-&gt;ipywidgets-&gt;pycaret) (22.3.0) Requirement already satisfied: ptyprocess in /usr/local/lib/python3.7/dist-packages (from terminado&gt;=0.8.1-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.5.0-&gt;ipywidgets-&gt;pycaret) (0.7.0) Collecting numpy Downloading numpy-1.19.5-cp37-cp37m-manylinux2010_x86_64.whl (14.8 MB) |████████████████████████████████| 14.8 MB 324 kB/s Requirement already satisfied: PyWavelets in /usr/local/lib/python3.7/dist-packages (from imagehash-&gt;visions[type_image_path]==0.7.4-&gt;pandas-profiling&gt;=2.8.0-&gt;pycaret) (1.2.0) Requirement already satisfied: sqlalchemy in /usr/local/lib/python3.7/dist-packages (from mlflow-&gt;pycaret) (1.4.31) Collecting prometheus-flask-exporter Downloading prometheus_flask_exporter-0.18.7-py3-none-any.whl (17 kB) Collecting querystring-parser Downloading querystring_parser-1.2.4-py2.py3-none-any.whl (7.9 kB) Collecting docker&gt;=4.0.0 Downloading docker-5.0.3-py2.py3-none-any.whl (146 kB) |████████████████████████████████| 146 kB 65.1 MB/s Requirement already satisfied: click&gt;=7.0 in /usr/local/lib/python3.7/dist-packages (from mlflow-&gt;pycaret) (7.1.2) Requirement already satisfied: cloudpickle in /usr/local/lib/python3.7/dist-packages (from mlflow-&gt;pycaret) (1.3.0) Collecting gunicorn Downloading gunicorn-20.1.0-py3-none-any.whl (79 kB) |████████████████████████████████| 79 kB 7.2 MB/s Collecting gitpython&gt;=2.1.0 Downloading GitPython-3.1.26-py3-none-any.whl (180 kB) |████████████████████████████████| 180 kB 62.8 MB/s Requirement already satisfied: Flask in /usr/local/lib/python3.7/dist-packages (from mlflow-&gt;pycaret) (1.1.4) Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from mlflow-&gt;pycaret) (21.3) Collecting alembic Downloading alembic-1.7.6-py3-none-any.whl (210 kB) |████████████████████████████████| 210 kB 64.8 MB/s Collecting databricks-cli&gt;=0.8.7 Downloading databricks-cli-0.16.4.tar.gz (58 kB) |████████████████████████████████| 58 kB 5.3 MB/s Requirement already satisfied: entrypoints in /usr/local/lib/python3.7/dist-packages (from mlflow-&gt;pycaret) (0.4) Requirement already satisfied: protobuf&gt;=3.7.0 in /usr/local/lib/python3.7/dist-packages (from mlflow-&gt;pycaret) (3.17.3) Requirement already satisfied: sqlparse&gt;=0.3.1 in /usr/local/lib/python3.7/dist-packages (from mlflow-&gt;pycaret) (0.4.2) Requirement already satisfied: tabulate&gt;=0.7.7 in /usr/local/lib/python3.7/dist-packages (from databricks-cli&gt;=0.8.7-&gt;mlflow-&gt;pycaret) (0.8.9) Collecting websocket-client&gt;=0.32.0 Downloading websocket_client-1.2.3-py3-none-any.whl (53 kB) |████████████████████████████████| 53 kB 1.8 MB/s Collecting gitdb&lt;5,&gt;=4.0.1 Downloading gitdb-4.0.9-py3-none-any.whl (63 kB) |████████████████████████████████| 63 kB 1.3 MB/s Collecting smmap&lt;6,&gt;=3.0.1 Downloading smmap-5.0.0-py3-none-any.whl (24 kB) Collecting Mako Downloading Mako-1.1.6-py2.py3-none-any.whl (75 kB) |████████████████████████████████| 75 kB 3.0 MB/s Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.7/dist-packages (from sqlalchemy-&gt;mlflow-&gt;pycaret) (1.1.2) Requirement already satisfied: Werkzeug&lt;2.0,&gt;=0.15 in /usr/local/lib/python3.7/dist-packages (from Flask-&gt;mlflow-&gt;pycaret) (1.0.1) Requirement already satisfied: itsdangerous&lt;2.0,&gt;=0.24 in /usr/local/lib/python3.7/dist-packages (from Flask-&gt;mlflow-&gt;pycaret) (1.1.0) Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.5.0-&gt;ipywidgets-&gt;pycaret) (0.7.1) Requirement already satisfied: mistune&lt;2,&gt;=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.5.0-&gt;ipywidgets-&gt;pycaret) (0.8.4) Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.5.0-&gt;ipywidgets-&gt;pycaret) (4.1.0) Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.5.0-&gt;ipywidgets-&gt;pycaret) (0.5.0) Requirement already satisfied: pandocfilters&gt;=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.5.0-&gt;ipywidgets-&gt;pycaret) (1.5.0) Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach-&gt;nbconvert-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.5.0-&gt;ipywidgets-&gt;pycaret) (0.5.1) Requirement already satisfied: prometheus-client in /usr/local/lib/python3.7/dist-packages (from prometheus-flask-exporter-&gt;mlflow-&gt;pycaret) (0.13.1) Collecting funcy Downloading funcy-1.17-py2.py3-none-any.whl (33 kB) Requirement already satisfied: numexpr in /usr/local/lib/python3.7/dist-packages (from pyLDAvis-&gt;pycaret) (2.8.1) Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyLDAvis-&gt;pycaret) (0.16.0) Requirement already satisfied: sklearn in /usr/local/lib/python3.7/dist-packages (from pyLDAvis-&gt;pycaret) (0.0) Collecting pyLDAvis Downloading pyLDAvis-3.3.0.tar.gz (1.7 MB) |████████████████████████████████| 1.7 MB 53.2 MB/s Installing build dependencies ... done Getting requirements to build wheel ... done Installing backend dependencies ... done Preparing wheel metadata ... done Downloading pyLDAvis-3.2.2.tar.gz (1.7 MB) |████████████████████████████████| 1.7 MB 39.9 MB/s Requirement already satisfied: numba&gt;=0.35 in /usr/local/lib/python3.7/dist-packages (from pyod-&gt;pycaret) (0.51.2) Requirement already satisfied: statsmodels in /usr/local/lib/python3.7/dist-packages (from pyod-&gt;pycaret) (0.10.2) Requirement already satisfied: llvmlite&lt;0.35,&gt;=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba&gt;=0.35-&gt;pyod-&gt;pycaret) (0.34.0) Requirement already satisfied: patsy&gt;=0.4.0 in /usr/local/lib/python3.7/dist-packages (from statsmodels-&gt;pyod-&gt;pycaret) (0.5.2) Collecting pynndescent&gt;=0.5 Downloading pynndescent-0.5.6.tar.gz (1.1 MB) |████████████████████████████████| 1.1 MB 53.8 MB/s Building wheels for collected packages: htmlmin, imagehash, databricks-cli, pyLDAvis, pyod, umap-learn, pynndescent Building wheel for htmlmin (setup.py) ... done Created wheel for htmlmin: filename=htmlmin-0.1.12-py3-none-any.whl size=27098 sha256=0ac26f6e77bb0d8a04f93546efc75eed0a196c9e537d277132208e70ad801eb3 Stored in directory: /root/.cache/pip/wheels/70/e1/52/5b14d250ba868768823940c3229e9950d201a26d0bd3ee8655 Building wheel for imagehash (setup.py) ... done Created wheel for imagehash: filename=ImageHash-4.2.1-py2.py3-none-any.whl size=295206 sha256=ccd4f3d602715f900eacec9a7eef0791c0942040ffd232acb8aacab888d9a716 Stored in directory: /root/.cache/pip/wheels/4c/d5/59/5e3e297533ddb09407769762985d134135064c6831e29a914e Building wheel for databricks-cli (setup.py) ... done Created wheel for databricks-cli: filename=databricks_cli-0.16.4-py3-none-any.whl size=106877 sha256=409ad12cdeb5adba7dd68436a2576f5a2c2237433dd8852e2e6e90f37a3db9dc Stored in directory: /root/.cache/pip/wheels/a2/a1/6d/fa1d22ea25ed8593887437fe1c7e00f6ef307fc240ccd4dc5c Building wheel for pyLDAvis (setup.py) ... done Created wheel for pyLDAvis: filename=pyLDAvis-3.2.2-py2.py3-none-any.whl size=135617 sha256=76a19ed4cadece8c56bd4f4abdadf93e4573a3f305926e2b06844cdfaa240793 Stored in directory: /root/.cache/pip/wheels/f8/b1/9b/560ac1931796b7303f7b517b949d2d31a4fbc512aad3b9f284 Building wheel for pyod (setup.py) ... done Created wheel for pyod: filename=pyod-0.9.7-py3-none-any.whl size=136279 sha256=99724be18903b4eb3bb9d2e7097ccdd76ec0d5c1b3180aa93f1d5c032a61520c Stored in directory: /root/.cache/pip/wheels/ce/14/ae/60cbb36511e59bc12f8f0883805f586db3b315972b54865d33 Building wheel for umap-learn (setup.py) ... done Created wheel for umap-learn: filename=umap_learn-0.5.2-py3-none-any.whl size=82708 sha256=2a1dda13364b7d5dbe854b2862e804f37037167509fd128ed6fb48ec701d081b Stored in directory: /root/.cache/pip/wheels/84/1b/c6/aaf68a748122632967cef4dffef68224eb16798b6793257d82 Building wheel for pynndescent (setup.py) ... done Created wheel for pynndescent: filename=pynndescent-0.5.6-py3-none-any.whl size=53943 sha256=90a74ad012d471181f331ceb99f4dc27cf1d6f60ba7605230c22964ea6f5c78e Stored in directory: /root/.cache/pip/wheels/03/f1/56/f80d72741e400345b5a5b50ec3d929aca581bf45e0225d5c50 Successfully built htmlmin imagehash databricks-cli pyLDAvis pyod umap-learn pynndescent Installing collected packages: numpy, tangled-up-in-unicode, smmap, scipy, multimethod, joblib, websocket-client, visions, scikit-learn, requests, Mako, imagehash, gitdb, querystring-parser, pyyaml, pynndescent, pydantic, prometheus-flask-exporter, phik, htmlmin, gunicorn, gitpython, funcy, docker, databricks-cli, alembic, yellowbrick, umap-learn, scikit-plot, pyod, pyLDAvis, pandas-profiling, mlxtend, mlflow, lightgbm, kmodes, imbalanced-learn, Boruta, pycaret Attempting uninstall: numpy Found existing installation: numpy 1.21.5 Uninstalling numpy-1.21.5: Successfully uninstalled numpy-1.21.5 Attempting uninstall: scipy Found existing installation: scipy 1.4.1 Uninstalling scipy-1.4.1: Successfully uninstalled scipy-1.4.1 Attempting uninstall: joblib Found existing installation: joblib 1.1.0 Uninstalling joblib-1.1.0: Successfully uninstalled joblib-1.1.0 Attempting uninstall: scikit-learn Found existing installation: scikit-learn 1.0.2 Uninstalling scikit-learn-1.0.2: Successfully uninstalled scikit-learn-1.0.2 Attempting uninstall: requests Found existing installation: requests 2.23.0 Uninstalling requests-2.23.0: Successfully uninstalled requests-2.23.0 Attempting uninstall: pyyaml Found existing installation: PyYAML 3.13 Uninstalling PyYAML-3.13: Successfully uninstalled PyYAML-3.13 Attempting uninstall: pandas-profiling Found existing installation: pandas-profiling 1.4.1 Uninstalling pandas-profiling-1.4.1: Successfully uninstalled pandas-profiling-1.4.1 Attempting uninstall: mlxtend Found existing installation: mlxtend 0.14.0 Uninstalling mlxtend-0.14.0: Successfully uninstalled mlxtend-0.14.0 Attempting uninstall: lightgbm Found existing installation: lightgbm 2.2.3 Uninstalling lightgbm-2.2.3: Successfully uninstalled lightgbm-2.2.3 Attempting uninstall: imbalanced-learn Found existing installation: imbalanced-learn 0.8.1 Uninstalling imbalanced-learn-0.8.1: Successfully uninstalled imbalanced-learn-0.8.1 ERROR: pip&#39;s dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. tensorflow 2.8.0 requires tf-estimator-nightly==2.8.0.dev2021122109, which is not installed. tensorflow 2.8.0 requires numpy&gt;=1.20, but you have numpy 1.19.5 which is incompatible. google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.27.1 which is incompatible. datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible. albumentations 0.1.12 requires imgaug&lt;0.2.7,&gt;=0.2.5, but you have imgaug 0.2.9 which is incompatible. Successfully installed Boruta-0.3 Mako-1.1.6 alembic-1.7.6 databricks-cli-0.16.4 docker-5.0.3 funcy-1.17 gitdb-4.0.9 gitpython-3.1.26 gunicorn-20.1.0 htmlmin-0.1.12 imagehash-4.2.1 imbalanced-learn-0.7.0 joblib-1.0.1 kmodes-0.11.1 lightgbm-3.3.2 mlflow-1.23.1 mlxtend-0.19.0 multimethod-1.7 numpy-1.19.5 pandas-profiling-3.1.0 phik-0.12.0 prometheus-flask-exporter-0.18.7 pyLDAvis-3.2.2 pycaret-2.3.6 pydantic-1.9.0 pynndescent-0.5.6 pyod-0.9.7 pyyaml-5.4.1 querystring-parser-1.2.4 requests-2.27.1 scikit-learn-0.23.2 scikit-plot-0.3.7 scipy-1.5.4 smmap-5.0.0 tangled-up-in-unicode-0.1.0 umap-learn-0.5.2 visions-0.7.4 websocket-client-1.2.3 yellowbrick-1.3.post1 . import pandas as pd import numpy as np import seaborn as sns import matplotlib.pyplot as plt import statsmodels.api as sm from sklearn.preprocessing import LabelEncoder import warnings warnings.filterwarnings(action=&#39;ignore&#39;) . /usr/local/lib/python3.7/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead. import pandas.util.testing as tm . path=&#39;/content/drive/MyDrive/Colab Notebooks/식수인원예측/&#39; . train = pd.read_csv(path+&#39;train.csv&#39;) test = pd.read_csv(path+&#39;test.csv&#39;) . &#45216;&#51676; &#45936;&#51060;&#53552; &#51204;&#52376;&#47532; . def process_date(df): df[&#39;일자&#39;] = pd.to_datetime(df[&#39;일자&#39;], format=&quot;%Y-%m-%d&quot;) df[&#39;년&#39;] = df[&#39;일자&#39;].dt.year df[&#39;월&#39;] = df[&#39;일자&#39;].dt.month df[&#39;일&#39;] = df[&#39;일자&#39;].dt.day #df = df.drop(&#39;일자&#39;, axis=1) return df . train = process_date(train) day_encoder = LabelEncoder() train[&#39;요일&#39;] = day_encoder.fit_transform(train[&#39;요일&#39;]) test = process_date(test) day_encoder = LabelEncoder() test[&#39;요일&#39;] = day_encoder.fit_transform(test[&#39;요일&#39;]) . fit_transform을 사용하면 코드가 더 간단해지지만 원래는 fit을 해준 후에 transform을 진행해야 train과 test가 같은 값으로 인코딩 된다고 한다. . train[&#39;요일&#39;].head() . 0 3 1 4 2 2 3 1 4 0 Name: 요일, dtype: int64 . test[&#39;요일&#39;].head() . 0 2 1 1 2 0 3 3 4 4 Name: 요일, dtype: int64 . train이 월 화 수 목 금 test가 수 목 금 월 화 순서였는데 인코딩은 동일하게 된 것으로 보인다. . &#49885;&#49324; &#44032;&#45733;&#51088; . train[&#39;식사가능자&#39;] = train[&#39;본사정원수&#39;] - train[&#39;본사휴가자수&#39;] - train[&#39;본사출장자수&#39;] - train[&#39;현본사소속재택근무자수&#39;] . &#51060;&#51204; &#51452; &#49885;&#44228; . from datetime import timedelta train[&#39;전주중식계&#39;] = 0 train[&#39;전주석식계&#39;] = 0 idx = pd.DatetimeIndex(train[&#39;일자&#39;]) + timedelta(weeks=-1) for i in range(len(train)): try: train[&#39;전주중식계&#39;][i] = train[train[&#39;일자&#39;] == str(idx[i])[:10]][&#39;중식계&#39;] train[&#39;전주석식계&#39;][i] = train[train[&#39;일자&#39;] == str(idx[i])[:10]][&#39;석식계&#39;] except: train[&#39;전주중식계&#39;][i] = train.iloc[i,:][&#39;중식계&#39;] train[&#39;전주석식계&#39;][i] = train.iloc[i,:][&#39;석식계&#39;] train[[&#39;중식계&#39;, &#39;전주중식계&#39;, &#39;석식계&#39;, &#39;전주석식계&#39;, &#39;일자&#39;]].head(10) . 중식계 전주중식계 석식계 전주석식계 일자 . 0 1039.0 | NaN | 331.0 | NaN | 2016-02-01 | . 1 867.0 | NaN | 560.0 | NaN | 2016-02-02 | . 2 1017.0 | NaN | 573.0 | NaN | 2016-02-03 | . 3 978.0 | NaN | 525.0 | NaN | 2016-02-04 | . 4 925.0 | NaN | 330.0 | NaN | 2016-02-05 | . 5 1045.0 | 978.0 | 550.0 | 525.0 | 2016-02-11 | . 6 909.0 | 925.0 | 598.0 | 330.0 | 2016-02-12 | . 7 1268.0 | NaN | 672.0 | NaN | 2016-02-15 | . 8 1014.0 | NaN | 523.0 | NaN | 2016-02-16 | . 9 916.0 | NaN | 588.0 | NaN | 2016-02-17 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; &#50836;&#51068; &#54217;&#44512; &#49885;&#44228; . fig, ax = plt.subplots(1,2,figsize=(10,5)) sns.barplot(x=&#39;요일&#39;, y=&#39;중식계&#39;, data=train, ax=ax[0]) sns.barplot(x=&#39;요일&#39;, y=&#39;석식계&#39;, data=train, ax=ax[1]) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7ffb5ce30bd0&gt; . 순서대로 금-목-수-월-화 . train[&#39;요일평균중식계&#39;] = 0 tmp = train.groupby(&#39;요일&#39;).mean()[&#39;중식계&#39;].values for i in range(len(tmp)): train[&#39;요일평균중식계&#39;][train[&#39;요일&#39;] == i] = tmp[i] . train[&#39;요일평균석식계&#39;] = 0 tmp = train.groupby(&#39;요일&#39;).mean()[&#39;석식계&#39;].values for i in range(len(tmp)): train[&#39;요일평균석식계&#39;][train[&#39;요일&#39;] == i] = tmp[i] . &#50900; &#54217;&#44512; &#49885;&#44228; . fig, ax = plt.subplots(1,2,figsize=(10,5)) sns.barplot(x=&#39;월&#39;, y=&#39;중식계&#39;, data=train, ax=ax[0]) sns.barplot(x=&#39;월&#39;, y=&#39;석식계&#39;, data=train, ax=ax[1]) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7ffb5cdc8cd0&gt; . train[&#39;월평균중식계&#39;] = 0 tmp = train.groupby(&#39;월&#39;).mean()[&#39;중식계&#39;] tmp_k = tmp.keys(); tmp_v = tmp.values for i in tmp_k: train[&#39;월평균중식계&#39;][train[&#39;월&#39;] == i] = tmp_v[i-1] . train[&#39;월평균석식계&#39;] = 0 tmp = train.groupby(&#39;월&#39;).mean()[&#39;석식계&#39;] tmp_k = tmp.keys(); tmp_v = tmp.values for i in tmp_k: train[&#39;월평균석식계&#39;][train[&#39;월&#39;] == i] = tmp_v[i-1] . &#44277;&#55092;&#51068; &#51204;&#54980; . train[&#39;공휴일전후&#39;] = 0 train[&#39;공휴일전후&#39;][17] = 1 train[&#39;공휴일전후&#39;][3] = 1 train[&#39;공휴일전후&#39;][62] = 1 # train[&#39;공휴일전후&#39;][67] = 1 # train[&#39;공휴일전후&#39;][82] = 1 train[&#39;공휴일전후&#39;][131] = 1 # train[&#39;공휴일전후&#39;][130] = 1 train[&#39;공휴일전후&#39;][152] = 1 train[&#39;공휴일전후&#39;][226] = 1 train[&#39;공휴일전후&#39;][221] = 1 train[&#39;공휴일전후&#39;][224] = 1 # train[&#39;공휴일전후&#39;][244] = 1 train[&#39;공휴일전후&#39;][245] = 1 # train[&#39;공휴일전후&#39;][267] = 1 train[&#39;공휴일전후&#39;][310] = 1 train[&#39;공휴일전후&#39;][311] = 1 train[&#39;공휴일전후&#39;][309] = 1 train[&#39;공휴일전후&#39;][330] = 1 train[&#39;공휴일전후&#39;][379] = 1 train[&#39;공휴일전후&#39;][467] = 1 # train[&#39;공휴일전후&#39;][469] = 1 train[&#39;공휴일전후&#39;][470] = 1 train[&#39;공휴일전후&#39;][502] = 1 # train[&#39;공휴일전후&#39;][501] = 1 # train[&#39;공휴일전후&#39;][511] = 1 train[&#39;공휴일전후&#39;][565] = 1 train[&#39;공휴일전후&#39;][623] = 1 train[&#39;공휴일전후&#39;][651] = 1 # train[&#39;공휴일전후&#39;][650] = 1 train[&#39;공휴일전후&#39;][705] = 1 # train[&#39;공휴일전후&#39;][707] = 1 train[&#39;공휴일전후&#39;][709] = 1 # train[&#39;공휴일전후&#39;][733] = 1 # train[&#39;공휴일전후&#39;][748] = 1 # train[&#39;공휴일전후&#39;][792] = 1 train[&#39;공휴일전후&#39;][815] = 1 train[&#39;공휴일전후&#39;][864] = 1 # train[&#39;공휴일전후&#39;][863] = 1 train[&#39;공휴일전후&#39;][950] = 1 train[&#39;공휴일전후&#39;][951] = 1 train[&#39;공휴일전후&#39;][953] = 1 train[&#39;공휴일전후&#39;][954] = 1 train[&#39;공휴일전후&#39;][955] = 1 train[&#39;공휴일전후&#39;][971] = 1 # train[&#39;공휴일전후&#39;][970] = 1 # train[&#39;공휴일전후&#39;][1037] = 1 train[&#39;공휴일전후&#39;][1038] = 1 train[&#39;공휴일전후&#39;][1099] = 1 train[&#39;공휴일전후&#39;][1129] = 1 # train[&#39;공휴일전후&#39;][1128] = 1 train[&#39;공휴일전후&#39;][1187] = 1 # train[&#39;공휴일전후&#39;][1186] = 1 . &#51088;&#44592;&#44228;&#48156;&#51032; &#45216; . train[&#39;자기계발의날&#39;] = 0 train[&#39;자기계발의날&#39;].loc[(train[&#39;석식계&#39;] == 0) &amp; (train[&#39;공휴일전후&#39;] == 0) &amp; (train[&#39;요일&#39;] == 2)] = 1 . 석식이 없고, 공휴일 전휴가 아닌 수요일을 자기계발의 날로 설정 . &#53580;&#49828;&#53944; &#45936;&#51060;&#53552; &#51204;&#52376;&#47532; . &#49885;&#49324;&#44032;&#45733;&#51088; . test[&#39;식사가능자&#39;] = test[&#39;본사정원수&#39;] - test[&#39;본사휴가자수&#39;] - test[&#39;본사출장자수&#39;] - test[&#39;현본사소속재택근무자수&#39;] . &#50836;&#51068;&#54217;&#44512;/ &#50900;&#54217;&#44512; &#49885;&#44228; . def make_dow_avg(df_test, df_prev): dow_avg_ln = df_prev.groupby(&#39;요일&#39;).mean()[&#39;중식계&#39;] dow_avg_dn = df_prev.groupby(&#39;요일&#39;).mean()[&#39;석식계&#39;] df_test[&#39;요일평균중식계&#39;] = 0 tmp = dow_avg_ln.values for i in range(len(tmp)): df_test[&#39;요일평균중식계&#39;][df_test[&#39;요일&#39;] == i] = tmp[i] df_test[&#39;요일평균석식계&#39;] = 0 tmp = dow_avg_dn.values for i in range(len(tmp)): df_test[&#39;요일평균석식계&#39;][df_test[&#39;요일&#39;] == i] = tmp[i] return df_test def make_month_avg(df_test, df_prev): month_avg_ln = df_prev.groupby(&#39;월&#39;).mean()[&#39;중식계&#39;] month_avg_dn = df_prev.groupby(&#39;월&#39;).mean()[&#39;석식계&#39;] df_test[&#39;월평균중식계&#39;] = 0 tmp = month_avg_ln tmp_k = tmp.keys(); tmp_v = tmp.values for i in tmp_k: df_test[&#39;월평균중식계&#39;][df_test[&#39;월&#39;] == i] = tmp_v[i-1] df_test[&#39;월평균석식계&#39;] = 0 tmp = month_avg_dn tmp_k = tmp.keys(); tmp_v = tmp.values for i in tmp_k: df_test[&#39;월평균석식계&#39;][df_test[&#39;월&#39;] == i] = tmp_v[i-1] return df_test . &#44277;&#55092;&#51068; &#51204;&#54980; . test[&#39;공휴일전후&#39;] = 0 test[&#39;공휴일전후&#39;][10] =1 test[&#39;공휴일전후&#39;][20] = 1 . train.columns . Index([&#39;일자&#39;, &#39;요일&#39;, &#39;본사정원수&#39;, &#39;본사휴가자수&#39;, &#39;본사출장자수&#39;, &#39;본사시간외근무명령서승인건수&#39;, &#39;현본사소속재택근무자수&#39;, &#39;조식메뉴&#39;, &#39;중식메뉴&#39;, &#39;석식메뉴&#39;, &#39;중식계&#39;, &#39;석식계&#39;, &#39;년&#39;, &#39;월&#39;, &#39;일&#39;, &#39;식사가능자&#39;, &#39;전주중식계&#39;, &#39;전주석식계&#39;, &#39;요일평균중식계&#39;, &#39;요일평균석식계&#39;, &#39;월평균중식계&#39;, &#39;월평균석식계&#39;, &#39;공휴일전후&#39;, &#39;자기계발의날&#39;], dtype=&#39;object&#39;) . test.columns . Index([&#39;일자&#39;, &#39;요일&#39;, &#39;본사정원수&#39;, &#39;본사휴가자수&#39;, &#39;본사출장자수&#39;, &#39;본사시간외근무명령서승인건수&#39;, &#39;현본사소속재택근무자수&#39;, &#39;조식메뉴&#39;, &#39;중식메뉴&#39;, &#39;석식메뉴&#39;, &#39;년&#39;, &#39;월&#39;, &#39;일&#39;, &#39;식사가능자&#39;, &#39;공휴일전후&#39;], dtype=&#39;object&#39;) . &#47784;&#45944; . &#52395; &#51452; &#50696;&#52769; . submission_df = pd.read_csv(path+&#39;sample_submission.csv&#39;) . res = [] X_test = test[[&#39;요일&#39;, &#39;본사시간외근무명령서승인건수&#39;, &#39;공휴일전후&#39;, &#39;월&#39;, &#39;일자&#39;, &#39;일&#39;, &#39;년&#39;, &#39;본사휴가자수&#39;, &#39;본사출장자수&#39;, &#39;식사가능자&#39;]][:5] . from datetime import timedelta X_test = make_dow_avg(X_test, train) X_test = make_month_avg(X_test, train) X_test[&#39;전주중식계&#39;] = 0 X_test[&#39;전주석식계&#39;] = 0 index = pd.DatetimeIndex(X_test[&#39;일자&#39;]) + timedelta(weeks=-1) for i in range(5): X_test[&#39;전주중식계&#39;][i] = train[train[&#39;일자&#39;] == str(index[i])[:10]][&#39;중식계&#39;] X_test[&#39;전주석식계&#39;][i] = train[train[&#39;일자&#39;] == str(index[i])[:10]][&#39;석식계&#39;] . &#52395; &#51452; &#51473;&#49885;&#44228; . from pycaret.regression import * . X_train_ln = train[[&#39;요일&#39;, &#39;본사시간외근무명령서승인건수&#39;, &#39;전주중식계&#39;, &#39;요일평균중식계&#39;, &#39;월평균중식계&#39;, &#39;공휴일전후&#39;, &#39;본사휴가자수&#39;, &#39;본사출장자수&#39;, &#39;식사가능자&#39;, &#39;중식계&#39;]] . reg = setup(session_id=1, data=X_train_ln, target=&#39;중식계&#39;, #numeric_imputation = &#39;mean&#39;, normalize = True, #categorical_features=[&#39;월&#39;, &#39;요일&#39;, &#39;공휴일전후&#39;], silent=True) . Description Value . 0 session_id | 1 | . 1 Target | 중식계 | . 2 Original Data | (1205, 10) | . 3 Missing Values | True | . 4 Numeric Features | 7 | . 5 Categorical Features | 2 | . 6 Ordinal Features | False | . 7 High Cardinality Features | False | . 8 High Cardinality Method | None | . 9 Transformed Train Set | (843, 13) | . 10 Transformed Test Set | (362, 13) | . 11 Shuffle Train-Test | True | . 12 Stratify Train-Test | False | . 13 Fold Generator | KFold | . 14 Fold Number | 10 | . 15 CPU Jobs | -1 | . 16 Use GPU | False | . 17 Log Experiment | False | . 18 Experiment Name | reg-default-name | . 19 USI | 193f | . 20 Imputation Type | simple | . 21 Iterative Imputation Iteration | None | . 22 Numeric Imputer | mean | . 23 Iterative Imputation Numeric Model | None | . 24 Categorical Imputer | constant | . 25 Iterative Imputation Categorical Model | None | . 26 Unknown Categoricals Handling | least_frequent | . 27 Normalize | True | . 28 Normalize Method | zscore | . 29 Transformation | False | . 30 Transformation Method | None | . 31 PCA | False | . 32 PCA Method | None | . 33 PCA Components | None | . 34 Ignore Low Variance | False | . 35 Combine Rare Levels | False | . 36 Rare Level Threshold | None | . 37 Numeric Binning | False | . 38 Remove Outliers | False | . 39 Outliers Threshold | None | . 40 Remove Multicollinearity | False | . 41 Multicollinearity Threshold | None | . 42 Remove Perfect Collinearity | True | . 43 Clustering | False | . 44 Clustering Iteration | None | . 45 Polynomial Features | False | . 46 Polynomial Degree | None | . 47 Trignometry Features | False | . 48 Polynomial Threshold | None | . 49 Group Features | False | . 50 Feature Selection | False | . 51 Feature Selection Method | classic | . 52 Features Selection Threshold | None | . 53 Feature Interaction | False | . 54 Feature Ratio | False | . 55 Interaction Threshold | None | . 56 Transform Target | False | . 57 Transform Target Method | box-cox | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; regression example 모델 생성, 추후에 필요한 모든 환경을 initialize . gbr_l = tune_model(create_model(&#39;gbr&#39;, criterion=&#39;mae&#39;), optimize=&#39;MAE&#39;) . MAE MSE RMSE R2 RMSLE MAPE . 0 88.0361 | 13281.0895 | 115.2436 | 0.7322 | 0.1487 | 0.1080 | . 1 77.5321 | 11508.5195 | 107.2778 | 0.6667 | 0.1162 | 0.0850 | . 2 66.4988 | 7948.7003 | 89.1555 | 0.8111 | 0.1101 | 0.0821 | . 3 58.9143 | 6671.9290 | 81.6819 | 0.8588 | 0.1073 | 0.0775 | . 4 59.4015 | 5754.7130 | 75.8598 | 0.8542 | 0.0857 | 0.0667 | . 5 65.1363 | 8964.1559 | 94.6792 | 0.8017 | 0.1220 | 0.0806 | . 6 80.2396 | 12571.9561 | 112.1247 | 0.7166 | 0.1264 | 0.0920 | . 7 68.3173 | 6871.6142 | 82.8952 | 0.7904 | 0.0964 | 0.0786 | . 8 74.3584 | 9514.7625 | 97.5436 | 0.8046 | 0.1092 | 0.0861 | . 9 72.6869 | 9897.9667 | 99.4885 | 0.8170 | 0.1430 | 0.0969 | . Mean 71.1121 | 9298.5407 | 95.5950 | 0.7853 | 0.1165 | 0.0853 | . SD 8.8017 | 2432.0294 | 12.6546 | 0.0584 | 0.0184 | 0.0109 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; Gradient Boosting Regressor. 기본적으로 fold = 10 이므로 10번 반복하며 모델을 tuning . pred_holdouts = predict_model(gbr_l) final_model_l = finalize_model(gbr_l) final_model_l . Model MAE MSE RMSE R2 RMSLE MAPE . 0 Gradient Boosting Regressor | 68.1824 | 8365.509 | 91.4632 | 0.8065 | 0.1156 | 0.0851 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; GradientBoostingRegressor(alpha=0.9, ccp_alpha=0.0, criterion=&#39;mae&#39;, init=None, learning_rate=0.05, loss=&#39;ls&#39;, max_depth=4, max_features=&#39;sqrt&#39;, max_leaf_nodes=None, min_impurity_decrease=0.05, min_impurity_split=None, min_samples_leaf=2, min_samples_split=4, min_weight_fraction_leaf=0.0, n_estimators=260, n_iter_no_change=None, presort=&#39;deprecated&#39;, random_state=1, subsample=0.8, tol=0.0001, validation_fraction=0.1, verbose=0, warm_start=False) . predict 에서는 데이터를 나누지 않고 마지막으로 데이터 전체를 학습 . pred_ln = predict_model(final_model_l, X_test[[&#39;요일&#39;, &#39;본사시간외근무명령서승인건수&#39;, &#39;전주중식계&#39;, &#39;요일평균중식계&#39;, &#39;월평균중식계&#39;, &#39;공휴일전후&#39;,&#39;본사휴가자수&#39;, &#39;본사출장자수&#39;, &#39;식사가능자&#39;]]) . submission_df[&#39;중식계&#39;][:5] = pred_ln[&#39;Label&#39;] . &#52395; &#51452; &#49437;&#49885;&#44228; . X_train_dn = train[[&#39;요일&#39;, &#39;본사시간외근무명령서승인건수&#39;, &#39;전주석식계&#39;, &#39;요일평균석식계&#39;, &#39;월평균석식계&#39;, &#39;공휴일전후&#39;, &#39;본사휴가자수&#39;, &#39;본사출장자수&#39;, &#39;식사가능자&#39;, &#39;석식계&#39;]] . reg = setup(session_id=2, data=X_train_dn, target=&#39;석식계&#39;, #numeric_imputation = &#39;mean&#39;, normalize = True, #categorical_features=[&#39;월&#39;, &#39;요일&#39;, &#39;공휴일전후&#39;], silent=True) . Description Value . 0 session_id | 2 | . 1 Target | 석식계 | . 2 Original Data | (1205, 10) | . 3 Missing Values | True | . 4 Numeric Features | 7 | . 5 Categorical Features | 2 | . 6 Ordinal Features | False | . 7 High Cardinality Features | False | . 8 High Cardinality Method | None | . 9 Transformed Train Set | (843, 13) | . 10 Transformed Test Set | (362, 13) | . 11 Shuffle Train-Test | True | . 12 Stratify Train-Test | False | . 13 Fold Generator | KFold | . 14 Fold Number | 10 | . 15 CPU Jobs | -1 | . 16 Use GPU | False | . 17 Log Experiment | False | . 18 Experiment Name | reg-default-name | . 19 USI | b39f | . 20 Imputation Type | simple | . 21 Iterative Imputation Iteration | None | . 22 Numeric Imputer | mean | . 23 Iterative Imputation Numeric Model | None | . 24 Categorical Imputer | constant | . 25 Iterative Imputation Categorical Model | None | . 26 Unknown Categoricals Handling | least_frequent | . 27 Normalize | True | . 28 Normalize Method | zscore | . 29 Transformation | False | . 30 Transformation Method | None | . 31 PCA | False | . 32 PCA Method | None | . 33 PCA Components | None | . 34 Ignore Low Variance | False | . 35 Combine Rare Levels | False | . 36 Rare Level Threshold | None | . 37 Numeric Binning | False | . 38 Remove Outliers | False | . 39 Outliers Threshold | None | . 40 Remove Multicollinearity | False | . 41 Multicollinearity Threshold | None | . 42 Remove Perfect Collinearity | True | . 43 Clustering | False | . 44 Clustering Iteration | None | . 45 Polynomial Features | False | . 46 Polynomial Degree | None | . 47 Trignometry Features | False | . 48 Polynomial Threshold | None | . 49 Group Features | False | . 50 Feature Selection | False | . 51 Feature Selection Method | classic | . 52 Features Selection Threshold | None | . 53 Feature Interaction | False | . 54 Feature Ratio | False | . 55 Interaction Threshold | None | . 56 Transform Target | False | . 57 Transform Target Method | box-cox | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; top5 = compare_models(n_select=5, sort=&#39;MAE&#39;) . Model MAE MSE RMSE R2 RMSLE MAPE TT (Sec) . et Extra Trees Regressor | 67.2353 | 1.016065e+04 | 100.4543 | 0.4401 | 1.0605 | 0.1328 | 0.580 | . rf Random Forest Regressor | 67.5529 | 1.012148e+04 | 100.1916 | 0.4550 | 1.0544 | 0.1333 | 0.645 | . gbr Gradient Boosting Regressor | 68.3785 | 1.005702e+04 | 99.9791 | 0.4546 | 1.0550 | 0.1359 | 0.131 | . huber Huber Regressor | 68.6161 | 1.101610e+04 | 104.2709 | 0.4158 | 1.0776 | 0.1341 | 0.026 | . lightgbm Light Gradient Boosting Machine | 70.3794 | 1.043318e+04 | 101.8196 | 0.4358 | 1.0502 | 0.1405 | 0.109 | . par Passive Aggressive Regressor | 70.3872 | 1.156027e+04 | 106.8342 | 0.3827 | 1.0800 | 0.1381 | 0.016 | . knn K Neighbors Regressor | 70.3970 | 1.049137e+04 | 102.1356 | 0.4320 | 1.0356 | 0.1470 | 0.061 | . lasso Lasso Regression | 71.4794 | 1.060829e+04 | 102.5668 | 0.4299 | 1.0637 | 0.1418 | 0.016 | . ridge Ridge Regression | 71.6534 | 1.057110e+04 | 102.4098 | 0.4283 | 1.0601 | 0.1420 | 0.016 | . lr Linear Regression | 71.6551 | 1.058428e+04 | 102.4765 | 0.4269 | 1.0600 | 0.1420 | 0.014 | . br Bayesian Ridge | 71.7560 | 1.058165e+04 | 102.4526 | 0.4297 | 1.0616 | 0.1426 | 0.028 | . en Elastic Net | 72.6006 | 1.154428e+04 | 106.7876 | 0.3928 | 1.0797 | 0.1457 | 0.016 | . llar Lasso Least Angle Regression | 81.3040 | 1.344913e+04 | 115.2241 | 0.2946 | 1.0923 | 0.1667 | 0.014 | . omp Orthogonal Matching Pursuit | 82.3024 | 1.304200e+04 | 113.7136 | 0.3077 | 1.0797 | 0.1702 | 0.014 | . ada AdaBoost Regressor | 83.8281 | 1.206659e+04 | 109.8245 | 0.3334 | 1.0224 | 0.1773 | 0.069 | . dt Decision Tree Regressor | 92.1222 | 1.914752e+04 | 138.0777 | -0.0605 | 1.4931 | 0.1976 | 0.019 | . dummy Dummy Regressor | 99.5370 | 1.930502e+04 | 137.9925 | -0.0092 | 1.1259 | 0.2086 | 0.012 | . lar Least Angle Regression | 31046.0929 | 1.479403e+10 | 38584.0678 | -667999.9297 | 1.6268 | 75.6116 | 0.017 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; blended_d = blend_models(top5, optimize=&#39;MAE&#39;) . MAE MSE RMSE R2 RMSLE MAPE . 0 68.8583 | 12045.4536 | 109.7518 | 0.4561 | 1.3143 | 0.1384 | . 1 63.8190 | 8483.8064 | 92.1076 | 0.2136 | 0.2381 | 0.1442 | . 2 66.1401 | 10699.6897 | 103.4393 | 0.4570 | 1.1357 | 0.1155 | . 3 63.8178 | 7727.3049 | 87.9051 | 0.5118 | 0.8896 | 0.1348 | . 4 60.9215 | 8427.4700 | 91.8013 | 0.5329 | 0.9361 | 0.1215 | . 5 61.2026 | 8246.7339 | 90.8115 | 0.4568 | 0.9186 | 0.1212 | . 6 69.0003 | 10058.5721 | 100.2924 | 0.5602 | 1.2597 | 0.1349 | . 7 60.5693 | 7021.4895 | 83.7943 | 0.6355 | 1.0583 | 0.1227 | . 8 66.7703 | 9515.9061 | 97.5495 | 0.5450 | 1.1022 | 0.1295 | . 9 67.0743 | 12253.3851 | 110.6950 | 0.5349 | 1.6850 | 0.0987 | . Mean 64.8174 | 9447.9811 | 96.8148 | 0.4904 | 1.0538 | 0.1262 | . SD 3.0458 | 1697.7639 | 8.6533 | 0.1064 | 0.3524 | 0.0125 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; pred_holdouts = predict_model(blended_d) final_model_d = finalize_model(blended_d) final_model_d . Model MAE MSE RMSE R2 RMSLE MAPE . 0 Voting Regressor | 65.969 | 10086.1659 | 100.4299 | 0.4811 | 1.1289 | 0.1303 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; VotingRegressor(estimators=[(&#39;et&#39;, ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion=&#39;mse&#39;, max_depth=None, max_features=&#39;auto&#39;, max_leaf_nodes=None, max_samples=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=-1, oob_score=False, random_state=2, ve... class_weight=None, colsample_bytree=1.0, importance_type=&#39;split&#39;, learning_rate=0.1, max_depth=-1, min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0, n_estimators=100, n_jobs=-1, num_leaves=31, objective=None, random_state=2, reg_alpha=0.0, reg_lambda=0.0, silent=&#39;warn&#39;, subsample=1.0, subsample_for_bin=200000, subsample_freq=0))], n_jobs=-1, verbose=False, weights=None) . pred_dn = predict_model(final_model_d, X_test[[&#39;요일&#39;, &#39;본사시간외근무명령서승인건수&#39;, &#39;전주석식계&#39;, &#39;요일평균석식계&#39;, &#39;월평균석식계&#39;, &#39;공휴일전후&#39;,&#39;본사휴가자수&#39;, &#39;본사출장자수&#39;, &#39;식사가능자&#39;]]) . submission_df[&#39;석식계&#39;][:5] = pred_dn[&#39;Label&#39;] . &#45208;&#47672;&#51648; &#51452; &#50696;&#52769; . res = [] X_test = test[[&#39;일자&#39;, &#39;요일&#39;, &#39;본사시간외근무명령서승인건수&#39;, &#39;공휴일전후&#39;, &#39;본사출장자수&#39;, &#39;본사휴가자수&#39;, &#39;식사가능자&#39;, &#39;월&#39;, &#39;일&#39;]] X_test[&quot;중식계&quot;] = 0 for i in range(len(X_test)): if i%5 == 0: res.append(X_test[i:i+5]) . res = res[1:] . &#45208;&#47672;&#51648; &#51452; &#51473;&#49885;&#44228; . X_train_ln = train[[&#39;요일&#39;, &#39;본사시간외근무명령서승인건수&#39;, &#39;요일평균중식계&#39;, &#39;월평균중식계&#39;, &#39;공휴일전후&#39;, &#39;본사휴가자수&#39;, &#39;본사출장자수&#39;, &#39;식사가능자&#39;, &#39;중식계&#39;]] . reg = setup(session_id=1, data=X_train_ln, target=&#39;중식계&#39;, #numeric_imputation = &#39;mean&#39;, normalize = True, #categorical_features=[&#39;월&#39;, &#39;요일&#39;, &#39;공휴일전후&#39;], silent=True) . Description Value . 0 session_id | 1 | . 1 Target | 중식계 | . 2 Original Data | (1205, 9) | . 3 Missing Values | False | . 4 Numeric Features | 6 | . 5 Categorical Features | 2 | . 6 Ordinal Features | False | . 7 High Cardinality Features | False | . 8 High Cardinality Method | None | . 9 Transformed Train Set | (843, 12) | . 10 Transformed Test Set | (362, 12) | . 11 Shuffle Train-Test | True | . 12 Stratify Train-Test | False | . 13 Fold Generator | KFold | . 14 Fold Number | 10 | . 15 CPU Jobs | -1 | . 16 Use GPU | False | . 17 Log Experiment | False | . 18 Experiment Name | reg-default-name | . 19 USI | 09bd | . 20 Imputation Type | simple | . 21 Iterative Imputation Iteration | None | . 22 Numeric Imputer | mean | . 23 Iterative Imputation Numeric Model | None | . 24 Categorical Imputer | constant | . 25 Iterative Imputation Categorical Model | None | . 26 Unknown Categoricals Handling | least_frequent | . 27 Normalize | True | . 28 Normalize Method | zscore | . 29 Transformation | False | . 30 Transformation Method | None | . 31 PCA | False | . 32 PCA Method | None | . 33 PCA Components | None | . 34 Ignore Low Variance | False | . 35 Combine Rare Levels | False | . 36 Rare Level Threshold | None | . 37 Numeric Binning | False | . 38 Remove Outliers | False | . 39 Outliers Threshold | None | . 40 Remove Multicollinearity | False | . 41 Multicollinearity Threshold | None | . 42 Remove Perfect Collinearity | True | . 43 Clustering | False | . 44 Clustering Iteration | None | . 45 Polynomial Features | False | . 46 Polynomial Degree | None | . 47 Trignometry Features | False | . 48 Polynomial Threshold | None | . 49 Group Features | False | . 50 Feature Selection | False | . 51 Feature Selection Method | classic | . 52 Features Selection Threshold | None | . 53 Feature Interaction | False | . 54 Feature Ratio | False | . 55 Interaction Threshold | None | . 56 Transform Target | False | . 57 Transform Target Method | box-cox | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; gbr_l = tune_model(create_model(&#39;gbr&#39;, criterion=&#39;mae&#39;), optimize=&#39;MAE&#39;) . MAE MSE RMSE R2 RMSLE MAPE . 0 88.2727 | 13746.7424 | 117.2465 | 0.7228 | 0.1524 | 0.1099 | . 1 82.7647 | 12924.7530 | 113.6871 | 0.6256 | 0.1237 | 0.0907 | . 2 66.3824 | 7994.5610 | 89.4123 | 0.8100 | 0.1121 | 0.0826 | . 3 62.3398 | 7374.8927 | 85.8772 | 0.8439 | 0.1218 | 0.0856 | . 4 59.5834 | 5892.5583 | 76.7630 | 0.8507 | 0.0885 | 0.0673 | . 5 69.2123 | 9104.5558 | 95.4178 | 0.7986 | 0.1243 | 0.0858 | . 6 75.4561 | 11448.2869 | 106.9967 | 0.7419 | 0.1211 | 0.0869 | . 7 71.9898 | 7911.8279 | 88.9485 | 0.7586 | 0.1041 | 0.0832 | . 8 70.9892 | 8850.8715 | 94.0791 | 0.8182 | 0.1081 | 0.0836 | . 9 70.2892 | 9311.9494 | 96.4984 | 0.8278 | 0.1409 | 0.0952 | . Mean 71.7280 | 9456.0999 | 96.4927 | 0.7798 | 0.1197 | 0.0871 | . SD 8.2623 | 2376.3789 | 12.0527 | 0.0656 | 0.0172 | 0.0102 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; pred_holdouts = predict_model(gbr_l) final_model_l = finalize_model(gbr_l) final_model_l . Model MAE MSE RMSE R2 RMSLE MAPE . 0 Gradient Boosting Regressor | 70.9156 | 9193.5047 | 95.8828 | 0.7873 | 0.12 | 0.0884 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; GradientBoostingRegressor(alpha=0.9, ccp_alpha=0.0, criterion=&#39;mae&#39;, init=None, learning_rate=0.05, loss=&#39;ls&#39;, max_depth=4, max_features=&#39;sqrt&#39;, max_leaf_nodes=None, min_impurity_decrease=0.05, min_impurity_split=None, min_samples_leaf=2, min_samples_split=4, min_weight_fraction_leaf=0.0, n_estimators=260, n_iter_no_change=None, presort=&#39;deprecated&#39;, random_state=1, subsample=0.8, tol=0.0001, validation_fraction=0.1, verbose=0, warm_start=False) . &#51473;&#49885; . from datetime import timedelta prev = train for idx, part_test in enumerate(res): part_test = make_dow_avg(part_test, prev) part_test = make_month_avg(part_test, prev) pred_ln = predict_model(final_model_l, part_test.drop([&#39;일자&#39;, &#39;월&#39;], axis=1)) submission_df[&#39;중식계&#39;][5*(idx+1):5*(idx+1)+5] = pred_ln[&#39;Label&#39;] . &#45208;&#47672;&#51648; &#51452; &#49437;&#49885;&#44228; . X_train_dn = train[[&#39;요일&#39;, &#39;본사시간외근무명령서승인건수&#39;, &#39;공휴일전후&#39;, &#39;요일평균석식계&#39;, &#39;월평균석식계&#39;, &#39;본사출장자수&#39;, &#39;식사가능자&#39;, &#39;월&#39;, &#39;일&#39;, &#39;석식계&#39;]] . reg = setup(session_id=2, data=X_train_dn, target=&#39;석식계&#39;, #numeric_imputation = &#39;mean&#39;, normalize = True, #categorical_features=[&#39;월&#39;, &#39;요일&#39;, &#39;공휴일전후&#39;], silent=True) . Description Value . 0 session_id | 2 | . 1 Target | 석식계 | . 2 Original Data | (1205, 10) | . 3 Missing Values | False | . 4 Numeric Features | 6 | . 5 Categorical Features | 3 | . 6 Ordinal Features | False | . 7 High Cardinality Features | False | . 8 High Cardinality Method | None | . 9 Transformed Train Set | (843, 24) | . 10 Transformed Test Set | (362, 24) | . 11 Shuffle Train-Test | True | . 12 Stratify Train-Test | False | . 13 Fold Generator | KFold | . 14 Fold Number | 10 | . 15 CPU Jobs | -1 | . 16 Use GPU | False | . 17 Log Experiment | False | . 18 Experiment Name | reg-default-name | . 19 USI | ee5f | . 20 Imputation Type | simple | . 21 Iterative Imputation Iteration | None | . 22 Numeric Imputer | mean | . 23 Iterative Imputation Numeric Model | None | . 24 Categorical Imputer | constant | . 25 Iterative Imputation Categorical Model | None | . 26 Unknown Categoricals Handling | least_frequent | . 27 Normalize | True | . 28 Normalize Method | zscore | . 29 Transformation | False | . 30 Transformation Method | None | . 31 PCA | False | . 32 PCA Method | None | . 33 PCA Components | None | . 34 Ignore Low Variance | False | . 35 Combine Rare Levels | False | . 36 Rare Level Threshold | None | . 37 Numeric Binning | False | . 38 Remove Outliers | False | . 39 Outliers Threshold | None | . 40 Remove Multicollinearity | False | . 41 Multicollinearity Threshold | None | . 42 Remove Perfect Collinearity | True | . 43 Clustering | False | . 44 Clustering Iteration | None | . 45 Polynomial Features | False | . 46 Polynomial Degree | None | . 47 Trignometry Features | False | . 48 Polynomial Threshold | None | . 49 Group Features | False | . 50 Feature Selection | False | . 51 Feature Selection Method | classic | . 52 Features Selection Threshold | None | . 53 Feature Interaction | False | . 54 Feature Ratio | False | . 55 Interaction Threshold | None | . 56 Transform Target | False | . 57 Transform Target Method | box-cox | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; top5 = compare_models(n_select=5, sort=&#39;MAE&#39;) . Model MAE MSE RMSE R2 RMSLE MAPE TT (Sec) . rf Random Forest Regressor | 60.2581 | 7575.4456 | 86.6464 | 0.5937 | 0.8682 | 0.1331 | 0.654 | . et Extra Trees Regressor | 61.8387 | 8157.4263 | 89.7794 | 0.5451 | 0.8934 | 0.1328 | 0.555 | . gbr Gradient Boosting Regressor | 62.4500 | 7491.1622 | 86.2427 | 0.5931 | 0.9307 | 0.1344 | 0.128 | . lightgbm Light Gradient Boosting Machine | 63.6084 | 7791.5239 | 87.9003 | 0.5776 | 0.8972 | 0.1410 | 0.064 | . knn K Neighbors Regressor | 71.6155 | 9926.4138 | 99.3647 | 0.4636 | 1.0003 | 0.1560 | 0.064 | . huber Huber Regressor | 72.1259 | 11552.9276 | 106.9963 | 0.3741 | 1.0768 | 0.1438 | 0.031 | . par Passive Aggressive Regressor | 73.7974 | 12126.8151 | 109.4854 | 0.3489 | 1.0844 | 0.1496 | 0.020 | . lasso Lasso Regression | 74.7545 | 10976.8838 | 104.5064 | 0.4026 | 1.0559 | 0.1525 | 0.017 | . lr Linear Regression | 75.6987 | 11059.7107 | 104.9258 | 0.3925 | 1.0509 | 0.1542 | 0.016 | . ridge Ridge Regression | 75.7466 | 11045.3426 | 104.8615 | 0.3944 | 1.0512 | 0.1545 | 0.016 | . ada AdaBoost Regressor | 75.8205 | 9925.3960 | 99.4202 | 0.4623 | 0.9777 | 0.1617 | 0.107 | . lar Least Angle Regression | 76.0398 | 11068.7819 | 104.9680 | 0.3920 | 1.0499 | 0.1552 | 0.020 | . br Bayesian Ridge | 76.1399 | 11113.0007 | 105.1838 | 0.3941 | 1.0543 | 0.1566 | 0.016 | . en Elastic Net | 77.1364 | 12122.3149 | 109.6086 | 0.3577 | 1.0754 | 0.1624 | 0.015 | . omp Orthogonal Matching Pursuit | 77.2797 | 12026.3925 | 109.2572 | 0.3583 | 1.0763 | 0.1580 | 0.015 | . dt Decision Tree Regressor | 80.9110 | 13798.0193 | 116.9324 | 0.2416 | 0.9675 | 0.1867 | 0.020 | . llar Lasso Least Angle Regression | 81.5116 | 13463.7665 | 115.2899 | 0.2939 | 1.0919 | 0.1676 | 0.016 | . dummy Dummy Regressor | 99.5370 | 19305.0209 | 137.9925 | -0.0092 | 1.1259 | 0.2086 | 0.013 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; blended_d = blend_models(top5, optimize=&#39;MAE&#39;) . MAE MSE RMSE R2 RMSLE MAPE . 0 63.4647 | 10312.7759 | 101.5518 | 0.5343 | 1.1519 | 0.1479 | . 1 60.6929 | 6627.2936 | 81.4082 | 0.3857 | 0.1872 | 0.1401 | . 2 60.0548 | 7270.8798 | 85.2695 | 0.6310 | 1.0255 | 0.1125 | . 3 59.0485 | 6023.2973 | 77.6099 | 0.6194 | 0.6935 | 0.1320 | . 4 58.3684 | 7171.5135 | 84.6848 | 0.6025 | 0.8754 | 0.1298 | . 5 54.8759 | 5725.9886 | 75.6703 | 0.6228 | 0.7918 | 0.1147 | . 6 61.0516 | 6180.1001 | 78.6136 | 0.7298 | 1.0631 | 0.1364 | . 7 59.5003 | 6149.9151 | 78.4214 | 0.6808 | 0.9731 | 0.1301 | . 8 64.1056 | 7325.7197 | 85.5904 | 0.6497 | 0.9554 | 0.1462 | . 9 61.0525 | 8735.7848 | 93.4654 | 0.6684 | 1.5694 | 0.1100 | . Mean 60.2215 | 7152.3268 | 84.2285 | 0.6124 | 0.9286 | 0.1300 | . SD 2.4727 | 1344.4377 | 7.6080 | 0.0900 | 0.3342 | 0.0129 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; pred_holdouts = predict_model(blended_d) final_model_d = finalize_model(blended_d) final_model_d . Model MAE MSE RMSE R2 RMSLE MAPE . 0 Voting Regressor | 63.0119 | 8294.7092 | 91.0753 | 0.5732 | 1.0245 | 0.1402 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; VotingRegressor(estimators=[(&#39;rf&#39;, RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion=&#39;mse&#39;, max_depth=None, max_features=&#39;auto&#39;, max_leaf_nodes=None, max_samples=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=-1, oob_score=False, random_state=2, v... min_split_gain=0.0, n_estimators=100, n_jobs=-1, num_leaves=31, objective=None, random_state=2, reg_alpha=0.0, reg_lambda=0.0, silent=&#39;warn&#39;, subsample=1.0, subsample_for_bin=200000, subsample_freq=0)), (&#39;knn&#39;, KNeighborsRegressor(algorithm=&#39;auto&#39;, leaf_size=30, metric=&#39;minkowski&#39;, metric_params=None, n_jobs=-1, n_neighbors=5, p=2, weights=&#39;uniform&#39;))], n_jobs=-1, verbose=False, weights=None) . &#49437;&#49885; . from datetime import timedelta prev = train for idx, part_test in enumerate(res): part_test = make_dow_avg(part_test, prev) part_test = make_month_avg(part_test, prev) pred_dn = predict_model(final_model_d, part_test.drop([&#39;일자&#39;], axis=1)) submission_df[&#39;석식계&#39;][5*(idx+1):5*(idx+1)+5] = pred_dn[&#39;Label&#39;] . Model &#48516;&#49437; . 모든 Model Prediction을 진행한 뒤, 저희가 만든 Model을 분석했습니다. 분석 도구로는 Pycaret의 plot_model을 사용했고, 이를 이용해 Model의 Residual 분포를 확인해보았습니다. . 중식: 중식계같은 경우 대부분 고르게 분포한 것처럼 보이지만 residual의 값이 양수로 치우쳐져 있는 것을 확인할 수 있습니다. 이에 따라 저희는 Model에서 얻은 최종 값에 먼저 내림을 통해 소수점을 모두 제거해준 뒤, 30을 추가로 더해 오차의 분포를 줄였습니다. | 석식: 석식계같은 경우는 대부분 고르게 분포한 것처럼 보이지만 residual의 값이 음수로 치우쳐져 있는 것을 확인할 수 있습니다. 이에 따라 저희는 Model에서 얻은 최종 값에 먼저 내림을 통해 소수점을 모두 제거해준 뒤, 30을 추가로 빼주어 오차의 분포를 줄였습니다. | . import cv2 img = cv2.imread(&#39;./Lunch_First.png&#39;) plt.figure(figsize=(15, 8)) plt.title(&quot;First Week Lunch Model&quot;) plt.imshow(img) . . img = cv2.imread(&#39;./Lunch_Another.png&#39;) plt.figure(figsize=(15, 8)) plt.title(&quot;Another Week Lunch Model&quot;) plt.imshow(img) . . img = cv2.imread(&#39;./Dinner_First.png&#39;) plt.figure(figsize=(15, 8)) plt.title(&quot;First Week Dinner Model&quot;) plt.imshow(img) . . img = cv2.imread(&quot;./Dinner_Another.png&quot;) plt.figure(figsize=(15, 8)) plt.title(&quot;Another Week Dinner Model&quot;) plt.imshow(img) . . submission_df[&quot;중식계&quot;] = submission_df[&quot;중식계&quot;] // 1 + 30 . submission_df[&quot;석식계&quot;] = submission_df[&quot;석식계&quot;] // 1 - 30 . submission_df.to_csv(&quot;./submission.csv&quot;, index=False) .",
            "url": "https://shw9807.github.io/shw9807blog/%EB%8D%B0%EC%9D%B4%EC%BD%98/2022/02/17/%EC%8B%9D%EC%88%98%EC%9D%B8%EC%9B%90_%EC%98%88%EC%B8%A1.html",
            "relUrl": "/%EB%8D%B0%EC%9D%B4%EC%BD%98/2022/02/17/%EC%8B%9D%EC%88%98%EC%9D%B8%EC%9B%90_%EC%98%88%EC%B8%A1.html",
            "date": " • Feb 17, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "[Colab] 신용카드 사용자 연체 예측 - 하이퍼 파라미터",
            "content": ". 하이퍼파라미터 튜닝에 쓰고 있는 최신 Automl 기법이다. | 빠르게 튜닝이 가능하다는 장점이 있다. | 하이퍼파라미터 튜닝 방식을 지정할수 있다. -&gt; 직관적인 api인 튜닝된 lightgbm도 제공해준다. | 다른 라이브러리들에 비해 직관적인 장점이 있어 코딩하기 용이하다. | . !pip install optuna . Collecting optuna Downloading optuna-2.10.0-py3-none-any.whl (308 kB) |████████████████████████████████| 308 kB 8.9 MB/s Collecting cmaes&gt;=0.8.2 Downloading cmaes-0.8.2-py3-none-any.whl (15 kB) Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from optuna) (1.19.5) Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from optuna) (4.62.3) Requirement already satisfied: scipy!=1.4.0 in /usr/local/lib/python3.7/dist-packages (from optuna) (1.4.1) Collecting alembic Downloading alembic-1.7.6-py3-none-any.whl (210 kB) |████████████████████████████████| 210 kB 49.2 MB/s Collecting cliff Downloading cliff-3.10.0-py3-none-any.whl (80 kB) |████████████████████████████████| 80 kB 8.8 MB/s Requirement already satisfied: packaging&gt;=20.0 in /usr/local/lib/python3.7/dist-packages (from optuna) (21.3) Collecting colorlog Downloading colorlog-6.6.0-py2.py3-none-any.whl (11 kB) Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from optuna) (3.13) Requirement already satisfied: sqlalchemy&gt;=1.1.0 in /usr/local/lib/python3.7/dist-packages (from optuna) (1.4.31) Requirement already satisfied: pyparsing!=3.0.5,&gt;=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging&gt;=20.0-&gt;optuna) (3.0.7) Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.7/dist-packages (from sqlalchemy&gt;=1.1.0-&gt;optuna) (1.1.2) Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from sqlalchemy&gt;=1.1.0-&gt;optuna) (4.10.1) Collecting Mako Downloading Mako-1.1.6-py2.py3-none-any.whl (75 kB) |████████████████████████████████| 75 kB 4.3 MB/s Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from alembic-&gt;optuna) (5.4.0) Collecting stevedore&gt;=2.0.1 Downloading stevedore-3.5.0-py3-none-any.whl (49 kB) |████████████████████████████████| 49 kB 5.8 MB/s Requirement already satisfied: PrettyTable&gt;=0.7.2 in /usr/local/lib/python3.7/dist-packages (from cliff-&gt;optuna) (3.0.0) Collecting autopage&gt;=0.4.0 Downloading autopage-0.5.0-py3-none-any.whl (29 kB) Collecting pbr!=2.1.0,&gt;=2.0.0 Downloading pbr-5.8.1-py2.py3-none-any.whl (113 kB) |████████████████████████████████| 113 kB 49.6 MB/s Collecting cmd2&gt;=1.0.0 Downloading cmd2-2.3.3-py3-none-any.whl (149 kB) |████████████████████████████████| 149 kB 47.6 MB/s Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from cmd2&gt;=1.0.0-&gt;cliff-&gt;optuna) (3.10.0.2) Requirement already satisfied: attrs&gt;=16.3.0 in /usr/local/lib/python3.7/dist-packages (from cmd2&gt;=1.0.0-&gt;cliff-&gt;optuna) (21.4.0) Collecting pyperclip&gt;=1.6 Downloading pyperclip-1.8.2.tar.gz (20 kB) Requirement already satisfied: wcwidth&gt;=0.1.7 in /usr/local/lib/python3.7/dist-packages (from cmd2&gt;=1.0.0-&gt;cliff-&gt;optuna) (0.2.5) Requirement already satisfied: zipp&gt;=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata-&gt;sqlalchemy&gt;=1.1.0-&gt;optuna) (3.7.0) Requirement already satisfied: MarkupSafe&gt;=0.9.2 in /usr/local/lib/python3.7/dist-packages (from Mako-&gt;alembic-&gt;optuna) (2.0.1) Building wheels for collected packages: pyperclip Building wheel for pyperclip (setup.py) ... done Created wheel for pyperclip: filename=pyperclip-1.8.2-py3-none-any.whl size=11137 sha256=29461bb9e67280164fa0d16e167bbc12687ae02b465ef8870baaa064a101a4df Stored in directory: /root/.cache/pip/wheels/9f/18/84/8f69f8b08169c7bae2dde6bd7daf0c19fca8c8e500ee620a28 Successfully built pyperclip Installing collected packages: pyperclip, pbr, stevedore, Mako, cmd2, autopage, colorlog, cmaes, cliff, alembic, optuna Successfully installed Mako-1.1.6 alembic-1.7.6 autopage-0.5.0 cliff-3.10.0 cmaes-0.8.2 cmd2-2.3.3 colorlog-6.6.0 optuna-2.10.0 pbr-5.8.1 pyperclip-1.8.2 stevedore-3.5.0 . import numpy as np import pandas as pd import optuna from lightgbm import LGBMClassifier from optuna import Trial from optuna.samplers import TPESampler from sklearn.metrics import log_loss from sklearn.model_selection import train_test_split . &#44036;&#45800;&#54620; &#51204;&#52376;&#47532; . path = &quot;/content/drive/MyDrive/Colab Notebooks/신용카드 연체예측/&quot; train = pd.read_csv(path + &quot;train.csv&quot;) train = train.drop([&quot;index&quot;], axis=1) train.fillna(&quot;NAN&quot;, inplace=True) test = pd.read_csv(path + &quot;test.csv&quot;) test = test.drop([&quot;index&quot;], axis=1) test.fillna(&quot;NAN&quot;, inplace=True) . train_ohe = pd.get_dummies(train) test_ohe = pd.get_dummies(test) . X = train_ohe.drop([&quot;credit&quot;], axis=1) y = train[&quot;credit&quot;] X_test = test_ohe.copy() . Optuna는 objective하이퍼 파라미터의 성능을 평가하고 향후 시험에서 샘플링 할 위치를 결정하기 위해 숫자 값을 반환 하는 함수가 필요하다는 것을 의미하는 블랙 박스 최적화 프로그램 이다. | Optuna의 특정 인수를 object에 전달된다. | trial는 조정해야하는 하이퍼 파라미터를 지정 하기 위해 objective 함수에 전달된다. | 이것은 logloss 성능에 대한 피드백으로 Optuna에서 사용하는 모델에서 반환한다. | . def objective(trial: Trial) -&gt; float: params_lgb = { &quot;random_state&quot;: 42, &quot;verbosity&quot;: -1, &quot;learning_rate&quot;: 0.05, &quot;n_estimators&quot;: 10000, &quot;objective&quot;: &quot;multiclass&quot;, &quot;metric&quot;: &quot;multi_logloss&quot;, &quot;reg_alpha&quot;: trial.suggest_float(&quot;reg_alpha&quot;, 1e-8, 3e-5), &quot;reg_lambda&quot;: trial.suggest_float(&quot;reg_lambda&quot;, 1e-8, 9e-2), &quot;max_depth&quot;: trial.suggest_int(&quot;max_depth&quot;, 1, 20), &quot;num_leaves&quot;: trial.suggest_int(&quot;num_leaves&quot;, 2, 256), &quot;colsample_bytree&quot;: trial.suggest_float(&quot;colsample_bytree&quot;, 0.4, 1.0), &quot;subsample&quot;: trial.suggest_float(&quot;subsample&quot;, 0.3, 1.0), &quot;subsample_freq&quot;: trial.suggest_int(&quot;subsample_freq&quot;, 1, 10), &quot;min_child_samples&quot;: trial.suggest_int(&quot;min_child_samples&quot;, 5, 100), &quot;max_bin&quot;: trial.suggest_int(&quot;max_bin&quot;, 200, 500), } X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2) model = LGBMClassifier(**params_lgb) model.fit( X_train, y_train, eval_set=[(X_train, y_train), (X_valid, y_valid)], early_stopping_rounds=100, verbose=False, ) lgb_pred = model.predict_proba(X_valid) log_score = log_loss(y_valid, lgb_pred) return log_score . sampler = TPESampler(seed=42) study = optuna.create_study( study_name=&quot;lgbm_parameter_opt&quot;, direction=&quot;minimize&quot;, sampler=sampler, ) study.optimize(objective, n_trials=10) print(&quot;Best Score:&quot;, study.best_value) print(&quot;Best trial:&quot;, study.best_trial.params) . [I 2022-02-10 08:39:17,315] A new study created in memory with name: lgbm_parameter_opt [I 2022-02-10 08:39:40,151] Trial 0 finished with value: 0.7390804911846213 and parameters: {&#39;reg_alpha&#39;: 1.12424581642324e-05, &#39;reg_lambda&#39;: 0.08556428806974939, &#39;max_depth&#39;: 15, &#39;num_leaves&#39;: 154, &#39;colsample_bytree&#39;: 0.4936111842654619, &#39;subsample&#39;: 0.40919616423534183, &#39;subsample_freq&#39;: 1, &#39;min_child_samples&#39;: 88, &#39;max_bin&#39;: 380}. Best is trial 0 with value: 0.7390804911846213. [I 2022-02-10 08:39:52,191] Trial 1 finished with value: 0.7319514638826676 and parameters: {&#39;reg_alpha&#39;: 2.1245096608103405e-05, &#39;reg_lambda&#39;: 0.0018526142807772773, &#39;max_depth&#39;: 20, &#39;num_leaves&#39;: 214, &#39;colsample_bytree&#39;: 0.5274034664069657, &#39;subsample&#39;: 0.42727747704497043, &#39;subsample_freq&#39;: 2, &#39;min_child_samples&#39;: 34, &#39;max_bin&#39;: 357}. Best is trial 1 with value: 0.7319514638826676. [I 2022-02-10 08:40:08,554] Trial 2 finished with value: 0.7463491007324939 and parameters: {&#39;reg_alpha&#39;: 1.2964031109077052e-05, &#39;reg_lambda&#39;: 0.02621062970553237, &#39;max_depth&#39;: 13, &#39;num_leaves&#39;: 37, &#39;colsample_bytree&#39;: 0.5752867891211308, &#39;subsample&#39;: 0.5564532903055841, &#39;subsample_freq&#39;: 5, &#39;min_child_samples&#39;: 80, &#39;max_bin&#39;: 260}. Best is trial 1 with value: 0.7319514638826676. [I 2022-02-10 08:40:12,406] Trial 3 finished with value: 0.7989439848841935 and parameters: {&#39;reg_alpha&#39;: 1.5431890808024213e-05, &#39;reg_lambda&#39;: 0.05331731527343814, &#39;max_depth&#39;: 1, &#39;num_leaves&#39;: 156, &#39;colsample_bytree&#39;: 0.502314474212375, &#39;subsample&#39;: 0.3455361150896956, &#39;subsample_freq&#39;: 10, &#39;min_child_samples&#39;: 97, &#39;max_bin&#39;: 443}. Best is trial 1 with value: 0.7319514638826676. [I 2022-02-10 08:40:24,877] Trial 4 finished with value: 0.741057522983838 and parameters: {&#39;reg_alpha&#39;: 9.145366937509386e-06, &#39;reg_lambda&#39;: 0.008790499283853408, &#39;max_depth&#39;: 14, &#39;num_leaves&#39;: 114, &#39;colsample_bytree&#39;: 0.47322294090686734, &#39;subsample&#39;: 0.6466238370778892, &#39;subsample_freq&#39;: 1, &#39;min_child_samples&#39;: 92, &#39;max_bin&#39;: 277}. Best is trial 1 with value: 0.7319514638826676. [I 2022-02-10 08:40:45,942] Trial 5 finished with value: 0.7234749185219674 and parameters: {&#39;reg_alpha&#39;: 1.987904330777592e-05, &#39;reg_lambda&#39;: 0.028054003730936226, &#39;max_depth&#39;: 11, &#39;num_leaves&#39;: 141, &#39;colsample_bytree&#39;: 0.5109126733153162, &#39;subsample&#39;: 0.9787092394351908, &#39;subsample_freq&#39;: 8, &#39;min_child_samples&#39;: 95, &#39;max_bin&#39;: 469}. Best is trial 5 with value: 0.7234749185219674. [I 2022-02-10 08:41:03,983] Trial 6 finished with value: 0.7871661703633702 and parameters: {&#39;reg_alpha&#39;: 1.7941020364544445e-05, &#39;reg_lambda&#39;: 0.08296868193333816, &#39;max_depth&#39;: 2, &#39;num_leaves&#39;: 51, &#39;colsample_bytree&#39;: 0.4271363733463229, &#39;subsample&#39;: 0.527731231534285, &#39;subsample_freq&#39;: 4, &#39;min_child_samples&#39;: 31, &#39;max_bin&#39;: 449}. Best is trial 5 with value: 0.7234749185219674. [I 2022-02-10 08:41:17,286] Trial 7 finished with value: 0.7658377791555869 and parameters: {&#39;reg_alpha&#39;: 1.0709032267540741e-05, &#39;reg_lambda&#39;: 0.025284113062519174, &#39;max_depth&#39;: 11, &#39;num_leaves&#39;: 37, &#39;colsample_bytree&#39;: 0.8813181884524238, &#39;subsample&#39;: 0.35218545057583955, &#39;subsample_freq&#39;: 10, &#39;min_child_samples&#39;: 79, &#39;max_bin&#39;: 259}. Best is trial 5 with value: 0.7234749185219674. [I 2022-02-10 08:41:28,373] Trial 8 finished with value: 0.7308900281549479 and parameters: {&#39;reg_alpha&#39;: 1.7560829253683595e-07, &#39;reg_lambda&#39;: 0.07339153040632079, &#39;max_depth&#39;: 15, &#39;num_leaves&#39;: 187, &#39;colsample_bytree&#39;: 0.8627622080115674, &#39;subsample&#39;: 0.35183125621386324, &#39;subsample_freq&#39;: 4, &#39;min_child_samples&#39;: 16, &#39;max_bin&#39;: 459}. Best is trial 5 with value: 0.7234749185219674. [I 2022-02-10 08:41:48,182] Trial 9 finished with value: 0.7859314706082935 and parameters: {&#39;reg_alpha&#39;: 1.8702710823558463e-05, &#39;reg_lambda&#39;: 0.02978082892775818, &#39;max_depth&#39;: 2, &#39;num_leaves&#39;: 81, &#39;colsample_bytree&#39;: 0.5951099932160482, &#39;subsample&#39;: 0.8107243248366449, &#39;subsample_freq&#39;: 7, &#39;min_child_samples&#39;: 90, &#39;max_bin&#39;: 342}. Best is trial 5 with value: 0.7234749185219674. . Best Score: 0.7234749185219674 Best trial: {&#39;reg_alpha&#39;: 1.987904330777592e-05, &#39;reg_lambda&#39;: 0.028054003730936226, &#39;max_depth&#39;: 11, &#39;num_leaves&#39;: 141, &#39;colsample_bytree&#39;: 0.5109126733153162, &#39;subsample&#39;: 0.9787092394351908, &#39;subsample_freq&#39;: 8, &#39;min_child_samples&#39;: 95, &#39;max_bin&#39;: 469} . optuna.visualization.plot_optimization_history(study) . . . optuna.visualization.plot_parallel_coordinate(study) . . . optuna.visualization.plot_contour( study, params=[ &quot;max_depth&quot;, &quot;num_leaves&quot;, &quot;colsample_bytree&quot;, &quot;subsample&quot;, &quot;subsample_freq&quot;, &quot;min_child_samples&quot;, &quot;max_bin&quot;, ], ) . . . optuna.visualization.plot_param_importances(study) . . . 하이퍼파라미터 중요도는 전체 파라미터들의 영향력의 총합을 1이라고 생각했을 때 해당 하이퍼파라미터가 어느정도의 영향력을 가지고 있는지를 나타내준다. min_child_samples의 경우에는 거의 0에 가까운 영향력을 가지고 있지만 과적합을 피하기 위해서는 사용하는 것이 좋다. .",
            "url": "https://shw9807.github.io/shw9807blog/%EB%8D%B0%EC%9D%B4%EC%BD%98/2022/02/10/%EC%8B%A0%EC%9A%A9%EC%B9%B4%EB%93%9C_%EC%97%B0%EC%B2%B4%EC%98%88%EC%B8%A1_%ED%95%98%EC%9D%B4%ED%8D%BC%ED%8C%8C%EB%9D%BC%EB%AF%B8%ED%84%B0.html",
            "relUrl": "/%EB%8D%B0%EC%9D%B4%EC%BD%98/2022/02/10/%EC%8B%A0%EC%9A%A9%EC%B9%B4%EB%93%9C_%EC%97%B0%EC%B2%B4%EC%98%88%EC%B8%A1_%ED%95%98%EC%9D%B4%ED%8D%BC%ED%8C%8C%EB%9D%BC%EB%AF%B8%ED%84%B0.html",
            "date": " • Feb 10, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "[Colab] 신용카드 사용자 연체 예측",
            "content": ". !pip uninstall -y typing # this should avoid AttributeError: type object &#39;Callable&#39; has no attribute &#39;_abc_registry&#39; !pip install &quot;git+https://github.com/dreamquark-ai/tabnet.git@develop#egg=pytorch_tabnet&quot; --upgrade . WARNING: Skipping typing as it is not installed. Collecting pytorch_tabnet Cloning https://github.com/dreamquark-ai/tabnet.git (to revision develop) to /tmp/pip-install-vk88_98j/pytorch-tabnet_c577cf56909f451bb57123369d3b06a6 Running command git clone -q https://github.com/dreamquark-ai/tabnet.git /tmp/pip-install-vk88_98j/pytorch-tabnet_c577cf56909f451bb57123369d3b06a6 Installing build dependencies ... done Getting requirements to build wheel ... done Preparing wheel metadata ... done Requirement already satisfied: numpy&lt;2.0,&gt;=1.17 in /usr/local/lib/python3.7/dist-packages (from pytorch_tabnet) (1.19.5) Requirement already satisfied: tqdm&lt;5.0,&gt;=4.36 in /usr/local/lib/python3.7/dist-packages (from pytorch_tabnet) (4.62.3) Requirement already satisfied: torch&lt;2.0,&gt;=1.2 in /usr/local/lib/python3.7/dist-packages (from pytorch_tabnet) (1.10.0+cu111) Requirement already satisfied: scipy&gt;1.4 in /usr/local/lib/python3.7/dist-packages (from pytorch_tabnet) (1.4.1) Requirement already satisfied: scikit_learn&gt;0.21 in /usr/local/lib/python3.7/dist-packages (from pytorch_tabnet) (1.0.2) Requirement already satisfied: threadpoolctl&gt;=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit_learn&gt;0.21-&gt;pytorch_tabnet) (3.1.0) Requirement already satisfied: joblib&gt;=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit_learn&gt;0.21-&gt;pytorch_tabnet) (1.1.0) Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch&lt;2.0,&gt;=1.2-&gt;pytorch_tabnet) (3.10.0.2) Building wheels for collected packages: pytorch-tabnet Building wheel for pytorch-tabnet (PEP 517) ... done Created wheel for pytorch-tabnet: filename=pytorch_tabnet-3.1.1-py3-none-any.whl size=40626 sha256=af83d1a6c0e84932cedc83c69a1c85784f8a4b338d964fd21f6b3cb160c9779d Stored in directory: /tmp/pip-ephem-wheel-cache-83a06ce3/wheels/a6/8e/aa/6f5ef6a2e389c8b5f7ea1c74bbb03ece8773b03c2b8955c334 Successfully built pytorch-tabnet Installing collected packages: pytorch-tabnet Successfully installed pytorch-tabnet-3.1.1 . TabNet : 정형 데이터에 적합한 딥러닝 모델 . 장점은 크게 4가지라고 함 . TabNet은 전처리 과정이 필요하지 않다. | Decision step을 통해 feature selection을 진행한다. | 위의 과정으로 인해 decision step별이나(local interpretability) 모델 전체의(global interpretability) feature importance를 수치화 할 수 있다. | 무작위로 가려진 feature 값을 예측하는 unsupervised pretrain 단계를 적용하여 상당한 성능 향상을 보여준다. | TabNet에 대한 자세한 설명:https://themore-dont-know.tistory.com/2 . !pip install catboost !pip install optuna . Collecting catboost Downloading catboost-1.0.4-cp37-none-manylinux1_x86_64.whl (76.1 MB) |████████████████████████████████| 76.1 MB 1.1 MB/s Requirement already satisfied: graphviz in /usr/local/lib/python3.7/dist-packages (from catboost) (0.10.1) Requirement already satisfied: numpy&gt;=1.16.0 in /usr/local/lib/python3.7/dist-packages (from catboost) (1.19.5) Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from catboost) (1.15.0) Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from catboost) (3.2.2) Requirement already satisfied: plotly in /usr/local/lib/python3.7/dist-packages (from catboost) (5.5.0) Requirement already satisfied: pandas&gt;=0.24.0 in /usr/local/lib/python3.7/dist-packages (from catboost) (1.3.5) Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from catboost) (1.4.1) Requirement already satisfied: pytz&gt;=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas&gt;=0.24.0-&gt;catboost) (2018.9) Requirement already satisfied: python-dateutil&gt;=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas&gt;=0.24.0-&gt;catboost) (2.8.2) Requirement already satisfied: kiwisolver&gt;=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib-&gt;catboost) (1.3.2) Requirement already satisfied: cycler&gt;=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib-&gt;catboost) (0.11.0) Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,&gt;=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib-&gt;catboost) (3.0.7) Requirement already satisfied: tenacity&gt;=6.2.0 in /usr/local/lib/python3.7/dist-packages (from plotly-&gt;catboost) (8.0.1) Installing collected packages: catboost Successfully installed catboost-1.0.4 Collecting optuna Downloading optuna-2.10.0-py3-none-any.whl (308 kB) |████████████████████████████████| 308 kB 7.7 MB/s Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from optuna) (3.13) Collecting alembic Downloading alembic-1.7.6-py3-none-any.whl (210 kB) |████████████████████████████████| 210 kB 51.3 MB/s Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from optuna) (4.62.3) Requirement already satisfied: scipy!=1.4.0 in /usr/local/lib/python3.7/dist-packages (from optuna) (1.4.1) Requirement already satisfied: sqlalchemy&gt;=1.1.0 in /usr/local/lib/python3.7/dist-packages (from optuna) (1.4.31) Requirement already satisfied: packaging&gt;=20.0 in /usr/local/lib/python3.7/dist-packages (from optuna) (21.3) Collecting cmaes&gt;=0.8.2 Downloading cmaes-0.8.2-py3-none-any.whl (15 kB) Collecting colorlog Downloading colorlog-6.6.0-py2.py3-none-any.whl (11 kB) Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from optuna) (1.19.5) Collecting cliff Downloading cliff-3.10.0-py3-none-any.whl (80 kB) |████████████████████████████████| 80 kB 9.5 MB/s Requirement already satisfied: pyparsing!=3.0.5,&gt;=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging&gt;=20.0-&gt;optuna) (3.0.7) Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from sqlalchemy&gt;=1.1.0-&gt;optuna) (4.10.1) Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.7/dist-packages (from sqlalchemy&gt;=1.1.0-&gt;optuna) (1.1.2) Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from alembic-&gt;optuna) (5.4.0) Collecting Mako Downloading Mako-1.1.6-py2.py3-none-any.whl (75 kB) |████████████████████████████████| 75 kB 4.7 MB/s Collecting pbr!=2.1.0,&gt;=2.0.0 Downloading pbr-5.8.1-py2.py3-none-any.whl (113 kB) |████████████████████████████████| 113 kB 56.1 MB/s Collecting cmd2&gt;=1.0.0 Downloading cmd2-2.3.3-py3-none-any.whl (149 kB) |████████████████████████████████| 149 kB 67.7 MB/s Collecting autopage&gt;=0.4.0 Downloading autopage-0.5.0-py3-none-any.whl (29 kB) Collecting stevedore&gt;=2.0.1 Downloading stevedore-3.5.0-py3-none-any.whl (49 kB) |████████████████████████████████| 49 kB 6.1 MB/s Requirement already satisfied: PrettyTable&gt;=0.7.2 in /usr/local/lib/python3.7/dist-packages (from cliff-&gt;optuna) (3.0.0) Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from cmd2&gt;=1.0.0-&gt;cliff-&gt;optuna) (3.10.0.2) Collecting pyperclip&gt;=1.6 Downloading pyperclip-1.8.2.tar.gz (20 kB) Requirement already satisfied: wcwidth&gt;=0.1.7 in /usr/local/lib/python3.7/dist-packages (from cmd2&gt;=1.0.0-&gt;cliff-&gt;optuna) (0.2.5) Requirement already satisfied: attrs&gt;=16.3.0 in /usr/local/lib/python3.7/dist-packages (from cmd2&gt;=1.0.0-&gt;cliff-&gt;optuna) (21.4.0) Requirement already satisfied: zipp&gt;=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata-&gt;sqlalchemy&gt;=1.1.0-&gt;optuna) (3.7.0) Requirement already satisfied: MarkupSafe&gt;=0.9.2 in /usr/local/lib/python3.7/dist-packages (from Mako-&gt;alembic-&gt;optuna) (2.0.1) Building wheels for collected packages: pyperclip Building wheel for pyperclip (setup.py) ... done Created wheel for pyperclip: filename=pyperclip-1.8.2-py3-none-any.whl size=11137 sha256=155c92cf927017f0697a47c617051424d4c7d9cb9bc502efdc02f27dd7fca2ab Stored in directory: /root/.cache/pip/wheels/9f/18/84/8f69f8b08169c7bae2dde6bd7daf0c19fca8c8e500ee620a28 Successfully built pyperclip Installing collected packages: pyperclip, pbr, stevedore, Mako, cmd2, autopage, colorlog, cmaes, cliff, alembic, optuna Successfully installed Mako-1.1.6 alembic-1.7.6 autopage-0.5.0 cliff-3.10.0 cmaes-0.8.2 cmd2-2.3.3 colorlog-6.6.0 optuna-2.10.0 pbr-5.8.1 pyperclip-1.8.2 stevedore-3.5.0 . from typing import Dict, Tuple, Union, List import numpy as np import pandas as pd import torch import catboost from catboost import CatBoostClassifier, Pool from lightgbm import LGBMClassifier from xgboost import XGBClassifier from pytorch_tabnet.multitask import TabNetMultiTaskClassifier from sklearn.metrics import log_loss from sklearn.model_selection import StratifiedKFold from sklearn.preprocessing import LabelEncoder from sklearn.ensemble import RandomForestClassifier . LOAD DATASET . catboost가 이 대회의 핵심 알고리즘이라고 생각하여 categorical한 변수를 만드는게 중요하다고 생각하여 따로 함수를 만듦 | 다른 알고리즘들은 categorical한 변수보다 numeric한 변수 처리에 초점을 맞춤 | . index gender: 성별 . car: 차량 소유 여부 . reality: 부동산 소유 여부 . child_num: 자녀 수 . income_total: 연간 소득 . income_type: 소득 분류 [&#39;Commercial associate&#39;, &#39;Working&#39;, &#39;State servant&#39;, &#39;Pensioner&#39;, &#39;Student&#39;] . edu_type: 교육 수준 [&#39;Higher education&#39; ,&#39;Secondary / secondary special&#39;, &#39;Incomplete higher&#39;, &#39;Lower secondary&#39;, &#39;Academic degree&#39;] . family_type: 결혼 여부 [&#39;Married&#39;, &#39;Civil marriage&#39;, &#39;Separated&#39;, &#39;Single / not married&#39;, &#39;Widow&#39;] . house_type: 생활 방식 [&#39;Municipal apartment&#39;, &#39;House / apartment&#39;, &#39;With parents&#39;, &#39;Co-op apartment&#39;, &#39;Rented apartment&#39;, &#39;Office apartment&#39;] . DAYS_BIRTH: 출생일 데이터 수집 당시 (0)부터 역으로 셈, 즉, -1은 데이터 수집일 하루 전에 태어났음을 의미 . DAYS_EMPLOYED: 업무 시작일 데이터 수집 당시 (0)부터 역으로 셈, 즉, -1은 데이터 수집일 하루 전부터 일을 시작함을 의미 양수 값은 고용되지 않은 상태를 의미함 . FLAG_MOBIL: 핸드폰 소유 여부 . work_phone: 업무용 전화 소유 여부 . phone: 전화 소유 여부 . email: 이메일 소유 여부 . occyp_type: 직업 유형 . family_size: 가족 규모 . begin_month: 신용카드 발급 월 데이터 수집 당시 (0)부터 역으로 셈, 즉, -1은 데이터 수집일 한 달 전에 신용카드를 발급함을 의미 . credit: 사용자의 신용카드 대금 연체를 기준으로 한 신용도 =&gt; 낮을 수록 높은 신용의 신용카드 사용자를 의미함 . def category_income(data: pd.DataFrame) -&gt; pd.DataFrame: data[&quot;income_total&quot;] = data[&quot;income_total&quot;] / 10000 conditions = [ (data[&quot;income_total&quot;].le(18)), (data[&quot;income_total&quot;].gt(18) &amp; data[&quot;income_total&quot;].le(33)), (data[&quot;income_total&quot;].gt(33) &amp; data[&quot;income_total&quot;].le(49)), (data[&quot;income_total&quot;].gt(49) &amp; data[&quot;income_total&quot;].le(64)), (data[&quot;income_total&quot;].gt(64) &amp; data[&quot;income_total&quot;].le(80)), (data[&quot;income_total&quot;].gt(80) &amp; data[&quot;income_total&quot;].le(95)), (data[&quot;income_total&quot;].gt(95) &amp; data[&quot;income_total&quot;].le(111)), (data[&quot;income_total&quot;].gt(111) &amp; data[&quot;income_total&quot;].le(126)), (data[&quot;income_total&quot;].gt(126) &amp; data[&quot;income_total&quot;].le(142)), (data[&quot;income_total&quot;].gt(142)), ] choices = [i for i in range(10)] data[&quot;income_total&quot;] = np.select(conditions, choices) return data def load_dataset() -&gt; Tuple[pd.DataFrame, pd.DataFrame]: path = &quot;/content/drive/MyDrive/Colab Notebooks/신용카드 연체예측/&quot; train = pd.read_csv(path + &quot;train.csv&quot;) train = train.drop([&quot;index&quot;], axis=1) train.fillna(&quot;NAN&quot;, inplace=True) test = pd.read_csv(path + &quot;test.csv&quot;) test = test.drop([&quot;index&quot;], axis=1) test.fillna(&quot;NAN&quot;, inplace=True) # absolute train[&quot;DAYS_EMPLOYED&quot;] = train[&quot;DAYS_EMPLOYED&quot;].map(lambda x: 0 if x &gt; 0 else x) train[&quot;DAYS_EMPLOYED&quot;] = np.abs(train[&quot;DAYS_EMPLOYED&quot;]) test[&quot;DAYS_EMPLOYED&quot;] = test[&quot;DAYS_EMPLOYED&quot;].map(lambda x: 0 if x &gt; 0 else x) test[&quot;DAYS_EMPLOYED&quot;] = np.abs(test[&quot;DAYS_EMPLOYED&quot;]) train[&quot;DAYS_BIRTH&quot;] = np.abs(train[&quot;DAYS_BIRTH&quot;]) test[&quot;DAYS_BIRTH&quot;] = np.abs(test[&quot;DAYS_BIRTH&quot;]) train[&quot;begin_month&quot;] = np.abs(train[&quot;begin_month&quot;]).astype(int) test[&quot;begin_month&quot;] = np.abs(test[&quot;begin_month&quot;]).astype(int) # DAYS_BIRTH train[&quot;DAYS_BIRTH_month&quot;] = np.floor(train[&quot;DAYS_BIRTH&quot;] / 30) - ( (np.floor(train[&quot;DAYS_BIRTH&quot;] / 30) / 12).astype(int) * 12 ) train[&quot;DAYS_BIRTH_month&quot;] = train[&quot;DAYS_BIRTH_month&quot;].astype(int) train[&quot;DAYS_BIRTH_week&quot;] = np.floor(train[&quot;DAYS_BIRTH&quot;] / 7) - ( (np.floor(train[&quot;DAYS_BIRTH&quot;] / 7) / 4).astype(int) * 4 ) train[&quot;DAYS_BIRTH_week&quot;] = train[&quot;DAYS_BIRTH_week&quot;].astype(int) test[&quot;DAYS_BIRTH_month&quot;] = np.floor(test[&quot;DAYS_BIRTH&quot;] / 30) - ( (np.floor(test[&quot;DAYS_BIRTH&quot;] / 30) / 12).astype(int) * 12 ) test[&quot;DAYS_BIRTH_month&quot;] = test[&quot;DAYS_BIRTH_month&quot;].astype(int) test[&quot;DAYS_BIRTH_week&quot;] = np.floor(test[&quot;DAYS_BIRTH&quot;] / 7) - ( (np.floor(test[&quot;DAYS_BIRTH&quot;] / 7) / 4).astype(int) * 4 ) test[&quot;DAYS_BIRTH_week&quot;] = test[&quot;DAYS_BIRTH_week&quot;].astype(int) # Age train[&quot;Age&quot;] = np.abs(train[&quot;DAYS_BIRTH&quot;]) // 360 test[&quot;Age&quot;] = np.abs(test[&quot;DAYS_BIRTH&quot;]) // 360 # DAYS_EMPLOYED train[&quot;DAYS_EMPLOYED_month&quot;] = np.floor(train[&quot;DAYS_EMPLOYED&quot;] / 30) - ( (np.floor(train[&quot;DAYS_EMPLOYED&quot;] / 30) / 12).astype(int) * 12 ) train[&quot;DAYS_EMPLOYED_month&quot;] = train[&quot;DAYS_EMPLOYED_month&quot;].astype(int) train[&quot;DAYS_EMPLOYED_week&quot;] = np.floor(train[&quot;DAYS_EMPLOYED&quot;] / 7) - ( (np.floor(train[&quot;DAYS_EMPLOYED&quot;] / 7) / 4).astype(int) * 4 ) train[&quot;DAYS_EMPLOYED_week&quot;] = train[&quot;DAYS_EMPLOYED_week&quot;].astype(int) test[&quot;DAYS_EMPLOYED_month&quot;] = np.floor(test[&quot;DAYS_EMPLOYED&quot;] / 30) - ( (np.floor(test[&quot;DAYS_EMPLOYED&quot;] / 30) / 12).astype(int) * 12 ) test[&quot;DAYS_EMPLOYED_month&quot;] = test[&quot;DAYS_EMPLOYED_month&quot;].astype(int) test[&quot;DAYS_EMPLOYED_week&quot;] = np.floor(test[&quot;DAYS_EMPLOYED&quot;] / 7) - ( (np.floor(test[&quot;DAYS_EMPLOYED&quot;] / 7) / 4).astype(int) * 4 ) test[&quot;DAYS_EMPLOYED_week&quot;] = test[&quot;DAYS_EMPLOYED_week&quot;].astype(int) # EMPLOYED train[&quot;EMPLOYED&quot;] = train[&quot;DAYS_EMPLOYED&quot;] / 360 test[&quot;EMPLOYED&quot;] = test[&quot;DAYS_EMPLOYED&quot;] / 360 # before_EMPLOYED train[&quot;before_EMPLOYED&quot;] = train[&quot;DAYS_BIRTH&quot;] - train[&quot;DAYS_EMPLOYED&quot;] train[&quot;before_EMPLOYED_month&quot;] = np.floor(train[&quot;before_EMPLOYED&quot;] / 30) - ( (np.floor(train[&quot;before_EMPLOYED&quot;] / 30) / 12).astype(int) * 12 ) train[&quot;before_EMPLOYED_month&quot;] = train[&quot;before_EMPLOYED_month&quot;].astype(int) train[&quot;before_EMPLOYED_week&quot;] = np.floor(train[&quot;before_EMPLOYED&quot;] / 7) - ( (np.floor(train[&quot;before_EMPLOYED&quot;] / 7) / 4).astype(int) * 4 ) train[&quot;before_EMPLOYED_week&quot;] = train[&quot;before_EMPLOYED_week&quot;].astype(int) test[&quot;before_EMPLOYED&quot;] = test[&quot;DAYS_BIRTH&quot;] - test[&quot;DAYS_EMPLOYED&quot;] test[&quot;before_EMPLOYED_month&quot;] = np.floor(test[&quot;before_EMPLOYED&quot;] / 30) - ( (np.floor(test[&quot;before_EMPLOYED&quot;] / 30) / 12).astype(int) * 12 ) test[&quot;before_EMPLOYED_month&quot;] = test[&quot;before_EMPLOYED_month&quot;].astype(int) test[&quot;before_EMPLOYED_week&quot;] = np.floor(test[&quot;before_EMPLOYED&quot;] / 7) - ( (np.floor(test[&quot;before_EMPLOYED&quot;] / 7) / 4).astype(int) * 4 ) test[&quot;before_EMPLOYED_week&quot;] = test[&quot;before_EMPLOYED_week&quot;].astype(int) # gender_car_reality train[&quot;user_code&quot;] = ( train[&quot;gender&quot;].astype(str) + &quot;_&quot; + train[&quot;car&quot;].astype(str) + &quot;_&quot; + train[&quot;reality&quot;].astype(str) ) test[&quot;user_code&quot;] = ( test[&quot;gender&quot;].astype(str) + &quot;_&quot; + test[&quot;car&quot;].astype(str) + &quot;_&quot; + test[&quot;reality&quot;].astype(str) ) del_cols = [ &quot;gender&quot;, &quot;car&quot;, &quot;reality&quot;, &quot;email&quot;, &quot;child_num&quot;, &quot;DAYS_BIRTH&quot;, &quot;DAYS_EMPLOYED&quot;, ] train.drop(train.loc[train[&quot;family_size&quot;] &gt; 7, &quot;family_size&quot;].index, inplace=True) train.drop(del_cols, axis=1, inplace=True) test.drop(del_cols, axis=1, inplace=True) cat_cols = [ &quot;income_type&quot;, &quot;edu_type&quot;, &quot;family_type&quot;, &quot;house_type&quot;, &quot;occyp_type&quot;, &quot;user_code&quot;, ] for col in cat_cols: label_encoder = LabelEncoder() label_encoder = label_encoder.fit(train[col]) train[col] = label_encoder.transform(train[col]) test[col] = label_encoder.transform(test[col]) return train, test def cat_load_dataset() -&gt; Tuple[pd.DataFrame, pd.DataFrame]: path = &quot;/content/drive/MyDrive/Colab Notebooks/신용카드 연체예측/&quot; train = pd.read_csv(path + &quot;train.csv&quot;) train = train.drop([&quot;index&quot;], axis=1) train.fillna(&quot;NAN&quot;, inplace=True) test = pd.read_csv(path + &quot;test.csv&quot;) test = test.drop([&quot;index&quot;], axis=1) test.fillna(&quot;NAN&quot;, inplace=True) # absolute train[&quot;DAYS_EMPLOYED&quot;] = train[&quot;DAYS_EMPLOYED&quot;].map(lambda x: 0 if x &gt; 0 else x) train[&quot;DAYS_EMPLOYED&quot;] = np.abs(train[&quot;DAYS_EMPLOYED&quot;]) test[&quot;DAYS_EMPLOYED&quot;] = test[&quot;DAYS_EMPLOYED&quot;].map(lambda x: 0 if x &gt; 0 else x) test[&quot;DAYS_EMPLOYED&quot;] = np.abs(test[&quot;DAYS_EMPLOYED&quot;]) train[&quot;DAYS_BIRTH&quot;] = np.abs(train[&quot;DAYS_BIRTH&quot;]) test[&quot;DAYS_BIRTH&quot;] = np.abs(test[&quot;DAYS_BIRTH&quot;]) train[&quot;begin_month&quot;] = np.abs(train[&quot;begin_month&quot;]).astype(int) test[&quot;begin_month&quot;] = np.abs(test[&quot;begin_month&quot;]).astype(int) # income_total train = category_income(train) test = category_income(test) # DAYS_BIRTH train[&quot;DAYS_BIRTH_month&quot;] = np.floor(train[&quot;DAYS_BIRTH&quot;] / 30) - ( (np.floor(train[&quot;DAYS_BIRTH&quot;] / 30) / 12).astype(int) * 12 ) train[&quot;DAYS_BIRTH_month&quot;] = train[&quot;DAYS_BIRTH_month&quot;].astype(int) train[&quot;DAYS_BIRTH_week&quot;] = np.floor(train[&quot;DAYS_BIRTH&quot;] / 7) - ( (np.floor(train[&quot;DAYS_BIRTH&quot;] / 7) / 4).astype(int) * 4 ) train[&quot;DAYS_BIRTH_week&quot;] = train[&quot;DAYS_BIRTH_week&quot;].astype(int) test[&quot;DAYS_BIRTH_month&quot;] = np.floor(test[&quot;DAYS_BIRTH&quot;] / 30) - ( (np.floor(test[&quot;DAYS_BIRTH&quot;] / 30) / 12).astype(int) * 12 ) test[&quot;DAYS_BIRTH_month&quot;] = test[&quot;DAYS_BIRTH_month&quot;].astype(int) test[&quot;DAYS_BIRTH_week&quot;] = np.floor(test[&quot;DAYS_BIRTH&quot;] / 7) - ( (np.floor(test[&quot;DAYS_BIRTH&quot;] / 7) / 4).astype(int) * 4 ) test[&quot;DAYS_BIRTH_week&quot;] = test[&quot;DAYS_BIRTH_week&quot;].astype(int) # Age train[&quot;Age&quot;] = np.abs(train[&quot;DAYS_BIRTH&quot;]) // 360 test[&quot;Age&quot;] = np.abs(test[&quot;DAYS_BIRTH&quot;]) // 360 # DAYS_EMPLOYED train[&quot;DAYS_EMPLOYED_month&quot;] = np.floor(train[&quot;DAYS_EMPLOYED&quot;] / 30) - ( (np.floor(train[&quot;DAYS_EMPLOYED&quot;] / 30) / 12).astype(int) * 12 ) train[&quot;DAYS_EMPLOYED_month&quot;] = train[&quot;DAYS_EMPLOYED_month&quot;].astype(int) train[&quot;DAYS_EMPLOYED_week&quot;] = np.floor(train[&quot;DAYS_EMPLOYED&quot;] / 7) - ( (np.floor(train[&quot;DAYS_EMPLOYED&quot;] / 7) / 4).astype(int) * 4 ) train[&quot;DAYS_EMPLOYED_week&quot;] = train[&quot;DAYS_EMPLOYED_week&quot;].astype(int) test[&quot;DAYS_EMPLOYED_month&quot;] = np.floor(test[&quot;DAYS_EMPLOYED&quot;] / 30) - ( (np.floor(test[&quot;DAYS_EMPLOYED&quot;] / 30) / 12).astype(int) * 12 ) test[&quot;DAYS_EMPLOYED_month&quot;] = test[&quot;DAYS_EMPLOYED_month&quot;].astype(int) test[&quot;DAYS_EMPLOYED_week&quot;] = np.floor(test[&quot;DAYS_EMPLOYED&quot;] / 7) - ( (np.floor(test[&quot;DAYS_EMPLOYED&quot;] / 7) / 4).astype(int) * 4 ) test[&quot;DAYS_EMPLOYED_week&quot;] = test[&quot;DAYS_EMPLOYED_week&quot;].astype(int) # EMPLOYED train[&quot;EMPLOYED&quot;] = train[&quot;DAYS_EMPLOYED&quot;] / 360 test[&quot;EMPLOYED&quot;] = test[&quot;DAYS_EMPLOYED&quot;] / 360 # before_EMPLOYED train[&quot;before_EMPLOYED&quot;] = train[&quot;DAYS_BIRTH&quot;] - train[&quot;DAYS_EMPLOYED&quot;] train[&quot;before_EMPLOYED_month&quot;] = np.floor(train[&quot;before_EMPLOYED&quot;] / 30) - ( (np.floor(train[&quot;before_EMPLOYED&quot;] / 30) / 12).astype(int) * 12 ) train[&quot;before_EMPLOYED_month&quot;] = train[&quot;before_EMPLOYED_month&quot;].astype(int) train[&quot;before_EMPLOYED_week&quot;] = np.floor(train[&quot;before_EMPLOYED&quot;] / 7) - ( (np.floor(train[&quot;before_EMPLOYED&quot;] / 7) / 4).astype(int) * 4 ) train[&quot;before_EMPLOYED_week&quot;] = train[&quot;before_EMPLOYED_week&quot;].astype(int) test[&quot;before_EMPLOYED&quot;] = test[&quot;DAYS_BIRTH&quot;] - test[&quot;DAYS_EMPLOYED&quot;] test[&quot;before_EMPLOYED_month&quot;] = np.floor(test[&quot;before_EMPLOYED&quot;] / 30) - ( (np.floor(test[&quot;before_EMPLOYED&quot;] / 30) / 12).astype(int) * 12 ) test[&quot;before_EMPLOYED_month&quot;] = test[&quot;before_EMPLOYED_month&quot;].astype(int) test[&quot;before_EMPLOYED_week&quot;] = np.floor(test[&quot;before_EMPLOYED&quot;] / 7) - ( (np.floor(test[&quot;before_EMPLOYED&quot;] / 7) / 4).astype(int) * 4 ) test[&quot;before_EMPLOYED_week&quot;] = test[&quot;before_EMPLOYED_week&quot;].astype(int) # gender_car_reality train[&quot;user_code&quot;] = ( train[&quot;gender&quot;].astype(str) + &quot;_&quot; + train[&quot;car&quot;].astype(str) + &quot;_&quot; + train[&quot;reality&quot;].astype(str) ) test[&quot;user_code&quot;] = ( test[&quot;gender&quot;].astype(str) + &quot;_&quot; + test[&quot;car&quot;].astype(str) + &quot;_&quot; + test[&quot;reality&quot;].astype(str) ) del_cols = [ &quot;gender&quot;, &quot;car&quot;, &quot;reality&quot;, &quot;email&quot;, &quot;child_num&quot;, &quot;DAYS_BIRTH&quot;, &quot;DAYS_EMPLOYED&quot;, ] train.drop(train.loc[train[&quot;family_size&quot;] &gt; 7, &quot;family_size&quot;].index, inplace=True) train.drop(del_cols, axis=1, inplace=True) test.drop(del_cols, axis=1, inplace=True) cat_cols = [ &quot;income_type&quot;, &quot;edu_type&quot;, &quot;family_type&quot;, &quot;house_type&quot;, &quot;occyp_type&quot;, &quot;user_code&quot;, ] for col in cat_cols: label_encoder = LabelEncoder() label_encoder = label_encoder.fit(train[col]) train[col] = label_encoder.transform(train[col]) test[col] = label_encoder.transform(test[col]) return train, test . MODELS . def stratified_kfold_cat( params: Dict[str, Union[int, float, str, List[str]]], n_fold: int, X: pd.DataFrame, y: pd.DataFrame, X_test: pd.DataFrame, ) -&gt; Tuple[np.ndarray, np.ndarray]: folds = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=42) splits = folds.split(X, y) cat_oof = np.zeros((X.shape[0], 3)) cat_preds = np.zeros((X_test.shape[0], 3)) cat_cols = [c for c in X.columns if X[c].dtypes == &quot;int64&quot;] for fold, (train_idx, valid_idx) in enumerate(splits): print(f&quot;============ Fold {fold} ============ n&quot;) X_train, X_valid = X.iloc[train_idx], X.iloc[valid_idx] y_train, y_valid = y.iloc[train_idx], y.iloc[valid_idx] train_data = Pool(data=X_train, label=y_train, cat_features=cat_cols) valid_data = Pool(data=X_valid, label=y_valid, cat_features=cat_cols) model = CatBoostClassifier(**params) model.fit( train_data, eval_set=valid_data, early_stopping_rounds=100, use_best_model=True, verbose=100, ) cat_oof[valid_idx] = model.predict_proba(X_valid) cat_preds += model.predict_proba(X_test) / n_fold log_score = log_loss(y, cat_oof) print(f&quot;Log Loss Score: {log_score:.5f} n&quot;) return cat_oof, cat_preds # Light GBM def stratified_kfold_lgbm( params: Dict[str, Union[int, float, str]], n_fold: int, X: pd.DataFrame, y: pd.DataFrame, X_test: pd.DataFrame, ) -&gt; Tuple[np.ndarray, np.ndarray]: folds = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=42) splits = folds.split(X, y) lgb_oof = np.zeros((X.shape[0], 3)) lgb_preds = np.zeros((X_test.shape[0], 3)) for fold, (train_idx, valid_idx) in enumerate(splits): print(f&quot;============ Fold {fold} ============ n&quot;) X_train, X_valid = X.iloc[train_idx], X.iloc[valid_idx] y_train, y_valid = y.iloc[train_idx], y.iloc[valid_idx] pre_model = LGBMClassifier(**params) pre_model.fit( X_train, y_train, eval_set=[(X_train, y_train), (X_valid, y_valid)], early_stopping_rounds=100, verbose=100, ) params2 = params.copy() params2[&quot;learning_rate&quot;] = params[&quot;learning_rate&quot;] * 0.1 model = LGBMClassifier(**params2) model.fit( X_train, y_train, eval_set=[(X_train, y_train), (X_valid, y_valid)], early_stopping_rounds=100, verbose=100, init_model=pre_model, ) lgb_oof[valid_idx] = model.predict_proba(X_valid) lgb_preds += model.predict_proba(X_test) / n_fold log_score = log_loss(y, lgb_oof) print(f&quot;Log Loss Score: {log_score:.5f}&quot;) return lgb_oof, lgb_preds # XGB def stratified_kfold_xgb( params: Dict[str, Union[int, float, str]], n_fold: int, X: pd.DataFrame, y: pd.DataFrame, X_test: pd.DataFrame, ) -&gt; Tuple[np.ndarray, np.ndarray]: folds = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=42) splits = folds.split(X, y) xgb_oof = np.zeros((X.shape[0], 3)) xgb_preds = np.zeros((X_test.shape[0], 3)) for fold, (train_idx, valid_idx) in enumerate(splits): print(f&quot;============ Fold {fold} ============ n&quot;) X_train, X_valid = X.iloc[train_idx], X.iloc[valid_idx] y_train, y_valid = y.iloc[train_idx], y.iloc[valid_idx] model = XGBClassifier(**params) model.fit( X_train, y_train, eval_set=[(X_train, y_train), (X_valid, y_valid)], early_stopping_rounds=100, verbose=100, ) xgb_oof[valid_idx] = model.predict_proba(X_valid) xgb_preds += model.predict_proba(X_test) / n_fold log_score = log_loss(y, xgb_oof) print(f&quot;Log Loss Score: {log_score:.5f}&quot;) return xgb_oof, xgb_preds # Random Foreset def stratified_kfold_rf( params: Dict[str, Union[int, float, str, bool]], n_fold: int, X: pd.DataFrame, y: pd.DataFrame, X_test: pd.DataFrame, ) -&gt; Tuple[np.ndarray, np.ndarray]: folds = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=42) splits = folds.split(X, y) rf_oof = np.zeros((X.shape[0], 3)) rf_preds = np.zeros((X_test.shape[0], 3)) for fold, (train_idx, valid_idx) in enumerate(splits): print(f&quot;============ Fold {fold} ============ n&quot;) X_train, X_valid = X.iloc[train_idx], X.iloc[valid_idx] y_train, y_valid = y.iloc[train_idx], y.iloc[valid_idx] model = RandomForestClassifier(**params) model.fit( X_train, y_train, ) rf_oof[valid_idx] = model.predict_proba(X_valid) rf_preds += model.predict_proba(X_test) / n_fold print(f&quot;Log Loss Score: {log_loss(y_valid, rf_oof[valid_idx]):.5f}&quot;) log_score = log_loss(y, rf_oof) print(f&quot;Log Loss Score: {log_score:.5f}&quot;) return rf_oof, rf_preds . CAT TRAIN . catboost 같은 경우 categorical한 feature를 고정시켜주는게 핵심이라고 생각하여 하이퍼파라미터튜닝에 신경을 씀 | 하이퍼파라미터튜닝은 optuna 라이브러리로 했음 | . train_cat, test_cat = cat_load_dataset() X = train_cat.drop(&quot;credit&quot;, axis=1) y = train_cat[&quot;credit&quot;] X_test = test_cat.copy() . cat_params = { &quot;learning_rate&quot;: 0.026612467217016746, &quot;l2_leaf_reg&quot;: 0.3753065117824262, &quot;max_depth&quot;: 8, &quot;bagging_temperature&quot;: 1, &quot;min_data_in_leaf&quot;: 57, &quot;max_bin&quot;: 494, &quot;random_state&quot;: 42, &quot;eval_metric&quot;: &quot;MultiClass&quot;, &quot;loss_function&quot;: &quot;MultiClass&quot;, &quot;od_type&quot;: &quot;Iter&quot;, &quot;od_wait&quot;: 500, &quot;iterations&quot;: 10000, &quot;cat_features&quot;: [ &quot;income_total&quot;, &quot;income_type&quot;, &quot;edu_type&quot;, &quot;family_type&quot;, &quot;house_type&quot;, &quot;FLAG_MOBIL&quot;, &quot;work_phone&quot;, &quot;phone&quot;, &quot;occyp_type&quot;, &quot;begin_month&quot;, &quot;DAYS_BIRTH_month&quot;, &quot;DAYS_BIRTH_week&quot;, &quot;Age&quot;, &quot;DAYS_EMPLOYED_month&quot;, &quot;DAYS_EMPLOYED_week&quot;, &quot;before_EMPLOYED&quot;, &quot;before_EMPLOYED_month&quot;, &quot;before_EMPLOYED_week&quot;, &quot;user_code&quot;, ], } cat_oof, cat_preds = stratified_kfold_cat(cat_params, 10, X, y, X_test) . ============ Fold 0 ============ 0: learn: 1.0832640 test: 1.0829576 best: 1.0829576 (0) total: 612ms remaining: 1h 42m 1s 100: learn: 0.7323907 test: 0.6929501 best: 0.6929501 (100) total: 44.8s remaining: 1h 13m 10s 200: learn: 0.7058060 test: 0.6735975 best: 0.6735975 (200) total: 1m 43s remaining: 1h 23m 43s 300: learn: 0.6885144 test: 0.6706689 best: 0.6706689 (300) total: 2m 45s remaining: 1h 28m 45s 400: learn: 0.6701975 test: 0.6682080 best: 0.6681988 (388) total: 3m 47s remaining: 1h 30m 43s 500: learn: 0.6494751 test: 0.6664689 best: 0.6662605 (486) total: 4m 50s remaining: 1h 31m 54s 600: learn: 0.6290599 test: 0.6657848 best: 0.6657848 (600) total: 5m 54s remaining: 1h 32m 16s 700: learn: 0.6080674 test: 0.6648291 best: 0.6648201 (698) total: 6m 57s remaining: 1h 32m 14s 800: learn: 0.5895904 test: 0.6646412 best: 0.6643652 (761) total: 8m 1s remaining: 1h 32m 7s Stopped by overfitting detector (100 iterations wait) bestTest = 0.6643651535 bestIteration = 761 Shrink model to first 762 iterations. ============ Fold 1 ============ 0: learn: 1.0831624 test: 1.0831592 best: 1.0831592 (0) total: 300ms remaining: 50m 1s 100: learn: 0.7348941 test: 0.6976274 best: 0.6976274 (100) total: 43.6s remaining: 1h 11m 13s 200: learn: 0.7091219 test: 0.6726237 best: 0.6726237 (200) total: 1m 41s remaining: 1h 22m 48s 300: learn: 0.6922515 test: 0.6687022 best: 0.6687022 (300) total: 2m 42s remaining: 1h 27m 30s 400: learn: 0.6724169 test: 0.6655868 best: 0.6655868 (400) total: 3m 46s remaining: 1h 30m 15s 500: learn: 0.6510442 test: 0.6630204 best: 0.6630204 (500) total: 4m 50s remaining: 1h 31m 49s 600: learn: 0.6316988 test: 0.6614526 best: 0.6614526 (600) total: 5m 53s remaining: 1h 32m 6s 700: learn: 0.6129803 test: 0.6607424 best: 0.6606660 (687) total: 6m 57s remaining: 1h 32m 12s 800: learn: 0.5930475 test: 0.6602049 best: 0.6601429 (793) total: 8m 1s remaining: 1h 32m 10s 900: learn: 0.5758447 test: 0.6597748 best: 0.6596636 (885) total: 9m 5s remaining: 1h 31m 47s Stopped by overfitting detector (100 iterations wait) bestTest = 0.6596635705 bestIteration = 885 Shrink model to first 886 iterations. ============ Fold 2 ============ 0: learn: 1.0831179 test: 1.0832346 best: 1.0832346 (0) total: 296ms remaining: 49m 16s 100: learn: 0.7276329 test: 0.6945366 best: 0.6945366 (100) total: 46.9s remaining: 1h 16m 32s 200: learn: 0.7020572 test: 0.6763344 best: 0.6763344 (200) total: 1m 49s remaining: 1h 28m 43s 300: learn: 0.6849021 test: 0.6734732 best: 0.6734732 (300) total: 2m 50s remaining: 1h 31m 25s 400: learn: 0.6678492 test: 0.6713501 best: 0.6713501 (400) total: 3m 52s remaining: 1h 32m 47s 500: learn: 0.6489240 test: 0.6697702 best: 0.6697702 (500) total: 4m 56s remaining: 1h 33m 37s 600: learn: 0.6286190 test: 0.6682808 best: 0.6682423 (598) total: 5m 59s remaining: 1h 33m 43s 700: learn: 0.6096438 test: 0.6678665 best: 0.6678645 (698) total: 7m 3s remaining: 1h 33m 38s 800: learn: 0.5927210 test: 0.6675491 best: 0.6673454 (766) total: 8m 7s remaining: 1h 33m 15s 900: learn: 0.5757733 test: 0.6672852 best: 0.6670491 (844) total: 9m 11s remaining: 1h 32m 46s Stopped by overfitting detector (100 iterations wait) bestTest = 0.6670490964 bestIteration = 844 Shrink model to first 845 iterations. ============ Fold 3 ============ 0: learn: 1.0831514 test: 1.0830642 best: 1.0830642 (0) total: 310ms remaining: 51m 42s 100: learn: 0.7303778 test: 0.7092536 best: 0.7092536 (100) total: 49.1s remaining: 1h 20m 16s 200: learn: 0.7023113 test: 0.6888512 best: 0.6888512 (200) total: 2m 1s remaining: 1h 38m 47s 300: learn: 0.6853219 test: 0.6845243 best: 0.6845243 (300) total: 3m 10s remaining: 1h 42m 11s 400: learn: 0.6675973 test: 0.6826500 best: 0.6826052 (388) total: 4m 26s remaining: 1h 46m 19s 500: learn: 0.6481532 test: 0.6808902 best: 0.6807720 (498) total: 5m 35s remaining: 1h 46m 7s 600: learn: 0.6285784 test: 0.6800921 best: 0.6800914 (595) total: 6m 41s remaining: 1h 44m 44s 700: learn: 0.6083968 test: 0.6796023 best: 0.6794399 (663) total: 7m 46s remaining: 1h 43m 12s 800: learn: 0.5899674 test: 0.6789752 best: 0.6788943 (792) total: 8m 51s remaining: 1h 41m 38s 900: learn: 0.5721327 test: 0.6783820 best: 0.6783820 (900) total: 9m 55s remaining: 1h 40m 14s 1000: learn: 0.5544633 test: 0.6780686 best: 0.6779982 (942) total: 10m 59s remaining: 1h 38m 45s 1100: learn: 0.5367893 test: 0.6780652 best: 0.6776773 (1037) total: 12m 9s remaining: 1h 38m 14s Stopped by overfitting detector (100 iterations wait) bestTest = 0.6776773442 bestIteration = 1037 Shrink model to first 1038 iterations. ============ Fold 4 ============ 0: learn: 1.0831773 test: 1.0829793 best: 1.0829793 (0) total: 296ms remaining: 49m 23s 100: learn: 0.7273797 test: 0.6978671 best: 0.6978671 (100) total: 52.4s remaining: 1h 25m 35s 200: learn: 0.7017828 test: 0.6804129 best: 0.6804129 (200) total: 1m 52s remaining: 1h 31m 20s 300: learn: 0.6860989 test: 0.6769156 best: 0.6768962 (299) total: 2m 54s remaining: 1h 33m 37s 400: learn: 0.6681431 test: 0.6751760 best: 0.6751278 (397) total: 3m 57s remaining: 1h 34m 36s 500: learn: 0.6485897 test: 0.6737932 best: 0.6737814 (499) total: 5m remaining: 1h 35m 3s 600: learn: 0.6285877 test: 0.6730592 best: 0.6728910 (598) total: 6m 4s remaining: 1h 35m 3s 700: learn: 0.6097368 test: 0.6721781 best: 0.6721426 (699) total: 7m 14s remaining: 1h 35m 58s 800: learn: 0.5926144 test: 0.6721474 best: 0.6719096 (776) total: 8m 19s remaining: 1h 35m 39s Stopped by overfitting detector (100 iterations wait) bestTest = 0.6719096303 bestIteration = 776 Shrink model to first 777 iterations. ============ Fold 5 ============ 0: learn: 1.0832186 test: 1.0833499 best: 1.0833499 (0) total: 298ms remaining: 49m 34s 100: learn: 0.7249238 test: 0.7056807 best: 0.7056807 (100) total: 45.7s remaining: 1h 14m 35s 200: learn: 0.6995494 test: 0.6906425 best: 0.6906425 (200) total: 1m 45s remaining: 1h 25m 49s 300: learn: 0.6821473 test: 0.6874748 best: 0.6874748 (300) total: 2m 47s remaining: 1h 30m 6s 400: learn: 0.6642446 test: 0.6860589 best: 0.6860589 (400) total: 3m 50s remaining: 1h 31m 59s 500: learn: 0.6449000 test: 0.6855384 best: 0.6855013 (499) total: 4m 57s remaining: 1h 34m 5s 600: learn: 0.6241052 test: 0.6848923 best: 0.6848844 (589) total: 6m 1s remaining: 1h 34m 17s 700: learn: 0.6055032 test: 0.6847242 best: 0.6844358 (666) total: 7m 8s remaining: 1h 34m 49s Stopped by overfitting detector (100 iterations wait) bestTest = 0.6844358101 bestIteration = 666 Shrink model to first 667 iterations. ============ Fold 6 ============ 0: learn: 1.0831277 test: 1.0830831 best: 1.0830831 (0) total: 307ms remaining: 51m 6s 100: learn: 0.7299032 test: 0.7012839 best: 0.7012839 (100) total: 47.2s remaining: 1h 17m 8s 200: learn: 0.7043960 test: 0.6834182 best: 0.6833724 (188) total: 1m 50s remaining: 1h 29m 28s 300: learn: 0.6855325 test: 0.6793321 best: 0.6793321 (300) total: 2m 51s remaining: 1h 32m 8s 400: learn: 0.6663620 test: 0.6766051 best: 0.6766051 (400) total: 3m 57s remaining: 1h 34m 33s 500: learn: 0.6459442 test: 0.6751095 best: 0.6750316 (451) total: 5m remaining: 1h 35m 3s 600: learn: 0.6271073 test: 0.6742075 best: 0.6741912 (599) total: 6m 4s remaining: 1h 35m 3s 700: learn: 0.6080896 test: 0.6741347 best: 0.6738342 (628) total: 7m 8s remaining: 1h 34m 50s Stopped by overfitting detector (100 iterations wait) bestTest = 0.6738341764 bestIteration = 628 Shrink model to first 629 iterations. ============ Fold 7 ============ 0: learn: 1.0832149 test: 1.0831962 best: 1.0831962 (0) total: 307ms remaining: 51m 7s 100: learn: 0.7276994 test: 0.7009006 best: 0.7009006 (100) total: 46.4s remaining: 1h 15m 49s 200: learn: 0.7030936 test: 0.6848146 best: 0.6847949 (198) total: 1m 47s remaining: 1h 27m 26s 300: learn: 0.6856515 test: 0.6815006 best: 0.6815006 (300) total: 2m 49s remaining: 1h 30m 58s 400: learn: 0.6681527 test: 0.6796503 best: 0.6795603 (384) total: 3m 52s remaining: 1h 32m 50s 500: learn: 0.6503799 test: 0.6787021 best: 0.6785460 (493) total: 4m 56s remaining: 1h 33m 43s 600: learn: 0.6319088 test: 0.6780125 best: 0.6779053 (597) total: 6m remaining: 1h 34m 4s 700: learn: 0.6122103 test: 0.6774097 best: 0.6773104 (698) total: 7m 5s remaining: 1h 34m 800: learn: 0.5939343 test: 0.6764882 best: 0.6764710 (799) total: 8m 11s remaining: 1h 34m 10s 900: learn: 0.5762685 test: 0.6764979 best: 0.6762442 (823) total: 9m 15s remaining: 1h 33m 32s 1000: learn: 0.5586911 test: 0.6759459 best: 0.6759226 (999) total: 10m 19s remaining: 1h 32m 49s 1100: learn: 0.5415433 test: 0.6763338 best: 0.6757223 (1032) total: 11m 23s remaining: 1h 32m 4s Stopped by overfitting detector (100 iterations wait) bestTest = 0.6757223098 bestIteration = 1032 Shrink model to first 1033 iterations. ============ Fold 8 ============ 0: learn: 1.0831574 test: 1.0832749 best: 1.0832749 (0) total: 294ms remaining: 48m 55s 100: learn: 0.7315047 test: 0.7031190 best: 0.7031190 (100) total: 41.5s remaining: 1h 7m 51s 200: learn: 0.7056228 test: 0.6837846 best: 0.6837846 (200) total: 1m 41s remaining: 1h 22m 21s 300: learn: 0.6886137 test: 0.6786870 best: 0.6786859 (299) total: 2m 41s remaining: 1h 26m 42s 400: learn: 0.6690231 test: 0.6763612 best: 0.6762533 (394) total: 3m 43s remaining: 1h 29m 21s 500: learn: 0.6483094 test: 0.6741831 best: 0.6741430 (497) total: 4m 47s remaining: 1h 30m 45s 600: learn: 0.6292264 test: 0.6731320 best: 0.6731320 (600) total: 5m 50s remaining: 1h 31m 24s 700: learn: 0.6111467 test: 0.6726734 best: 0.6726604 (662) total: 6m 54s remaining: 1h 31m 35s 800: learn: 0.5925022 test: 0.6727895 best: 0.6724140 (727) total: 7m 57s remaining: 1h 31m 21s Stopped by overfitting detector (100 iterations wait) bestTest = 0.672413971 bestIteration = 727 Shrink model to first 728 iterations. ============ Fold 9 ============ 0: learn: 1.0832009 test: 1.0827439 best: 1.0827439 (0) total: 295ms remaining: 49m 9s 100: learn: 0.7324903 test: 0.7048082 best: 0.7048082 (100) total: 41s remaining: 1h 6m 55s 200: learn: 0.7065402 test: 0.6859564 best: 0.6859419 (198) total: 1m 37s remaining: 1h 19m 28s 300: learn: 0.6876098 test: 0.6816683 best: 0.6816683 (300) total: 2m 38s remaining: 1h 25m 10s 400: learn: 0.6702645 test: 0.6800155 best: 0.6800155 (400) total: 3m 41s remaining: 1h 28m 33s 500: learn: 0.6496183 test: 0.6780545 best: 0.6779703 (494) total: 4m 44s remaining: 1h 30m 1s 600: learn: 0.6291321 test: 0.6770233 best: 0.6769609 (596) total: 5m 48s remaining: 1h 30m 49s 700: learn: 0.6095260 test: 0.6765122 best: 0.6765086 (674) total: 6m 51s remaining: 1h 31m 2s 800: learn: 0.5920527 test: 0.6764972 best: 0.6763723 (712) total: 7m 54s remaining: 1h 30m 52s Stopped by overfitting detector (100 iterations wait) bestTest = 0.6763722857 bestIteration = 712 Shrink model to first 713 iterations. Log Loss Score: 0.67234 . LGBM TRAIN . 하이퍼파라미터 튜닝은 역시 optuna를 사용 . train, test = load_dataset() X = train.drop(&quot;credit&quot;, axis=1) y = train[&quot;credit&quot;] X_test = test.copy() . NameError Traceback (most recent call last) &lt;ipython-input-1-a46e421ec841&gt; in &lt;module&gt;() -&gt; 1 train, test = load_dataset() 2 X = train.drop(&#34;credit&#34;, axis=1) 3 y = train[&#34;credit&#34;] 4 X_test = test.copy() NameError: name &#39;load_dataset&#39; is not defined . lgb_params = { &quot;reg_alpha&quot;: 5.998770177220496e-05, &quot;reg_lambda&quot;: 0.07127674208132959, &quot;max_depth&quot;: 18, &quot;num_leaves&quot;: 125, &quot;colsample_bytree&quot;: 0.4241631237880101, &quot;subsample&quot;: 0.8876057928391585, &quot;subsample_freq&quot;: 5, &quot;min_child_samples&quot;: 5, &quot;max_bin&quot;: 449, &quot;random_state&quot;: 42, &quot;boosting_type&quot;: &quot;gbdt&quot;, &quot;learning_rate&quot;: 0.05, &quot;n_estimators&quot;: 10000, &quot;objective&quot;: &quot;multiclass&quot;, &quot;metric&quot;: &quot;multi_logloss&quot;, } lgbm_oof, lgbm_preds = stratified_kfold_lgbm(lgb_params, 10, X, y, X_test) . NameError Traceback (most recent call last) &lt;ipython-input-2-00803de4ed59&gt; in &lt;module&gt;() 16 &#34;metric&#34;: &#34;multi_logloss&#34;, 17 } &gt; 18 lgbm_oof, lgbm_preds = stratified_kfold_lgbm(lgb_params, 10, X, y, X_test) NameError: name &#39;stratified_kfold_lgbm&#39; is not defined . XGB TRAIN . xgb_params = { &quot;eta&quot;: 0.023839252347297356, &quot;reg_alpha&quot;: 6.99554614267605e-06, &quot;reg_lambda&quot;: 0.010419988953061583, &quot;max_depth&quot;: 15, &quot;max_leaves&quot;: 159, &quot;colsample_bytree&quot;: 0.4515469593932409, &quot;subsample&quot;: 0.7732694309118915, &quot;min_child_weight&quot;: 5, &quot;gamma&quot;: 0.6847131315687576, &quot;random_state&quot;: 42, &quot;n_estimators&quot;: 10000, &quot;objective&quot;: &quot;multi:softmax&quot;, &quot;eval_metric&quot;: &quot;mlogloss&quot;, } xgb_oof, xgb_preds = stratified_kfold_xgb(xgb_params, 10, X, y, X_test) . RANDOM FOREST TRAIN . rf_params = { &quot;criterion&quot;: &quot;gini&quot;, &quot;n_estimators&quot;: 300, &quot;min_samples_split&quot;: 10, &quot;min_samples_leaf&quot;: 2, &quot;max_features&quot;: &quot;auto&quot;, &quot;oob_score&quot;: True, &quot;random_state&quot;: 42, &quot;n_jobs&quot;: -1, } rf_oof, rf_preds = stratified_kfold_rf(rf_params, 10, X, y, X_test) . train, test = load_dataset() train_x = train.drop(&quot;credit&quot;, axis = 1) train_y = train[&#39;credit&#39;].values . &#48176;&#50676; &#54633;&#52824;&#44592; . catboost와 lgbm, xgb, randomforest를 통해 생성한 배열들을 합쳐준다. . train_pred = np.concatenate([cat_oof, lgbm_oof, xgb_oof, rf_oof], axis=1) train_pred.shape . test_pred = np.concatenate([cat_preds, lgbm_preds, xgb_preds, rf_preds], axis=1) test_pred.shape . PYTORCH TABULAR . Stacking Ensemble을 사용하며 학습 진행 | . TabNet은 입력으로 Categorical변수를 Embedding 하기 때문에, Categorical 변수라는 것을 지정해주어야 한다. . device = &quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot; . 이후 TabNet의 Classifier를 정의해준다. Approximate Range of Model Hyperparameters: n_d, n_a: 8 to 512 batch size: 256 to 32768 virtual batch size: 128 to 2048 sparsity regularization constant: 0 to 0.00001 number of shared GLU Blocks: 2 to 10 number of independent decision Blocks: 2 to 10 relaxation constant: 1 to 2.5 number of decision steps: 2 to 10 batch normalization momentum: 0.5 to 0.98 . 첫 번째 공유 GLU 블록(또는 블록이 공유되지 않는 경우 첫 번째 독립 블록)은 입력 피쳐의 차원을 n_a+n_d와 같은 차원으로 줄이기 때문에 독특하다. n_a는 다음 단계의 주의 변압기에 입력된 형상의 치수이고 n_d는 최종 결과를 계산하는 데 사용되는 형상의 치수이다. 이러한 기능은 스플리터에 도달할 때까지 함께 처리됩니다. ReLU 활성화는 n_d 차원 벡터에 적용됩니다. . mask_type을 entmax로 설정했는데, 이는 softmax함수의 한 종류이다. softmax 함수는 하나의 샘플 데이터에 대한 예측값으로 모든 가능한 정답지에 대해서 정답일 확률의 합이 1이 되도록 하는것 즉 정답지의 총 개수가 k일때, k차원의 벡터를 입력받아 각 클래스에 대한 확률을 추정하는 것 . entmax는 sparsemax를 더 일반화시킨 것으로 좀더 훈련이 용이하다. . n_fold = 10 folds = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=42) splits = folds.split(train_pred, train_y) net_oof = np.zeros((train_pred.shape[0], 3)) net_preds = np.zeros((test_pred.shape[0], 3)) for fold, (train_idx, valid_idx) in enumerate(splits): print(f&quot;============ Fold {fold} ============ n&quot;) X_train, X_valid = train_pred[train_idx], train_pred[valid_idx] y_train, y_valid = train_y[train_idx], train_y[valid_idx] model = TabNetMultiTaskClassifier( n_d=64, n_a=64, n_steps=1, lambda_sparse=1e-4, optimizer_fn=torch.optim.Adam, optimizer_params=dict(lr=2e-2), scheduler_params = {&quot;gamma&quot;: 0.9, &quot;step_size&quot;: 50}, scheduler_fn=torch.optim.lr_scheduler.StepLR, mask_type=&quot;entmax&quot;, device_name=device ) model.fit( X_train, y_train.reshape(-1,1), eval_set=[(X_valid, y_valid.reshape(-1,1))], max_epochs=100, batch_size=1024, eval_metric=[&quot;logloss&quot;], virtual_batch_size=128, num_workers=1, drop_last=False ) net_oof[valid_idx] = model.predict_proba(X_valid) net_preds += model.predict_proba(test_pred)[0] / n_fold log_score = log_loss(train_y, net_oof) print(f&quot;Log Loss Score: {log_score:.5f}&quot;) .",
            "url": "https://shw9807.github.io/shw9807blog/%EB%8D%B0%EC%9D%B4%EC%BD%98/2022/02/10/%EC%8B%A0%EC%9A%A9%EC%B9%B4%EB%93%9C_%EC%97%B0%EC%B2%B4%EC%98%88%EC%B8%A1.html",
            "relUrl": "/%EB%8D%B0%EC%9D%B4%EC%BD%98/2022/02/10/%EC%8B%A0%EC%9A%A9%EC%B9%B4%EB%93%9C_%EC%97%B0%EC%B2%B4%EC%98%88%EC%B8%A1.html",
            "date": " • Feb 10, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "[Colab] 물류 유통량 예측",
            "content": ". &#47932;&#47448; &#50976;&#53685;&#47049; &#50696;&#52769; . 1.Library &amp; Data Load . from google.colab import drive drive.mount(&#39;/content/drive&#39;) . Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(&#34;/content/drive&#34;, force_remount=True). . !pip install catboost !pip install optuna . Requirement already satisfied: catboost in /usr/local/lib/python3.7/dist-packages (1.0.4) Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from catboost) (1.15.0) Requirement already satisfied: pandas&gt;=0.24.0 in /usr/local/lib/python3.7/dist-packages (from catboost) (1.3.5) Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from catboost) (3.2.2) Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from catboost) (1.4.1) Requirement already satisfied: graphviz in /usr/local/lib/python3.7/dist-packages (from catboost) (0.10.1) Requirement already satisfied: numpy&gt;=1.16.0 in /usr/local/lib/python3.7/dist-packages (from catboost) (1.19.5) Requirement already satisfied: plotly in /usr/local/lib/python3.7/dist-packages (from catboost) (5.5.0) Requirement already satisfied: pytz&gt;=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas&gt;=0.24.0-&gt;catboost) (2018.9) Requirement already satisfied: python-dateutil&gt;=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas&gt;=0.24.0-&gt;catboost) (2.8.2) Requirement already satisfied: cycler&gt;=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib-&gt;catboost) (0.11.0) Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,&gt;=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib-&gt;catboost) (3.0.7) Requirement already satisfied: kiwisolver&gt;=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib-&gt;catboost) (1.3.2) Requirement already satisfied: tenacity&gt;=6.2.0 in /usr/local/lib/python3.7/dist-packages (from plotly-&gt;catboost) (8.0.1) Requirement already satisfied: optuna in /usr/local/lib/python3.7/dist-packages (2.10.0) Requirement already satisfied: scipy!=1.4.0 in /usr/local/lib/python3.7/dist-packages (from optuna) (1.4.1) Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from optuna) (4.62.3) Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from optuna) (3.13) Requirement already satisfied: cmaes&gt;=0.8.2 in /usr/local/lib/python3.7/dist-packages (from optuna) (0.8.2) Requirement already satisfied: alembic in /usr/local/lib/python3.7/dist-packages (from optuna) (1.7.6) Requirement already satisfied: colorlog in /usr/local/lib/python3.7/dist-packages (from optuna) (6.6.0) Requirement already satisfied: sqlalchemy&gt;=1.1.0 in /usr/local/lib/python3.7/dist-packages (from optuna) (1.4.31) Requirement already satisfied: cliff in /usr/local/lib/python3.7/dist-packages (from optuna) (3.10.0) Requirement already satisfied: packaging&gt;=20.0 in /usr/local/lib/python3.7/dist-packages (from optuna) (21.3) Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from optuna) (1.19.5) Requirement already satisfied: pyparsing!=3.0.5,&gt;=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging&gt;=20.0-&gt;optuna) (3.0.7) Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.7/dist-packages (from sqlalchemy&gt;=1.1.0-&gt;optuna) (1.1.2) Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from sqlalchemy&gt;=1.1.0-&gt;optuna) (4.10.1) Requirement already satisfied: Mako in /usr/local/lib/python3.7/dist-packages (from alembic-&gt;optuna) (1.1.6) Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from alembic-&gt;optuna) (5.4.0) Requirement already satisfied: pbr!=2.1.0,&gt;=2.0.0 in /usr/local/lib/python3.7/dist-packages (from cliff-&gt;optuna) (5.8.0) Requirement already satisfied: stevedore&gt;=2.0.1 in /usr/local/lib/python3.7/dist-packages (from cliff-&gt;optuna) (3.5.0) Requirement already satisfied: cmd2&gt;=1.0.0 in /usr/local/lib/python3.7/dist-packages (from cliff-&gt;optuna) (2.3.3) Requirement already satisfied: autopage&gt;=0.4.0 in /usr/local/lib/python3.7/dist-packages (from cliff-&gt;optuna) (0.5.0) Requirement already satisfied: PrettyTable&gt;=0.7.2 in /usr/local/lib/python3.7/dist-packages (from cliff-&gt;optuna) (3.0.0) Requirement already satisfied: attrs&gt;=16.3.0 in /usr/local/lib/python3.7/dist-packages (from cmd2&gt;=1.0.0-&gt;cliff-&gt;optuna) (21.4.0) Requirement already satisfied: pyperclip&gt;=1.6 in /usr/local/lib/python3.7/dist-packages (from cmd2&gt;=1.0.0-&gt;cliff-&gt;optuna) (1.8.2) Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from cmd2&gt;=1.0.0-&gt;cliff-&gt;optuna) (3.10.0.2) Requirement already satisfied: wcwidth&gt;=0.1.7 in /usr/local/lib/python3.7/dist-packages (from cmd2&gt;=1.0.0-&gt;cliff-&gt;optuna) (0.2.5) Requirement already satisfied: zipp&gt;=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata-&gt;sqlalchemy&gt;=1.1.0-&gt;optuna) (3.7.0) Requirement already satisfied: MarkupSafe&gt;=0.9.2 in /usr/local/lib/python3.7/dist-packages (from Mako-&gt;alembic-&gt;optuna) (2.0.1) . import pandas as pd import numpy as np from catboost import CatBoostRegressor from tqdm import tqdm from sklearn.model_selection import StratifiedKFold,train_test_split from sklearn.metrics import mean_squared_error import random import optuna from optuna.samplers import TPESampler . path=&#39;/content/drive/MyDrive/Colab Notebooks/물류유통량예측/&#39; . train = pd.read_csv(path+&#39;train_df.csv&#39;, encoding=&#39;CP949&#39;) test = pd.read_csv(path+&#39;test_df.csv&#39;, encoding=&#39;CP949&#39;) submission = pd.read_csv(path+&#39;sample_submission.csv&#39;) . index : 인덱스 | 송하인_격자공간고유번호 | 수하인 격자공간고유번호 | 택배_카테고리 | 운송장_건수 | . 2. Data Preprocess . train.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 32000 entries, 0 to 31999 Data columns (total 6 columns): # Column Non-Null Count Dtype -- -- 0 index 32000 non-null int64 1 SEND_SPG_INNB 32000 non-null int64 2 REC_SPG_INNB 32000 non-null int64 3 DL_GD_LCLS_NM 32000 non-null object 4 DL_GD_MCLS_NM 32000 non-null object 5 INVC_CONT 32000 non-null int64 dtypes: int64(4), object(2) memory usage: 1.5+ MB . train[&#39;SEND_SPG_INNB&#39;].head(10) . 0 1129000014045300 1 1135000009051200 2 1135000030093100 3 1154500002014200 4 1165000021008300 5 1168000013091300 6 1171000019003100 7 2623000012072300 8 2626000011052400 9 2726000034007300 Name: SEND_SPG_INNB, dtype: int64 . train.describe() . INVC_CONT . count 32000.000000 | . mean 4.767875 | . std 5.752122 | . min 3.000000 | . 25% 3.000000 | . 50% 3.000000 | . 75% 5.000000 | . max 239.000000 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; 송하인 격자공간고유번호와 수하인격자공간고유번호가 연속형 자료로 취급하면 안된다고 생각하였습니다. . 격자공간고유번호의 16자리 중 자릿수마다 담고 있는 정보가 다를 것이라 생각하였고, 데이터를 탐색해본 결과 1 ~ 5자리, 6 ~ 9자리, 10, 11 ~ 16자리가 가지고 있는 정보가 다를 것이라 생각했습니다. . 그리고 송하인 격자공간고유번호의 unique 수는 꽤 되지만 수하인 격자공간고유번호의 unique 수는 얼마 되지 않았습니다. . 따라서 송하인 격자공간고유번호는 1 ~ 5, 6 ~ 9, 10, 11 ~ 16자릿수로 나누고 수하인 격자공간고유번호는 자릿수 별로 변수를 생성하였습니다. . 총 22개의 설명변수로 이루어진 데이터로 변환하였습니다. . def numround(number, digit): num=[] while(number!=0): num.append(number % 10) number = number //10 return int(num[-digit]) . for i in tqdm(range(16)): train[f&#39;SEND_SPG_INNB_{i+1}&#39;] = 0 train[f&#39;REC_SPG_INNB_{i+1}&#39;] = 0 test[f&#39;SEND_SPG_INNB_{i+1}&#39;] = 0 test[f&#39;REC_SPG_INNB_{i+1}&#39;] = 0 for j in range(train.shape[0]): train.loc[j,f&#39;SEND_SPG_INNB_{i+1}&#39;]=numround(train.loc[j,&#39;SEND_SPG_INNB&#39;],i+1) train.loc[j,f&#39;REC_SPG_INNB_{i+1}&#39;]=numround(train.loc[j,&#39;REC_SPG_INNB&#39;],i+1) for j in range(test.shape[0]): test.loc[j,f&#39;SEND_SPG_INNB_{i+1}&#39;]=numround(test.loc[j,&#39;SEND_SPG_INNB&#39;],i+1) test.loc[j,f&#39;REC_SPG_INNB_{i+1}&#39;]=numround(test.loc[j,&#39;REC_SPG_INNB&#39;],i+1) . 100%|██████████| 16/16 [07:34&lt;00:00, 28.40s/it] . train[&#39;SEND_SPG_INNB_1~5&#39;]=train[&#39;SEND_SPG_INNB_1&#39;]+train[&#39;SEND_SPG_INNB_2&#39;]+train[&#39;SEND_SPG_INNB_3&#39;]+train[&#39;SEND_SPG_INNB_4&#39;]+train[&#39;SEND_SPG_INNB_5&#39;] train[&#39;SEND_SPG_INNB_6~9&#39;]=train[&#39;SEND_SPG_INNB_6&#39;]+train[&#39;SEND_SPG_INNB_7&#39;]+train[&#39;SEND_SPG_INNB_8&#39;]+train[&#39;SEND_SPG_INNB_9&#39;] train[&#39;SEND_SPG_INNB_10&#39;]=train[&#39;SEND_SPG_INNB_10&#39;] train[&#39;SEND_SPG_INNB_11~16&#39;]=train[&#39;SEND_SPG_INNB_11&#39;]+train[&#39;SEND_SPG_INNB_12&#39;]+train[&#39;SEND_SPG_INNB_13&#39;]+train[&#39;SEND_SPG_INNB_14&#39;]+train[&#39;SEND_SPG_INNB_15&#39;]+train[&#39;SEND_SPG_INNB_16&#39;] test[&#39;SEND_SPG_INNB_1~5&#39;]=test[&#39;SEND_SPG_INNB_1&#39;]+test[&#39;SEND_SPG_INNB_2&#39;]+test[&#39;SEND_SPG_INNB_3&#39;]+test[&#39;SEND_SPG_INNB_4&#39;]+test[&#39;SEND_SPG_INNB_5&#39;] test[&#39;SEND_SPG_INNB_6~9&#39;]=test[&#39;SEND_SPG_INNB_6&#39;]+test[&#39;SEND_SPG_INNB_7&#39;]+test[&#39;SEND_SPG_INNB_8&#39;]+test[&#39;SEND_SPG_INNB_9&#39;] test[&#39;SEND_SPG_INNB_10&#39;]=test[&#39;SEND_SPG_INNB_10&#39;] test[&#39;SEND_SPG_INNB_11~16&#39;]=test[&#39;SEND_SPG_INNB_11&#39;]+test[&#39;SEND_SPG_INNB_12&#39;]+test[&#39;SEND_SPG_INNB_13&#39;]+test[&#39;SEND_SPG_INNB_14&#39;]+test[&#39;SEND_SPG_INNB_15&#39;]+test[&#39;SEND_SPG_INNB_16&#39;] . train.index=train[&#39;index&#39;] test.index=test[&#39;index&#39;] train.drop([&#39;REC_SPG_INNB&#39;,&#39;SEND_SPG_INNB&#39;,&#39;SEND_SPG_INNB_1&#39;,&#39;SEND_SPG_INNB_2&#39;,&#39;SEND_SPG_INNB_3&#39;,&#39;SEND_SPG_INNB_4&#39;,&#39;SEND_SPG_INNB_5&#39;,&#39;SEND_SPG_INNB_6&#39;,&#39;SEND_SPG_INNB_7&#39;, &#39;SEND_SPG_INNB_8&#39;,&#39;SEND_SPG_INNB_9&#39;,&#39;SEND_SPG_INNB_11&#39;,&#39;SEND_SPG_INNB_12&#39;,&#39;SEND_SPG_INNB_13&#39;,&#39;SEND_SPG_INNB_14&#39;,&#39;SEND_SPG_INNB_15&#39;,&#39;SEND_SPG_INNB_16&#39;,&#39;index&#39;],axis=1,inplace=True) test.drop([&#39;REC_SPG_INNB&#39;,&#39;SEND_SPG_INNB&#39;,&#39;SEND_SPG_INNB_1&#39;,&#39;SEND_SPG_INNB_2&#39;,&#39;SEND_SPG_INNB_3&#39;,&#39;SEND_SPG_INNB_4&#39;,&#39;SEND_SPG_INNB_5&#39;,&#39;SEND_SPG_INNB_6&#39;,&#39;SEND_SPG_INNB_7&#39;, &#39;SEND_SPG_INNB_8&#39;,&#39;SEND_SPG_INNB_9&#39;,&#39;SEND_SPG_INNB_11&#39;,&#39;SEND_SPG_INNB_12&#39;,&#39;SEND_SPG_INNB_13&#39;,&#39;SEND_SPG_INNB_14&#39;,&#39;SEND_SPG_INNB_15&#39;,&#39;SEND_SPG_INNB_16&#39;,&#39;index&#39;],axis=1,inplace=True) . for col in test.columns: train[col]=train[col].astype(&#39;category&#39;) test[col]=test[col].astype(&#39;category&#39;) . X = train.drop([&#39;INVC_CONT&#39;],axis=1) y = train[&#39;INVC_CONT&#39;] X_test = test.copy() . X.head() . DL_GD_LCLS_NM DL_GD_MCLS_NM REC_SPG_INNB_1 REC_SPG_INNB_2 REC_SPG_INNB_3 REC_SPG_INNB_4 REC_SPG_INNB_5 REC_SPG_INNB_6 REC_SPG_INNB_7 REC_SPG_INNB_8 REC_SPG_INNB_9 SEND_SPG_INNB_10 REC_SPG_INNB_10 REC_SPG_INNB_11 REC_SPG_INNB_12 REC_SPG_INNB_13 REC_SPG_INNB_14 REC_SPG_INNB_15 REC_SPG_INNB_16 SEND_SPG_INNB_1~5 SEND_SPG_INNB_6~9 SEND_SPG_INNB_11~16 . index . 0 패션의류 | 상의 | 5 | 0 | 1 | 1 | 0 | 0 | 0 | 2 | 2 | 4 | 0 | 0 | 4 | 6 | 3 | 0 | 0 | 13 | 1 | 12 | . 1 생활/건강 | 반려동물 | 5 | 0 | 1 | 1 | 0 | 0 | 0 | 1 | 7 | 9 | 8 | 0 | 3 | 7 | 3 | 0 | 0 | 10 | 0 | 8 | . 2 패션의류 | 기타패션의류 | 5 | 0 | 1 | 1 | 0 | 0 | 0 | 2 | 6 | 0 | 5 | 0 | 9 | 1 | 4 | 0 | 0 | 10 | 3 | 13 | . 3 식품 | 농산물 | 5 | 0 | 1 | 1 | 0 | 0 | 0 | 3 | 1 | 2 | 5 | 0 | 8 | 7 | 4 | 0 | 0 | 16 | 0 | 7 | . 4 식품 | 가공식품 | 5 | 0 | 1 | 1 | 0 | 0 | 0 | 1 | 7 | 1 | 7 | 0 | 5 | 1 | 2 | 0 | 0 | 13 | 2 | 11 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; y.head() . index 0 3 1 3 2 9 3 10 4 3 Name: INVC_CONT, dtype: int64 . X_test.head() . DL_GD_LCLS_NM DL_GD_MCLS_NM REC_SPG_INNB_1 REC_SPG_INNB_2 REC_SPG_INNB_3 REC_SPG_INNB_4 REC_SPG_INNB_5 REC_SPG_INNB_6 REC_SPG_INNB_7 REC_SPG_INNB_8 REC_SPG_INNB_9 SEND_SPG_INNB_10 REC_SPG_INNB_10 REC_SPG_INNB_11 REC_SPG_INNB_12 REC_SPG_INNB_13 REC_SPG_INNB_14 REC_SPG_INNB_15 REC_SPG_INNB_16 SEND_SPG_INNB_1~5 SEND_SPG_INNB_6~9 SEND_SPG_INNB_11~16 . index . 32000 식품 | 농산물 | 1 | 1 | 6 | 5 | 0 | 0 | 0 | 0 | 2 | 3 | 1 | 0 | 9 | 7 | 2 | 0 | 0 | 9 | 4 | 14 | . 32001 식품 | 농산물 | 1 | 1 | 5 | 4 | 5 | 0 | 0 | 0 | 0 | 4 | 2 | 0 | 6 | 6 | 4 | 0 | 0 | 9 | 4 | 8 | . 32002 식품 | 농산물 | 4 | 1 | 3 | 9 | 0 | 0 | 0 | 1 | 0 | 5 | 2 | 0 | 1 | 3 | 2 | 0 | 0 | 9 | 2 | 5 | . 32003 식품 | 농산물 | 4 | 2 | 2 | 1 | 0 | 0 | 0 | 0 | 4 | 5 | 0 | 0 | 9 | 3 | 4 | 0 | 0 | 9 | 2 | 5 | . 32004 식품 | 농산물 | 2 | 7 | 2 | 6 | 0 | 0 | 0 | 0 | 0 | 8 | 4 | 0 | 1 | 7 | 1 | 0 | 0 | 9 | 8 | 6 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; train.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Int64Index: 32000 entries, 0 to 31999 Data columns (total 23 columns): # Column Non-Null Count Dtype -- -- 0 DL_GD_LCLS_NM 32000 non-null category 1 DL_GD_MCLS_NM 32000 non-null category 2 INVC_CONT 32000 non-null int64 3 REC_SPG_INNB_1 32000 non-null category 4 REC_SPG_INNB_2 32000 non-null category 5 REC_SPG_INNB_3 32000 non-null category 6 REC_SPG_INNB_4 32000 non-null category 7 REC_SPG_INNB_5 32000 non-null category 8 REC_SPG_INNB_6 32000 non-null category 9 REC_SPG_INNB_7 32000 non-null category 10 REC_SPG_INNB_8 32000 non-null category 11 REC_SPG_INNB_9 32000 non-null category 12 SEND_SPG_INNB_10 32000 non-null category 13 REC_SPG_INNB_10 32000 non-null category 14 REC_SPG_INNB_11 32000 non-null category 15 REC_SPG_INNB_12 32000 non-null category 16 REC_SPG_INNB_13 32000 non-null category 17 REC_SPG_INNB_14 32000 non-null category 18 REC_SPG_INNB_15 32000 non-null category 19 REC_SPG_INNB_16 32000 non-null category 20 SEND_SPG_INNB_1~5 32000 non-null category 21 SEND_SPG_INNB_6~9 32000 non-null category 22 SEND_SPG_INNB_11~16 32000 non-null category dtypes: category(22), int64(1) memory usage: 1.2 MB . 3.&#47784;&#45944;&#47553; . Optuna로 Random Search를 통해 Catboost 최적의 파라미터를 사용하였습니다. objective 함수의 param에 파라미터를 넣고, 구간을 넣으면 랜덤한 값으로 학습되며 rmse값이 반환되는 함수입니다. &quot;trial&quot;에 반복 횟수를 작성하면 됩니다. . Catboost 특성상 학습이 오래 걸리기 때문에 최적의 파라미터를 찾아 cat_param로 정의하였습니다. . (아래코드는 AIBoo님의 신용카드 사용자 연체 예측 AI 경진대회 [Private 8위 0.66203] | TYKIM | Catboost 코드를 참고하여 수정하였습니다.) . # param = { # &quot;random_state&quot;:42, # &#39;learning_rate&#39; : trial.suggest_loguniform(&#39;learning_rate&#39;, 0.01, 0.05), # &#39;bagging_temperature&#39; :trial.suggest_loguniform(&#39;bagging_temperature&#39;, 0.01, 100.00), # &quot;n_estimators&quot;:trial.suggest_int(&quot;n_estimators&quot;, 500, 5000), # &quot;max_depth&quot;:trial.suggest_int(&quot;max_depth&quot;, 4, 16), # &#39;random_strength&#39; :trial.suggest_int(&#39;random_strength&#39;, 0, 100), # &quot;colsample_bylevel&quot;:trial.suggest_float(&quot;colsample_bylevel&quot;, 0.4, 1.0), # &quot;l2_leaf_reg&quot;:trial.suggest_float(&quot;l2_leaf_reg&quot;,1e-8,3e-5), # &quot;min_child_samples&quot;: trial.suggest_int(&quot;min_child_samples&quot;, 5, 100), # &quot;max_bin&quot;: trial.suggest_int(&quot;max_bin&quot;, 200, 500), # &#39;od_type&#39;: trial.suggest_categorical(&#39;od_type&#39;, [&#39;IncToDec&#39;, &#39;Iter&#39;]), # } # X_train, X_valid, y_train, y_valid = train_test_split(X,y,test_size=0.2) # cat_features = range(X_test.shape[1]) # cat = CatBoostRegressor(**param) # cat.fit(X_train, y_train, # eval_set=[(X_train, y_train), (X_valid,y_valid)], # early_stopping_rounds=35,cat_features=cat_features, # verbose=100) # cat_pred = cat.predict(X_valid) # rmse = np.sqrt(mean_squared_error(y_valid, cat_pred)) # return rmse . # study = optuna.create_study( # study_name = &#39;cat_parameter_opt&#39;, # direction = &#39;minimize&#39;, # sampler = sampler, # ) # study.optimize(objective, n_trials=10) # print(&quot;Best Score:&quot;,study.best_value) # print(&quot;Best trial&quot;,study.best_trial.params) . cat_param={&#39;learning_rate&#39;: 0.018272261776066247, &#39;bagging_temperature&#39;: 63.512210106407046, &#39;n_estimators&#39;: 3794, &#39;max_depth&#39;: 11, &#39;random_strength&#39;: 15, &#39;colsample_bylevel&#39;: 0.49359671220172163, &#39;l2_leaf_reg&#39;: 1.7519275289243016e-06, &#39;min_child_samples&#39;: 88, &#39;max_bin&#39;: 380, &#39;od_type&#39;: &#39;IncToDec&#39; } . skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42) folds = [] for train_idx, valid_idx in skf.split(train, train[&#39;INVC_CONT&#39;]): folds.append((train_idx,valid_idx)) . /usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:680: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=10. UserWarning, . train 데이터를 K-Fold 하여 각 Fold의 학습 값을 가지고 test 예측값을 구한 후 평균을 구하였습니다. . skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42) folds = [] for train_idx, valid_idx in skf.split(train, train[&#39;INVC_CONT&#39;]): folds.append((train_idx,valid_idx)) random.seed(42) cat_models={} cat_features =range(X_test.shape[1]) for fold in range(10): print(f&#39;===================================={fold+1}============================================&#39;) train_idx, valid_idx = folds[fold] X_train = train.drop([&#39;INVC_CONT&#39;],axis=1).iloc[train_idx] X_valid = train.drop([&#39;INVC_CONT&#39;],axis=1).iloc[valid_idx] y_train = train[&#39;INVC_CONT&#39;][train_idx].values y_valid = train[&#39;INVC_CONT&#39;][valid_idx].values cat = CatBoostRegressor(**cat_param) cat.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_valid,y_valid)], early_stopping_rounds=35,cat_features=cat_features, verbose=100) cat_models[fold] = cat print(f&#39;================================================================================ n n&#39;) . ====================================1============================================ . /usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:680: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=10. UserWarning, . 0: learn: 5.6240726 test: 5.6244095 test1: 6.7835557 best: 6.7835557 (0) total: 498ms remaining: 31m 30s 100: learn: 5.4960942 test: 5.5301616 test1: 6.6675145 best: 6.6675145 (100) total: 24.7s remaining: 15m 2s 200: learn: 5.4405946 test: 5.4798163 test1: 6.6059558 best: 6.6059558 (198) total: 36.2s remaining: 10m 47s 300: learn: 5.4140537 test: 5.4605846 test1: 6.5849293 best: 6.5849293 (300) total: 47.3s remaining: 9m 9s 400: learn: 5.3916725 test: 5.4422328 test1: 6.5653535 best: 6.5653535 (400) total: 57.7s remaining: 8m 7s 500: learn: 5.3395210 test: 5.3956074 test1: 6.5185137 best: 6.5185137 (500) total: 1m 8s remaining: 7m 30s 600: learn: 5.2901206 test: 5.3652920 test1: 6.4707527 best: 6.4707527 (600) total: 1m 19s remaining: 7m 4s 700: learn: 5.0282278 test: 5.2082914 test1: 6.2761017 best: 6.2761017 (700) total: 1m 40s remaining: 7m 23s 800: learn: 4.7363885 test: 5.0484783 test1: 6.1402382 best: 6.1401487 (798) total: 2m 11s remaining: 8m 11s 900: learn: 4.5672517 test: 4.9696530 test1: 6.0793294 best: 6.0788567 (881) total: 2m 42s remaining: 8m 41s 1000: learn: 4.4406792 test: 4.9196033 test1: 6.0718066 best: 6.0716713 (990) total: 3m 12s remaining: 8m 58s Stopped by overfitting detector (35 iterations wait) bestTest = 6.068832186 bestIteration = 1044 Shrink model to first 1045 iterations. ================================================================================ ====================================2============================================ 0: learn: 5.7998015 test: 5.7997917 test1: 5.2928846 best: 5.2928846 (0) total: 65.4ms remaining: 4m 8s 100: learn: 5.6398961 test: 5.7026359 test1: 5.2344849 best: 5.2344849 (100) total: 13.9s remaining: 8m 26s 200: learn: 5.5727985 test: 5.6661956 test1: 5.2212126 best: 5.2212126 (200) total: 24.1s remaining: 7m 10s 300: learn: 5.4880491 test: 5.6112859 test1: 5.2099747 best: 5.2099747 (300) total: 36.2s remaining: 6m 59s 400: learn: 5.4288432 test: 5.5728402 test1: 5.2043505 best: 5.2043505 (400) total: 47.2s remaining: 6m 39s 500: learn: 5.3730200 test: 5.5308387 test1: 5.1973897 best: 5.1973897 (500) total: 57.8s remaining: 6m 19s 600: learn: 5.2682635 test: 5.4467884 test1: 5.1827241 best: 5.1823130 (598) total: 1m 11s remaining: 6m 17s 700: learn: 5.0329989 test: 5.2904110 test1: 5.1763991 best: 5.1754082 (686) total: 1m 40s remaining: 7m 25s 800: learn: 4.7760561 test: 5.1586431 test1: 5.1691799 best: 5.1689916 (799) total: 2m 17s remaining: 8m 35s 900: learn: 4.5718300 test: 5.0578540 test1: 5.1657950 best: 5.1656201 (881) total: 2m 48s remaining: 9m Stopped by overfitting detector (35 iterations wait) bestTest = 5.163419657 bestIteration = 958 Shrink model to first 959 iterations. ================================================================================ ====================================3============================================ 0: learn: 5.7954233 test: 5.7954087 test1: 5.3358658 best: 5.3358658 (0) total: 61ms remaining: 3m 51s 100: learn: 5.6510996 test: 5.6987296 test1: 5.2296969 best: 5.2296969 (100) total: 11.8s remaining: 7m 12s 200: learn: 5.5891938 test: 5.6632554 test1: 5.1978101 best: 5.1978101 (200) total: 25.6s remaining: 7m 36s 300: learn: 5.5205015 test: 5.6164968 test1: 5.1640026 best: 5.1639988 (294) total: 38.8s remaining: 7m 30s 400: learn: 5.4715385 test: 5.5996822 test1: 5.1517844 best: 5.1517844 (400) total: 51.3s remaining: 7m 14s 500: learn: 5.4126707 test: 5.5432101 test1: 5.1051460 best: 5.1051388 (499) total: 1m 1s remaining: 6m 42s 600: learn: 5.3683705 test: 5.5178964 test1: 5.0840610 best: 5.0840609 (599) total: 1m 11s remaining: 6m 20s 700: learn: 5.1067467 test: 5.3031105 test1: 4.9499417 best: 4.9499417 (700) total: 1m 33s remaining: 6m 51s 800: learn: 4.8336476 test: 5.1569749 test1: 4.8936226 best: 4.8936226 (800) total: 2m 3s remaining: 7m 40s 900: learn: 4.6463427 test: 5.0731393 test1: 4.8750691 best: 4.8749592 (899) total: 2m 33s remaining: 8m 14s 1000: learn: 4.4988646 test: 5.0119748 test1: 4.8621792 best: 4.8619472 (998) total: 3m 4s remaining: 8m 34s Stopped by overfitting detector (35 iterations wait) bestTest = 4.857927859 bestIteration = 1045 Shrink model to first 1046 iterations. ================================================================================ ====================================4============================================ 0: learn: 5.6797253 test: 5.6797053 test1: 6.3581339 best: 6.3581339 (0) total: 61.3ms remaining: 3m 52s 100: learn: 5.5764211 test: 5.5851445 test1: 6.2563079 best: 6.2563079 (100) total: 12.6s remaining: 7m 42s 200: learn: 5.5165005 test: 5.5369824 test1: 6.1989641 best: 6.1989641 (200) total: 25.9s remaining: 7m 42s 300: learn: 5.4783855 test: 5.4982155 test1: 6.1413561 best: 6.1413522 (295) total: 36.6s remaining: 7m 4s 400: learn: 5.4373504 test: 5.4615927 test1: 6.0793381 best: 6.0793381 (400) total: 45.6s remaining: 6m 25s 500: learn: 5.3895524 test: 5.4217469 test1: 6.0430049 best: 6.0429846 (497) total: 58.1s remaining: 6m 21s 600: learn: 5.3229585 test: 5.3672285 test1: 5.9601014 best: 5.9601014 (600) total: 1m 12s remaining: 6m 26s 700: learn: 5.0641650 test: 5.1685397 test1: 5.6283284 best: 5.6282761 (699) total: 1m 34s remaining: 6m 56s 800: learn: 4.8122561 test: 5.0117926 test1: 5.4345803 best: 5.4345803 (800) total: 2m 4s remaining: 7m 44s 900: learn: 4.5865353 test: 4.9277124 test1: 5.3770698 best: 5.3770698 (900) total: 2m 34s remaining: 8m 16s Stopped by overfitting detector (35 iterations wait) bestTest = 5.367815232 bestIteration = 948 Shrink model to first 949 iterations. ================================================================================ ====================================5============================================ 0: learn: 5.8127429 test: 5.8127445 test1: 5.1631856 best: 5.1631856 (0) total: 59.8ms remaining: 3m 46s 100: learn: 5.7174157 test: 5.7421257 test1: 5.1135334 best: 5.1135334 (100) total: 12.1s remaining: 7m 21s 200: learn: 5.6513823 test: 5.6897737 test1: 5.0990861 best: 5.0990861 (200) total: 23.2s remaining: 6m 54s 300: learn: 5.5762771 test: 5.6375500 test1: 5.0975761 best: 5.0945176 (267) total: 34.8s remaining: 6m 43s Stopped by overfitting detector (35 iterations wait) bestTest = 5.094517591 bestIteration = 267 Shrink model to first 268 iterations. ================================================================================ ====================================6============================================ 0: learn: 5.8718289 test: 5.8718225 test1: 4.5217283 best: 4.5217283 (0) total: 63.6ms remaining: 4m 1s 100: learn: 5.7675896 test: 5.7717752 test1: 4.4536227 best: 4.4536227 (100) total: 12.4s remaining: 7m 33s 200: learn: 5.7072065 test: 5.7210221 test1: 4.4333057 best: 4.4333057 (200) total: 24.5s remaining: 7m 18s 300: learn: 5.6531517 test: 5.6372884 test1: 4.4228929 best: 4.4228700 (299) total: 36.1s remaining: 6m 59s 400: learn: 5.5785283 test: 5.5630332 test1: 4.4138547 best: 4.4138547 (400) total: 48.7s remaining: 6m 52s 500: learn: 5.5368764 test: 5.5235730 test1: 4.4087818 best: 4.4087818 (500) total: 1m remaining: 6m 39s 600: learn: 5.4462576 test: 5.4463752 test1: 4.4044104 best: 4.4044104 (600) total: 1m 14s remaining: 6m 35s 700: learn: 5.1627216 test: 5.2381077 test1: 4.3989134 best: 4.3981901 (695) total: 1m 33s remaining: 6m 50s 800: learn: 4.8764121 test: 5.1291066 test1: 4.3813348 best: 4.3810835 (798) total: 2m 2s remaining: 7m 39s Stopped by overfitting detector (35 iterations wait) bestTest = 4.375711979 bestIteration = 855 Shrink model to first 856 iterations. ================================================================================ ====================================7============================================ 0: learn: 5.5896379 test: 5.5896739 test1: 7.0398490 best: 7.0398490 (0) total: 62.8ms remaining: 3m 58s 100: learn: 5.4895956 test: 5.5207092 test1: 6.9712621 best: 6.9712621 (100) total: 11.6s remaining: 7m 2s 200: learn: 5.4368356 test: 5.4905300 test1: 6.9470753 best: 6.9470753 (200) total: 22.4s remaining: 6m 41s 300: learn: 5.4108806 test: 5.4764698 test1: 6.9373469 best: 6.9373353 (298) total: 32.4s remaining: 6m 16s 400: learn: 5.3796268 test: 5.4556132 test1: 6.8839617 best: 6.8839617 (400) total: 43.7s remaining: 6m 9s 500: learn: 5.3317076 test: 5.4298767 test1: 6.8320513 best: 6.8320250 (497) total: 55.8s remaining: 6m 6s 600: learn: 5.2595996 test: 5.3902005 test1: 6.7159785 best: 6.7159785 (600) total: 1m 10s remaining: 6m 12s 700: learn: 5.1266059 test: 5.3415476 test1: 6.6241471 best: 6.6238443 (696) total: 1m 30s remaining: 6m 38s 800: learn: 4.8878021 test: 5.2295321 test1: 6.3647524 best: 6.3647524 (800) total: 2m remaining: 7m 30s 900: learn: 4.6774448 test: 5.1363369 test1: 6.2224111 best: 6.2201793 (898) total: 2m 30s remaining: 8m 4s 1000: learn: 4.5147885 test: 5.0836647 test1: 6.1634589 best: 6.1634589 (1000) total: 3m 1s remaining: 8m 25s 1100: learn: 4.3961307 test: 5.0453149 test1: 6.1036841 best: 6.1036841 (1100) total: 3m 31s remaining: 8m 38s 1200: learn: 4.2735652 test: 5.0100648 test1: 6.0608318 best: 6.0608318 (1200) total: 4m 2s remaining: 8m 43s 1300: learn: 4.1715964 test: 4.9854930 test1: 6.0428858 best: 6.0428858 (1300) total: 4m 32s remaining: 8m 42s 1400: learn: 4.0832148 test: 4.9626169 test1: 6.0374177 best: 6.0363832 (1389) total: 5m 3s remaining: 8m 38s Stopped by overfitting detector (35 iterations wait) bestTest = 6.036383207 bestIteration = 1389 Shrink model to first 1390 iterations. ================================================================================ ====================================8============================================ 0: learn: 5.7761549 test: 5.7761372 test1: 5.5206589 best: 5.5206589 (0) total: 60.1ms remaining: 3m 48s 100: learn: 5.6259120 test: 5.6818344 test1: 5.4627034 best: 5.4627034 (100) total: 12.7s remaining: 7m 45s 200: learn: 5.5214357 test: 5.6009615 test1: 5.4442310 best: 5.4442310 (200) total: 24.1s remaining: 7m 10s 300: learn: 5.4240376 test: 5.4945688 test1: 5.4326722 best: 5.4326722 (299) total: 35.9s remaining: 6m 56s 400: learn: 5.3796432 test: 5.4740359 test1: 5.4293208 best: 5.4293208 (400) total: 45.8s remaining: 6m 27s 500: learn: 5.3351087 test: 5.4464290 test1: 5.4225107 best: 5.4225107 (500) total: 57.3s remaining: 6m 16s Stopped by overfitting detector (35 iterations wait) bestTest = 5.42192434 bestIteration = 518 Shrink model to first 519 iterations. ================================================================================ ====================================9============================================ 0: learn: 5.8113326 test: 5.8113592 test1: 5.1774233 best: 5.1774233 (0) total: 66.7ms remaining: 4m 12s 100: learn: 5.6893174 test: 5.7115796 test1: 5.0706084 best: 5.0706084 (100) total: 13.9s remaining: 8m 28s 200: learn: 5.6380443 test: 5.6733641 test1: 5.0465860 best: 5.0465860 (200) total: 23s remaining: 6m 51s 300: learn: 5.5796243 test: 5.6231843 test1: 5.0204694 best: 5.0201020 (293) total: 33.9s remaining: 6m 33s 400: learn: 5.5339410 test: 5.6036245 test1: 5.0077692 best: 5.0077644 (399) total: 45.4s remaining: 6m 24s 500: learn: 5.4983008 test: 5.5786004 test1: 4.9919552 best: 4.9919552 (500) total: 58.6s remaining: 6m 25s 600: learn: 5.4141082 test: 5.5007961 test1: 4.9789318 best: 4.9789318 (600) total: 1m 12s remaining: 6m 24s 700: learn: 5.1832211 test: 5.3333448 test1: 4.9449670 best: 4.9445323 (696) total: 1m 34s remaining: 6m 57s 800: learn: 4.9239177 test: 5.1933113 test1: 4.9167434 best: 4.9167405 (799) total: 2m 3s remaining: 7m 42s 900: learn: 4.7601542 test: 5.1326880 test1: 4.9092309 best: 4.9091543 (899) total: 2m 34s remaining: 8m 15s 1000: learn: 4.6155194 test: 5.0813784 test1: 4.9029460 best: 4.9019464 (991) total: 3m 4s remaining: 8m 35s Stopped by overfitting detector (35 iterations wait) bestTest = 4.899096889 bestIteration = 1043 Shrink model to first 1044 iterations. ================================================================================ ====================================10============================================ 0: learn: 5.7433502 test: 5.7433425 test1: 5.8209412 best: 5.8209412 (0) total: 60ms remaining: 3m 47s 100: learn: 5.5948456 test: 5.6177444 test1: 5.7560805 best: 5.7560805 (100) total: 12.4s remaining: 7m 33s 200: learn: 5.5226384 test: 5.5328046 test1: 5.7386751 best: 5.7386742 (199) total: 23.8s remaining: 7m 5s 300: learn: 5.4668698 test: 5.4790270 test1: 5.7304271 best: 5.7304151 (299) total: 34.9s remaining: 6m 44s 400: learn: 5.4168370 test: 5.4306267 test1: 5.7260604 best: 5.7253147 (390) total: 45.9s remaining: 6m 28s Stopped by overfitting detector (35 iterations wait) bestTest = 5.725314711 bestIteration = 390 Shrink model to first 391 iterations. ================================================================================ . submission.loc[:,&#39;INVC_CONT&#39;]=0 for fold in range(10): submission.loc[:,&#39;INVC_CONT&#39;] += cat_models[fold].predict(test)/10 . train을 K-fold한 값의 평균을 구하다 보니 예측값의 극단값이 작아질 수 밖에 없습니다. . 따라서 INVC_CONT가 30 이상인 값에 적당한 값을 곱하여서 조정하였습니다. . submission.loc[submission.INVC_CONT&gt;30,&#39;INVC_CONT&#39;]=submission.loc[submission.INVC_CONT&gt;30,&#39;INVC_CONT&#39;]*4.8 .",
            "url": "https://shw9807.github.io/shw9807blog/%EB%8D%B0%EC%9D%B4%EC%BD%98/2022/02/03/%EB%AC%BC%EB%A5%98%EC%9C%A0%ED%86%B5%EB%9F%89_%EC%98%88%EC%B8%A1.html",
            "relUrl": "/%EB%8D%B0%EC%9D%B4%EC%BD%98/2022/02/03/%EB%AC%BC%EB%A5%98%EC%9C%A0%ED%86%B5%EB%9F%89_%EC%98%88%EC%B8%A1.html",
            "date": " • Feb 3, 2022"
        }
        
    
  
    
        ,"post4": {
            "title": "[Colab] 펭귄 몸무게 예측",
            "content": "&#54189;&#44484; &#47800;&#47924;&#44172; &#50696;&#52769; . LIBRARY . import pandas as pd import numpy as np import seaborn as sns import matplotlib.pyplot as plt import matplotlib get_ipython().run_line_magic(&#39;matplotlib&#39;, &#39;inline&#39;) plt.style.use(&quot;seaborn&quot;) . DATA LOADING . !unzip -uq &quot;/content/drive/MyDrive/Colab Notebooks/펭귄몸무게/dataset.zip&quot; -d &quot;/content/drive/MyDrive/Colab Notebooks/펭귄몸무게&quot; . train = pd.read_csv(&quot;/content/drive/MyDrive/Colab Notebooks/펭귄몸무게/train.csv&quot;) test = pd.read_csv(&quot;/content/drive/MyDrive/Colab Notebooks/펭귄몸무게/test.csv&quot;) . train.head() . id Species Island Clutch Completion Culmen Length (mm) Culmen Depth (mm) Flipper Length (mm) Sex Delta 15 N (o/oo) Delta 13 C (o/oo) Body Mass (g) . 0 0 | Gentoo penguin (Pygoscelis papua) | Biscoe | Yes | 50.0 | 15.3 | 220 | MALE | 8.30515 | -25.19017 | 5550 | . 1 1 | Chinstrap penguin (Pygoscelis antarctica) | Dream | No | 49.5 | 19.0 | 200 | MALE | 9.63074 | -24.34684 | 3800 | . 2 2 | Gentoo penguin (Pygoscelis papua) | Biscoe | Yes | 45.1 | 14.4 | 210 | FEMALE | 8.51951 | -27.01854 | 4400 | . 3 3 | Gentoo penguin (Pygoscelis papua) | Biscoe | Yes | 44.5 | 14.7 | 214 | FEMALE | 8.20106 | -26.16524 | 4850 | . 4 4 | Gentoo penguin (Pygoscelis papua) | Biscoe | No | 49.6 | 16.0 | 225 | MALE | 8.38324 | -26.84272 | 5700 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; id : 샘플 아이디 | Species: 펭귄의 종을 나타내는 문자열 | Island : 샘플들이 수집된 Palmer Station 근처 섬 이름 | Clutch Completion : 관찰된 펭귄 둥지의 알이 2개인 경우 Full Clutch이며 Yes로 표기 | Culmen Length (mm) : 펭귄 옆모습 기준 부리의 가로 길이 | Culmen Depth (mm) : 펭귄 옆모습 기준 부리의 세로 길이 | Flipper Length (mm) : 펭귄의 팔(날개) 길이 | Sex : 펭귄의 성별 | Delta 15 N (o/oo) : 토양에 따라 변화하는 안정 동위원소 15N:14N의 비율 | Delta 13 C (o/oo) : 먹이에 따라 변화하는 안정 동위원소 13C:12C의 비율 | Body Mass (g): 펭귄의 몸무게를 나타내는 숫자 (g) | . DATA CLEANSING &amp; ANALYSIS . train.drop([&#39;id&#39;],axis=1,inplace=True) test.drop([&#39;id&#39;],axis=1,inplace=True) . train.shape,test.shape . ((114, 10), (228, 9)) . train.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 114 entries, 0 to 113 Data columns (total 10 columns): # Column Non-Null Count Dtype -- -- 0 Species 114 non-null object 1 Island 114 non-null object 2 Clutch Completion 114 non-null object 3 Culmen Length (mm) 114 non-null float64 4 Culmen Depth (mm) 114 non-null float64 5 Flipper Length (mm) 114 non-null int64 6 Sex 111 non-null object 7 Delta 15 N (o/oo) 111 non-null float64 8 Delta 13 C (o/oo) 111 non-null float64 9 Body Mass (g) 114 non-null int64 dtypes: float64(4), int64(2), object(4) memory usage: 9.0+ KB . train.describe() . Culmen Length (mm) Culmen Depth (mm) Flipper Length (mm) Delta 15 N (o/oo) Delta 13 C (o/oo) Body Mass (g) . count 114.000000 | 114.000000 | 114.000000 | 111.000000 | 111.000000 | 114.000000 | . mean 44.613158 | 17.014912 | 203.052632 | 8.737634 | -25.723051 | 4327.850877 | . std 5.321829 | 1.941363 | 14.653425 | 0.567698 | 0.859786 | 781.766484 | . min 33.500000 | 13.200000 | 174.000000 | 7.632200 | -27.018540 | 2700.000000 | . 25% 40.325000 | 15.225000 | 190.000000 | 8.272585 | -26.434025 | 3675.000000 | . 50% 45.200000 | 17.250000 | 199.000000 | 8.632590 | -25.955410 | 4250.000000 | . 75% 49.075000 | 18.600000 | 216.000000 | 9.264635 | -25.005945 | 4850.000000 | . max 55.100000 | 21.100000 | 231.000000 | 10.025440 | -24.102550 | 6300.000000 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; target = train[&#39;Body Mass (g)&#39;] object_columns = [&#39;Species&#39;,&#39;Island&#39;,&#39;Clutch Completion&#39;,&#39;Sex&#39;] numerical_columns= [&#39;Culmen Depth (mm)&#39;,&#39;Culmen Length (mm)&#39;,&#39;Flipper Length (mm)&#39;,&#39;Delta 15 N (o/oo)&#39;,&#39;Delta 13 C (o/oo)&#39;] . Object Columns . for _ in object_columns: print(_) print(train[_].value_counts()) plt.figure(figsize=(15,5)) sns.countplot(x=train[_],palette=&#39;Set3&#39;) plt.show() . Species Gentoo penguin (Pygoscelis papua) 48 Adelie Penguin (Pygoscelis adeliae) 41 Chinstrap penguin (Pygoscelis antarctica) 25 Name: Species, dtype: int64 . Island Biscoe 57 Dream 44 Torgersen 13 Name: Island, dtype: int64 . Clutch Completion Yes 102 No 12 Name: Clutch Completion, dtype: int64 . Sex MALE 56 FEMALE 55 Name: Sex, dtype: int64 . Numerical Columns . plt.style.use(&#39;fivethirtyeight&#39;) for _ in numerical_columns: print(_) f,ax=plt.subplots(1,2,figsize=(16,4)) plt.figure(figsize=(15,5)) sns.histplot(x=train[_],ax=ax[0],kde=True) sns.boxplot(x=train[_],ax=ax[1],color=&#39;white&#39;) plt.show() . Culmen Depth (mm) . &lt;Figure size 1080x360 with 0 Axes&gt; . Culmen Length (mm) . &lt;Figure size 1080x360 with 0 Axes&gt; . Flipper Length (mm) . &lt;Figure size 1080x360 with 0 Axes&gt; . Delta 15 N (o/oo) . &lt;Figure size 1080x360 with 0 Axes&gt; . Delta 13 C (o/oo) . &lt;Figure size 1080x360 with 0 Axes&gt; . f,ax= plt.subplots(2,1,figsize=(9,8)) sns.swarmplot(data=train, x=&#39;Species&#39;,y=&#39;Body Mass (g)&#39;,ax=ax[0]); ax[0].set_title(&quot;Body Mass(g) by Species&quot;) sns.swarmplot(data=train, x=&#39;Species&#39;,y=&#39;Body Mass (g)&#39;,hue=&#39;Sex&#39;,ax=ax[1]); ax[1].set_title(&quot;Body Mass(g) by [Species, Sex]&quot;) plt.tight_layout() #Show clearly. . f,ax=plt.subplots(1,3,figsize=(15,5)) sns.regplot(data=train, x=&#39;Culmen Length (mm)&#39;,y=&#39;Body Mass (g)&#39;,ax=ax[0],color=&#39;red&#39;) sns.regplot(data=train, x=&#39;Culmen Depth (mm)&#39;,y=&#39;Body Mass (g)&#39;,ax=ax[1],color=&#39;blue&#39;) sns.regplot(data=train, x=&#39;Flipper Length (mm)&#39;,y=&#39;Body Mass (g)&#39;,ax=ax[2],color=&#39;green&#39;) plt.tight_layout() . Observation . [Culmen Length (mm), Culmen Depth (mm), Flipper Length (mm)] 컬럼들이 전반적으로 Body Mass (g)과 상관관계를 가진다고 할 수 있습니다. . plt.style.use(&#39;seaborn&#39;) g= sns.pairplot(train,hue=&#39;Species&#39;) g.map_lower(sns.regplot); . plt.figure(figsize=(16,8)) sns.heatmap(train.corr(),annot=True,vmin=-1, vmax=1); . Processing . &#44208;&#52769;&#44050; &#52376;&#47532; . train.isnull().sum() . Species 0 Island 0 Clutch Completion 0 Culmen Length (mm) 0 Culmen Depth (mm) 0 Flipper Length (mm) 0 Sex 3 Delta 15 N (o/oo) 3 Delta 13 C (o/oo) 3 Body Mass (g) 0 dtype: int64 . Delta 피쳐들은 평균으로 채워볼 수 있습니다.. 또한, Sex 피쳐를 다루기 위해서 drop이나 mode로 채울 수 있습니다. 그러나 데이터가 적기 때문에 &#39;합리적으로&#39; 결측값을 채워봅시다. . train.corr()[&#39;Body Mass (g)&#39;] . Culmen Length (mm) 0.572063 Culmen Depth (mm) -0.490643 Flipper Length (mm) 0.864814 Delta 15 N (o/oo) -0.548678 Delta 13 C (o/oo) -0.468425 Body Mass (g) 1.000000 Name: Body Mass (g), dtype: float64 . sns.scatterplot(data=train, x=&#39;Flipper Length (mm)&#39;, y= &#39;Body Mass (g)&#39;, hue=&#39;Sex&#39;); . Flipper Length (mm) &amp; Body Mass (g)의 상관관계가 높은 것으로 보입니다.또한 성별로 나누었을 때도 구분이 됩니다. . train.groupby([&#39;Species&#39;,&#39;Sex&#39;])[&#39;Flipper Length (mm)&#39;].mean().reset_index() . Species Sex Flipper Length (mm) . 0 Adelie Penguin (Pygoscelis adeliae) | FEMALE | 187.166667 | . 1 Adelie Penguin (Pygoscelis adeliae) | MALE | 191.619048 | . 2 Chinstrap penguin (Pygoscelis antarctica) | FEMALE | 192.642857 | . 3 Chinstrap penguin (Pygoscelis antarctica) | MALE | 200.454545 | . 4 Gentoo penguin (Pygoscelis papua) | FEMALE | 213.217391 | . 5 Gentoo penguin (Pygoscelis papua) | MALE | 223.000000 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; train[train[&#39;Sex&#39;].isnull()] . Species Island Clutch Completion Culmen Length (mm) Culmen Depth (mm) Flipper Length (mm) Sex Delta 15 N (o/oo) Delta 13 C (o/oo) Body Mass (g) . 6 Adelie Penguin (Pygoscelis adeliae) | Torgersen | Yes | 42.0 | 20.2 | 190 | NaN | 9.13362 | -25.09368 | 4250 | . 8 Adelie Penguin (Pygoscelis adeliae) | Torgersen | Yes | 34.1 | 18.1 | 193 | NaN | NaN | NaN | 3475 | . 70 Gentoo penguin (Pygoscelis papua) | Biscoe | Yes | 46.2 | 14.4 | 214 | NaN | 8.24253 | -26.81540 | 4650 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; 예를 들어, 6번의 인덱스를 보면. Adelie Penguin (Pygoscelis adeliae 종의 Flipper Length (mm)는 190입니다. 위의 표와 함께 유추하면, 성별은 &#39;MALE&#39;로 유추해볼 수 있습니다. . (결측값이 많을 때에는 좋은 방법이 아닐 겁니다.) . test.groupby([&#39;Species&#39;,&#39;Sex&#39;])[&#39;Flipper Length (mm)&#39;].mean().reset_index() . Species Sex Flipper Length (mm) . 0 Adelie Penguin (Pygoscelis adeliae) | FEMALE | 188.000000 | . 1 Adelie Penguin (Pygoscelis adeliae) | MALE | 192.730769 | . 2 Chinstrap penguin (Pygoscelis antarctica) | FEMALE | 191.100000 | . 3 Chinstrap penguin (Pygoscelis antarctica) | MALE | 199.652174 | . 4 Gentoo penguin (Pygoscelis papua) | FEMALE | 212.371429 | . 5 Gentoo penguin (Pygoscelis papua) | MALE | 220.594595 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; test[test[&#39;Sex&#39;].isnull()] . Species Island Clutch Completion Culmen Length (mm) Culmen Depth (mm) Flipper Length (mm) Sex Delta 15 N (o/oo) Delta 13 C (o/oo) . 46 Adelie Penguin (Pygoscelis adeliae) | Torgersen | Yes | 37.8 | 17.1 | 186.0 | NaN | 8.63243 | -25.21315 | . 81 Adelie Penguin (Pygoscelis adeliae) | Dream | Yes | 37.5 | 18.9 | 179.0 | NaN | NaN | NaN | . 98 Gentoo penguin (Pygoscelis papua) | Biscoe | Yes | 47.3 | 13.8 | 216.0 | NaN | 8.25818 | -26.23886 | . 152 Gentoo penguin (Pygoscelis papua) | Biscoe | Yes | 44.5 | 15.7 | 217.0 | NaN | 8.04111 | -26.18444 | . 205 Adelie Penguin (Pygoscelis adeliae) | Torgersen | Yes | 37.8 | 17.3 | 180.0 | NaN | NaN | NaN | . 209 Gentoo penguin (Pygoscelis papua) | Biscoe | Yes | 44.5 | 14.3 | 216.0 | NaN | 7.96621 | -25.69327 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; train.fillna(train.mean(), inplace = True) test.fillna(train.mean(), inplace = True) train.loc[6,&#39;Sex&#39;]=&#39;MALE&#39; train.loc[8,&#39;Sex&#39;]=&#39;MALE&#39; train.loc[70,&#39;Sex&#39;]=&#39;FEMALE&#39; #test Dataset test.loc[46,&#39;Sex&#39;]=&#39;FEMALE&#39; test.loc[81,&#39;Sex&#39;]=&#39;FEMALE&#39; test.loc[98,&#39;Sex&#39;]=&#39;MALE&#39; test.loc[152,&#39;Sex&#39;]=&#39;MALE&#39; test.loc[205,&#39;Sex&#39;]=&#39;FEMALE&#39; test.loc[209,&#39;Sex&#39;]=&#39;FEMALE&#39; . Encoding . train = pd.get_dummies(train) test = pd.get_dummies(test) . Scale . from sklearn.preprocessing import StandardScaler ss=StandardScaler() train_scaler=ss.fit_transform(train[numerical_columns]) train[numerical_columns] = pd.DataFrame(data=train_scaler, columns=numerical_columns) test_scaler= ss.transform(test[numerical_columns]) test[numerical_columns] = pd.DataFrame(data=test_scaler, columns=numerical_columns) . standardscaler를 사용해 수치형 칼럼들의 값을 평균0, 분산1로 조정해주었다. . MODELING . from sklearn.model_selection import train_test_split from sklearn.linear_model import Ridge,BayesianRidge . X=train.drop([&#39;Body Mass (g)&#39;],axis=1) y= train[&#39;Body Mass (g)&#39;] X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42) . import matplotlib.pyplot as plt # alpha 값을 바꿀 때마다 score() 메서드의 결과를 저장할 리스트 train_score = [] test_score = [] . from sklearn.linear_model import Ridge from sklearn.model_selection import cross_val_score alphas = [0,0.01,0.1,1,10,100] for alpha in alphas: ridge = Ridge(alpha=alpha) ridge.fit(X_train,y_train) train_score.append(ridge.score(X_train,y_train)) test_score.append(ridge.score(X_test, y_test)) neg_mse_scores = cross_val_score(ridge, X, y, scoring = &#39;neg_mean_squared_error&#39;, cv = 5) avg_rmse = np.mean(np.sqrt(-neg_mse_scores)) print(&#39;alpha 값 &#39;, alpha, &#39;일때 평균 rmse :&#39;, np.round(avg_rmse,4)) . alpha 값 0 일때 평균 rmse : 341.8353 alpha 값 0.01 일때 평균 rmse : 335.4318 alpha 값 0.1 일때 평균 rmse : 334.5428 alpha 값 1 일때 평균 rmse : 330.299 alpha 값 10 일때 평균 rmse : 328.4763 alpha 값 100 일때 평균 rmse : 404.5087 . plt.plot(np.log10(alphas), train_score) plt.plot(np.log10(alphas), test_score) plt.xlabel(&#39;alpha&#39;) plt.ylabel(&#39;R^2&#39;) plt.show() . /usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: RuntimeWarning: divide by zero encountered in log10 &#34;&#34;&#34;Entry point for launching an IPython kernel. /usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: RuntimeWarning: divide by zero encountered in log10 . ridge = Ridge(alpha=0.01) ridge.fit(X_train,y_train) pred= ridge.predict(X_test) print(ridge.score(X_test,y_test)) . 0.8290727649695923 . ridge = Ridge(alpha=1) ridge.fit(X_train,y_train) pred= ridge.predict(X_test) print(ridge.score(X_test,y_test)) . 0.8255628368059518 . EVALUATION . import numpy as np def RMSE(true, pred): score = np.sqrt(np.mean(np.square(true-pred))) return score RMSE(y_test,pred) . 311.60898494936004 . import numpy as np def RMSE(true, pred): score = np.sqrt(np.mean(np.square(true-pred))) return score RMSE(y_test,pred) . 314.79211507025906 .",
            "url": "https://shw9807.github.io/shw9807blog/%EB%8D%B0%EC%9D%B4%EC%BD%98/2022/01/27/%ED%8E%AD%EA%B7%84%EB%AA%B8%EB%AC%B4%EA%B2%8C_%EC%98%88%EC%B8%A1.html",
            "relUrl": "/%EB%8D%B0%EC%9D%B4%EC%BD%98/2022/01/27/%ED%8E%AD%EA%B7%84%EB%AA%B8%EB%AC%B4%EA%B2%8C_%EC%98%88%EC%B8%A1.html",
            "date": " • Jan 27, 2022"
        }
        
    
  
    
        ,"post5": {
            "title": "[Colab] 타이타닉 데이터 분석",
            "content": "&#53440;&#51060;&#53440;&#45769; &#45936;&#51060;&#53552; &#48516;&#49437; . !sudo apt-get install -y fonts-nanum !sudo fc-cache -fv !rm ~/.cache/matplotlib -rf . Reading package lists... Done Building dependency tree Reading state information... Done The following NEW packages will be installed: fonts-nanum 0 upgraded, 1 newly installed, 0 to remove and 37 not upgraded. Need to get 9,604 kB of archives. After this operation, 29.5 MB of additional disk space will be used. Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 fonts-nanum all 20170925-1 [9,604 kB] Fetched 9,604 kB in 2s (6,127 kB/s) debconf: unable to initialize frontend: Dialog debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, &lt;&gt; line 1.) debconf: falling back to frontend: Readline debconf: unable to initialize frontend: Readline debconf: (This frontend requires a controlling tty.) debconf: falling back to frontend: Teletype dpkg-preconfigure: unable to re-open stdin: Selecting previously unselected package fonts-nanum. (Reading database ... 155229 files and directories currently installed.) Preparing to unpack .../fonts-nanum_20170925-1_all.deb ... Unpacking fonts-nanum (20170925-1) ... Setting up fonts-nanum (20170925-1) ... Processing triggers for fontconfig (2.12.6-0ubuntu2) ... /usr/share/fonts: caching, new cache contents: 0 fonts, 1 dirs /usr/share/fonts/truetype: caching, new cache contents: 0 fonts, 3 dirs /usr/share/fonts/truetype/humor-sans: caching, new cache contents: 1 fonts, 0 dirs /usr/share/fonts/truetype/liberation: caching, new cache contents: 16 fonts, 0 dirs /usr/share/fonts/truetype/nanum: caching, new cache contents: 10 fonts, 0 dirs /usr/local/share/fonts: caching, new cache contents: 0 fonts, 0 dirs /root/.local/share/fonts: skipping, no such directory /root/.fonts: skipping, no such directory /var/cache/fontconfig: cleaning cache directory /root/.cache/fontconfig: not cleaning non-existent cache directory /root/.fontconfig: not cleaning non-existent cache directory fc-cache: succeeded . import warnings; warnings.filterwarnings(&#39;ignore&#39;) import matplotlib.pyplot as plt import matplotlib as mpl import matplotlib.font_manager as fm import numpy as npn import pandas as pd import seaborn as sns plt.rc(&#39;font&#39;, family=&#39;NanumBarunGothic&#39;) . !unzip -uq &quot;/content/drive/MyDrive/Colab Notebooks/타이타닉/타이타닉.zip&quot; -d &quot;/content/drive/MyDrive/Colab Notebooks/타이타닉&quot; . test = pd.read_csv(&quot;/content/drive/MyDrive/Colab Notebooks/타이타닉/test.csv&quot;) train = pd.read_csv(&quot;/content/drive/MyDrive/Colab Notebooks/타이타닉/train.csv&quot;) submission = pd.read_csv(&quot;/content/drive/MyDrive/Colab Notebooks/타이타닉/submission.csv&quot;) . train.head() . PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked . 0 1 | 0 | 3 | Braund, Mr. Owen Harris | male | 22.0 | 1 | 0 | A/5 21171 | 7.2500 | NaN | S | . 1 2 | 1 | 1 | Cumings, Mrs. John Bradley (Florence Briggs Th... | female | 38.0 | 1 | 0 | PC 17599 | 71.2833 | C85 | C | . 2 3 | 1 | 3 | Heikkinen, Miss. Laina | female | 26.0 | 0 | 0 | STON/O2. 3101282 | 7.9250 | NaN | S | . 3 4 | 1 | 1 | Futrelle, Mrs. Jacques Heath (Lily May Peel) | female | 35.0 | 1 | 0 | 113803 | 53.1000 | C123 | S | . 4 5 | 0 | 3 | Allen, Mr. William Henry | male | 35.0 | 0 | 0 | 373450 | 8.0500 | NaN | S | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; PassengerID : 탑승객 고유 아이디 Survival : 탑승객 생존 유무 (0: 사망, 1: 생존) Pclass : 등실의 등급 Name : 이름 Sex : 성별 Age : 나이 Sibsp : 함께 탐승한 형제자매, 아내, 남편의 수 Parch : 함께 탐승한 부모, 자식의 수 Ticket :티켓 번호 Fare : 티켓의 요금 Cabin : 객실번호 Embarked : 배에 탑승한 항구 이름 ( C = Cherbourn, Q = Queenstown, S = Southampton) . 일단 객실 등급과 성별, 나이, 티켓요금, 탑승한 항구 이름 정도가 영향이 있지 않았을까 싶다. . train.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 891 entries, 0 to 890 Data columns (total 12 columns): # Column Non-Null Count Dtype -- -- 0 PassengerId 891 non-null int64 1 Survived 891 non-null int64 2 Pclass 891 non-null int64 3 Name 891 non-null object 4 Sex 891 non-null object 5 Age 714 non-null float64 6 SibSp 891 non-null int64 7 Parch 891 non-null int64 8 Ticket 891 non-null object 9 Fare 891 non-null float64 10 Cabin 204 non-null object 11 Embarked 889 non-null object dtypes: float64(2), int64(5), object(5) memory usage: 83.7+ KB . 객실번호와 나이를 제외한 변수들은 결측치를 가지지 않음을 확인할 수 있다. . train.describe() . PassengerId Survived Pclass Age SibSp Parch Fare . count 891.000000 | 891.000000 | 891.000000 | 714.000000 | 891.000000 | 891.000000 | 891.000000 | . mean 446.000000 | 0.383838 | 2.308642 | 29.699118 | 0.523008 | 0.381594 | 32.204208 | . std 257.353842 | 0.486592 | 0.836071 | 14.526497 | 1.102743 | 0.806057 | 49.693429 | . min 1.000000 | 0.000000 | 1.000000 | 0.420000 | 0.000000 | 0.000000 | 0.000000 | . 25% 223.500000 | 0.000000 | 2.000000 | 20.125000 | 0.000000 | 0.000000 | 7.910400 | . 50% 446.000000 | 0.000000 | 3.000000 | 28.000000 | 0.000000 | 0.000000 | 14.454200 | . 75% 668.500000 | 1.000000 | 3.000000 | 38.000000 | 1.000000 | 0.000000 | 31.000000 | . max 891.000000 | 1.000000 | 3.000000 | 80.000000 | 8.000000 | 6.000000 | 512.329200 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; train.loc[train[&#39;Age&#39;].isna()] . PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked . 5 6 | 0 | 3 | Moran, Mr. James | male | NaN | 0 | 0 | 330877 | 8.4583 | NaN | Q | . 17 18 | 1 | 2 | Williams, Mr. Charles Eugene | male | NaN | 0 | 0 | 244373 | 13.0000 | NaN | S | . 19 20 | 1 | 3 | Masselmani, Mrs. Fatima | female | NaN | 0 | 0 | 2649 | 7.2250 | NaN | C | . 26 27 | 0 | 3 | Emir, Mr. Farred Chehab | male | NaN | 0 | 0 | 2631 | 7.2250 | NaN | C | . 28 29 | 1 | 3 | O&#39;Dwyer, Miss. Ellen &quot;Nellie&quot; | female | NaN | 0 | 0 | 330959 | 7.8792 | NaN | Q | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 859 860 | 0 | 3 | Razi, Mr. Raihed | male | NaN | 0 | 0 | 2629 | 7.2292 | NaN | C | . 863 864 | 0 | 3 | Sage, Miss. Dorothy Edith &quot;Dolly&quot; | female | NaN | 8 | 2 | CA. 2343 | 69.5500 | NaN | S | . 868 869 | 0 | 3 | van Melkebeke, Mr. Philemon | male | NaN | 0 | 0 | 345777 | 9.5000 | NaN | S | . 878 879 | 0 | 3 | Laleff, Mr. Kristo | male | NaN | 0 | 0 | 349217 | 7.8958 | NaN | S | . 888 889 | 0 | 3 | Johnston, Miss. Catherine Helen &quot;Carrie&quot; | female | NaN | 1 | 2 | W./C. 6607 | 23.4500 | NaN | S | . 177 rows × 12 columns . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; train.loc[(train[&#39;Age&#39;].isna()) &amp; (train[&#39;SibSp&#39;] == 0 ) &amp; (train[&#39;Parch&#39;] == 0)] . PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked . 5 6 | 0 | 3 | Moran, Mr. James | male | NaN | 0 | 0 | 330877 | 8.4583 | NaN | Q | . 17 18 | 1 | 2 | Williams, Mr. Charles Eugene | male | NaN | 0 | 0 | 244373 | 13.0000 | NaN | S | . 19 20 | 1 | 3 | Masselmani, Mrs. Fatima | female | NaN | 0 | 0 | 2649 | 7.2250 | NaN | C | . 26 27 | 0 | 3 | Emir, Mr. Farred Chehab | male | NaN | 0 | 0 | 2631 | 7.2250 | NaN | C | . 28 29 | 1 | 3 | O&#39;Dwyer, Miss. Ellen &quot;Nellie&quot; | female | NaN | 0 | 0 | 330959 | 7.8792 | NaN | Q | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 837 838 | 0 | 3 | Sirota, Mr. Maurice | male | NaN | 0 | 0 | 392092 | 8.0500 | NaN | S | . 839 840 | 1 | 1 | Marechal, Mr. Pierre | male | NaN | 0 | 0 | 11774 | 29.7000 | C47 | C | . 859 860 | 0 | 3 | Razi, Mr. Raihed | male | NaN | 0 | 0 | 2629 | 7.2292 | NaN | C | . 868 869 | 0 | 3 | van Melkebeke, Mr. Philemon | male | NaN | 0 | 0 | 345777 | 9.5000 | NaN | S | . 878 879 | 0 | 3 | Laleff, Mr. Kristo | male | NaN | 0 | 0 | 349217 | 7.8958 | NaN | S | . 133 rows × 12 columns . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; train.groupby(&#39;Pclass&#39;)[&#39;Age&#39;].mean() . Pclass 1 38.233441 2 29.877630 3 25.140620 Name: Age, dtype: float64 . plt.figure(figsize=[10,10]) sns.scatterplot(data=train, x=&#39;Pclass&#39;, y = &#39;Age&#39;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f7044f33410&gt; . findfont: Font family [&#39;NanumBarunGothic&#39;] not found. Falling back to DejaVu Sans. . &#44208;&#52769;&#52824; &#52376;&#47532; . 객실등급에 따라서 나이의 평균에 차이가 있는 것으로 보인다. 아무래도 비싼 객실일수록 평균 나이대가 높아지는 경향이 있는 것으로 보인다. 나이의 결측치들을 객실등급별로 평균값으로 채워주면 될 것 같다. . meanage = train.groupby(&#39;Pclass&#39;)[&#39;Age&#39;].mean() . train.loc[(train[&#39;Age&#39;].isna()) &amp; (train[&#39;Pclass&#39;] == 1), &#39;Age&#39;] = meanage[1] train.loc[(train[&#39;Age&#39;].isna()) &amp; (train[&#39;Pclass&#39;] == 2),&#39;Age&#39;] = meanage[2] train.loc[(train[&#39;Age&#39;].isna()) &amp; (train[&#39;Pclass&#39;] == 3),&#39;Age&#39;] = meanage[3] . train.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 891 entries, 0 to 890 Data columns (total 12 columns): # Column Non-Null Count Dtype -- -- 0 PassengerId 891 non-null int64 1 Survived 891 non-null int64 2 Pclass 891 non-null int64 3 Name 891 non-null object 4 Sex 891 non-null object 5 Age 891 non-null float64 6 SibSp 891 non-null int64 7 Parch 891 non-null int64 8 Ticket 891 non-null object 9 Fare 891 non-null float64 10 Cabin 204 non-null object 11 Embarked 889 non-null object dtypes: float64(2), int64(5), object(5) memory usage: 83.7+ KB . Cabin은 결측치도 많고 별로 의미가 없을까 싶다. 변수 자체를 없애는 것이 나아보인다. Embarked는 결측치가 몇 개 존재하지 않는다. 가장 자주 나온 변수로 채워주어도 괜찮을 것으로 보인다. . train[&#39;Embarked&#39;].value_counts() . S 644 C 168 Q 77 Name: Embarked, dtype: int64 . S 가 가장 많이 나타나므로 결측치는 S로 채워주도록 하자. . train[&#39;Embarked&#39;].fillna(value = &#39;S&#39;, inplace = True) . train[&#39;Embarked&#39;].isna().sum() . 0 . 테스트 데이터에도 똑같이 적용해준다. . test.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 418 entries, 0 to 417 Data columns (total 11 columns): # Column Non-Null Count Dtype -- -- 0 PassengerId 418 non-null int64 1 Pclass 418 non-null int64 2 Name 418 non-null object 3 Sex 418 non-null object 4 Age 332 non-null float64 5 SibSp 418 non-null int64 6 Parch 418 non-null int64 7 Ticket 418 non-null object 8 Fare 417 non-null float64 9 Cabin 91 non-null object 10 Embarked 418 non-null object dtypes: float64(2), int64(4), object(5) memory usage: 36.0+ KB . test.loc[(test[&#39;Age&#39;].isna()) &amp; (test[&#39;Pclass&#39;] == 1), &#39;Age&#39;] = meanage[1] test.loc[(test[&#39;Age&#39;].isna()) &amp; (test[&#39;Pclass&#39;] == 2),&#39;Age&#39;] = meanage[2] test.loc[(test[&#39;Age&#39;].isna()) &amp; (test[&#39;Pclass&#39;] == 3),&#39;Age&#39;] = meanage[3] . test.isna().sum() . PassengerId 0 Pclass 0 Name 0 Sex 0 Age 0 SibSp 0 Parch 0 Ticket 0 Fare 1 Cabin 327 Embarked 0 dtype: int64 . 성별이 &#39;male&#39;과 &#39;female&#39;로 설정되어 있는데 0과 1의 값을 가지는 변수로 변환해주자 . train[&#39;Sex&#39;] = train[&#39;Sex&#39;].map({&#39;male&#39;:0, &#39;female&#39;:1}) . test[&#39;Sex&#39;] = test[&#39;Sex&#39;].map({&#39;male&#39;:0, &#39;female&#39;:1}) . &#45936;&#51060;&#53552; &#49884;&#44033;&#54868; . plt.figure(figsize=[10,10]) sns.barplot(data=train, x=&#39;Pclass&#39;, y = &#39;Survived&#39;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f702f48df90&gt; . 확실히 객실의 등급이 높을수록 생존할 확률이 높아지는것을 확인할 수 있다. . plt.figure(figsize=[10,10]) sns.barplot(data=train, x=&#39;Sex&#39;, y = &#39;Survived&#39;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f70268cff50&gt; . 여자가 남자에 비해서 압도적으로 생존확률이 높은 것을 확인할 수 있다. . plt.figure(figsize=[10,10]) sns.barplot(data=train, x=&#39;Embarked&#39;, y = &#39;Survived&#39;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f70260125d0&gt; . sns.lmplot(data=train, x=&#39;Age&#39;, y = &#39;Survived&#39;) . &lt;seaborn.axisgrid.FacetGrid at 0x7f702f3c2e10&gt; . 나이별로 생존률에 차이가 있는지를 어떻게 확인해보려고 했는데 잘 안된다.... 큰차이는 없다..? 나이가 많아질수록 생존확률이 낮아진다? 정도로 볼 수 있을 것 같다. . %matplotlib inline train.hist(bins = 50, figsize = (20,15)) plt.show() . &#48320;&#49688; &#49440;&#53469; &#48143; &#47784;&#45944; &#44396;&#52629; . sklearn.linear_model.LogisticRegression() . 로지스틱 함수를 통해 0과 1사이의 값을 산출하여, 탑승객의 생존 여부를 파악해봅시다. . X_train = train[[&#39;Pclass&#39;, &#39;Age&#39;]] y_train = train[&#39;Survived&#39;] X_test = test[[&#39;Pclass&#39;, &#39;Age&#39;]] . from sklearn.linear_model import LogisticRegression model = LogisticRegression() . model.fit(X_train, y_train) . LogisticRegression() . y_pred = model.predict(X_test) . submission[&#39;Survived&#39;] = y_pred . submission.to_csv(&#39;lr_model_Pclass_Age.csv&#39;, index = False) . sklearn.tree.DecisionTreeClassifier() . 특징변수들로부터 타깃변수를 맞추기 위해 경우를 쪼개나가는 알고리즘입니다. (예) 과일이 사과, 딸기, 포도 중 무엇인지 맞추려합니다 . 주어진 특징은 과일 1개의 가로최대길이, 세로최대길이, 과일의색상이라고 합시다 | 사과를 맞추기 위해서 10 ~ 13cm의 가로, 세로 최대길이와 빨간색의 과일을 탐색하게 되겠죠 | 위의 &#39;길이가 10 ~ 13cm인가? 아닌가?&#39;, &#39;색깔이 빨간색인가? 아닌가?&#39;의 기준이 경우를 쪼개나가는 기준이 됩니다. | . from sklearn.tree import DecisionTreeClassifier dt_model = DecisionTreeClassifier() . dt_model.fit(X_train, y_train) . DecisionTreeClassifier() . submission[&#39;Survived&#39;] = dt_model.predict(X_test) . submission.to_csv(&#39;dt_model.csv&#39;, index = False) . &#47784;&#45944; &#54617;&#49845; &#48143; &#44160;&#51613; . submission[&#39;Survived&#39;] = model.predict_proba(X_test)[:,1] . submission.to_csv(&#39;lr_proba.csv&#39;, index = False) . submission[&#39;Survived&#39;] = dt_model.predict_proba(X_test)[:,1] . submission.to_csv(&#39;dt_proba.csv&#39;, index = False) . DecisionTreeClassifier() . DecisionTreeClassifier() . dt_model_new = DecisionTreeClassifier(min_samples_split=10) . dt_model_new.fit(X_train, y_train) . DecisionTreeClassifier(min_samples_split=10) . submission[&#39;Survived&#39;] = dt_model_new.predict_proba(X_test)[:, 1] . submission.to_csv(&#39;dt_min_samples_10_proba.csv&#39;, index = False) . train . PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked . 0 1 | 0 | 3 | Braund, Mr. Owen Harris | 0 | 22.00000 | 1 | 0 | A/5 21171 | 7.2500 | NaN | S | . 1 2 | 1 | 1 | Cumings, Mrs. John Bradley (Florence Briggs Th... | 1 | 38.00000 | 1 | 0 | PC 17599 | 71.2833 | C85 | C | . 2 3 | 1 | 3 | Heikkinen, Miss. Laina | 1 | 26.00000 | 0 | 0 | STON/O2. 3101282 | 7.9250 | NaN | S | . 3 4 | 1 | 1 | Futrelle, Mrs. Jacques Heath (Lily May Peel) | 1 | 35.00000 | 1 | 0 | 113803 | 53.1000 | C123 | S | . 4 5 | 0 | 3 | Allen, Mr. William Henry | 0 | 35.00000 | 0 | 0 | 373450 | 8.0500 | NaN | S | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 886 887 | 0 | 2 | Montvila, Rev. Juozas | 0 | 27.00000 | 0 | 0 | 211536 | 13.0000 | NaN | S | . 887 888 | 1 | 1 | Graham, Miss. Margaret Edith | 1 | 19.00000 | 0 | 0 | 112053 | 30.0000 | B42 | S | . 888 889 | 0 | 3 | Johnston, Miss. Catherine Helen &quot;Carrie&quot; | 1 | 25.14062 | 1 | 2 | W./C. 6607 | 23.4500 | NaN | S | . 889 890 | 1 | 1 | Behr, Mr. Karl Howell | 0 | 26.00000 | 0 | 0 | 111369 | 30.0000 | C148 | C | . 890 891 | 0 | 3 | Dooley, Mr. Patrick | 0 | 32.00000 | 0 | 0 | 370376 | 7.7500 | NaN | Q | . 891 rows × 12 columns . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; . from sklearn.metrics import confusion_matrix, classification_report from sklearn.metrics import precision_score from sklearn.metrics import recall_score from sklearn.metrics import accuracy_score from sklearn.metrics import roc_auc_score . X_train = train[[&#39;Pclass&#39;, &#39;Age&#39;]] y_train = train[&#39;Survived&#39;] X_test = test[[&#39;Pclass&#39;, &#39;Age&#39;]] . model = LogisticRegression() model.fit(X_train, y_train) y_pred = model.predict(X_train) . y_true = y_train.values . cf_matrix = confusion_matrix(y_true, y_pred) . cf_matrix . array([[466, 83], [183, 159]]) . accuracy_score(y_true, y_pred) . 0.7014590347923682 . (cf_matrix[0,0] + cf_matrix[1,1]) / 891 . 0.7014590347923682 . precision_score(y_true, y_pred) . 0.6570247933884298 . (cf_matrix[1,1]) / (83 + 160) . 0.654320987654321 . recall_score(y_true, y_pred) . 0.4649122807017544 . (cf_matrix[1,1]) / (182 + 160) . 0.4649122807017544 . print(classification_report(y_true, y_pred)) . precision recall f1-score support 0 0.72 0.85 0.78 549 1 0.66 0.46 0.54 342 accuracy 0.70 891 macro avg 0.69 0.66 0.66 891 weighted avg 0.69 0.70 0.69 891 . 데이콘 채점 기준은 auc 라는 지표를 사용합니다. | auc값을 측정하기 위해서는, 예측을 확률값으로 해주어야 합니다. | 그 중에서 1에 속할 확률을 선택해주어야 합니다. | . roc_auc_score(y_true, y_pred) . 0.6568641549228261 . &#44208;&#44284; &#48143; &#44208;&#50616; . import pandas as pd from sklearn.tree import DecisionTreeClassifier from sklearn.model_selection import KFold from sklearn.metrics import roc_auc_score . model_10 = DecisionTreeClassifier(min_samples_split=10) model_20 = DecisionTreeClassifier(min_samples_split=20) model_30 = DecisionTreeClassifier(min_samples_split=30) . kfold = KFold(n_splits=5, shuffle=True, random_state=10) . score_10 = [] for trn_idx, val_idx in kfold.split(X_train): X_trn, y_trn = X_train.iloc[trn_idx, :], y_train.iloc[trn_idx] X_val, y_val = X_train.iloc[val_idx, :], y_train.iloc[val_idx] model_10.fit(X_trn, y_trn) y_pred = model_10.predict_proba(X_val)[:, 1] print(&#39;예측 완료&#39;) score_10.append(roc_auc_score(y_val.values, y_pred)) . 예측 완료 예측 완료 예측 완료 예측 완료 예측 완료 . score_10 . [0.666528811690102, 0.6923701298701299, 0.6616221174004193, 0.58994708994709, 0.7008647798742138] . score_20 = [] for trn_idx, val_idx in kfold.split(X_train): X_trn, y_trn = X_train.iloc[trn_idx, :], y_train.iloc[trn_idx] X_val, y_val = X_train.iloc[val_idx, :], y_train.iloc[val_idx] model_20.fit(X_trn, y_trn) y_pred = model_20.predict_proba(X_val)[:, 1] print(&#39;예측 완료&#39;) score_20.append(roc_auc_score(y_val.values, y_pred)) . 예측 완료 예측 완료 예측 완료 예측 완료 예측 완료 . score_20 . [0.6767300799558864, 0.696766774891775, 0.6660770440251572, 0.608531746031746, 0.7118055555555556] . score_30 = [] for trn_idx, val_idx in kfold.split(X_train): X_trn, y_trn = X_train.iloc[trn_idx, :], y_train.iloc[trn_idx] X_val, y_val = X_train.iloc[val_idx, :], y_train.iloc[val_idx] model_30.fit(X_trn, y_trn) y_pred = model_30.predict_proba(X_val)[:, 1] print(&#39;예측 완료&#39;) score_30.append(roc_auc_score(y_val.values, y_pred)) . 예측 완료 예측 완료 예측 완료 예측 완료 예측 완료 . score_30 . [0.6944444444444444, 0.7076569264069265, 0.6582154088050315, 0.6217592592592592, 0.7237945492662475] . import numpy as np . np.mean(score_10), np.mean(score_20), np.mean(score_30) . (0.662266585756391, 0.671982240092024, 0.6811741176363818) . 추가로 성별까지 변수에 추가하여 점수를 확인해보자. . X_train = train[[&#39;Pclass&#39;, &#39;Age&#39;, &#39;Sex&#39;]] y_train = train[&#39;Survived&#39;] X_test = test[[&#39;Pclass&#39;, &#39;Age&#39;, &#39;Sex&#39;]] . score_10 = [] for trn_idx, val_idx in kfold.split(X_train): X_trn, y_trn = X_train.iloc[trn_idx, :], y_train.iloc[trn_idx] X_val, y_val = X_train.iloc[val_idx, :], y_train.iloc[val_idx] model_10.fit(X_trn, y_trn) y_pred = model_10.predict_proba(X_val)[:, 1] print(&#39;예측 완료&#39;) score_10.append(roc_auc_score(y_val.values, y_pred)) . 예측 완료 예측 완료 예측 완료 예측 완료 예측 완료 . score_20 = [] for trn_idx, val_idx in kfold.split(X_train): X_trn, y_trn = X_train.iloc[trn_idx, :], y_train.iloc[trn_idx] X_val, y_val = X_train.iloc[val_idx, :], y_train.iloc[val_idx] model_20.fit(X_trn, y_trn) y_pred = model_20.predict_proba(X_val)[:, 1] print(&#39;예측 완료&#39;) score_20.append(roc_auc_score(y_val.values, y_pred)) . 예측 완료 예측 완료 예측 완료 예측 완료 예측 완료 . score_30 = [] for trn_idx, val_idx in kfold.split(X_train): X_trn, y_trn = X_train.iloc[trn_idx, :], y_train.iloc[trn_idx] X_val, y_val = X_train.iloc[val_idx, :], y_train.iloc[val_idx] model_30.fit(X_trn, y_trn) y_pred = model_30.predict_proba(X_val)[:, 1] print(&#39;예측 완료&#39;) score_30.append(roc_auc_score(y_val.values, y_pred)) . 예측 완료 예측 완료 예측 완료 예측 완료 예측 완료 . np.mean(score_10), np.mean(score_20), np.mean(score_30) . (0.8173547973745784, 0.8224093714579868, 0.8311338310733472) . from sklearn import tree . plt.figure( figsize=(20,15) ) tree.plot_tree(model_30, impurity=True, filled=True, rounded=True) . [Text(0.5275735294117647, 0.9615384615384616, &#39;X[2] &lt;= 0.5 ngini = 0.471 nsamples = 713 nvalue = [443, 270]&#39;), Text(0.2610294117647059, 0.8846153846153846, &#39;X[0] &lt;= 1.5 ngini = 0.31 nsamples = 465 nvalue = [376, 89]&#39;), Text(0.11764705882352941, 0.8076923076923077, &#39;X[1] &lt;= 36.5 ngini = 0.468 nsamples = 102 nvalue = [64, 38]&#39;), Text(0.058823529411764705, 0.7307692307692307, &#39;X[1] &lt;= 33.5 ngini = 0.484 nsamples = 34 nvalue = [14, 20]&#39;), Text(0.029411764705882353, 0.6538461538461539, &#39;gini = 0.499 nsamples = 25 nvalue = [13, 12]&#39;), Text(0.08823529411764706, 0.6538461538461539, &#39;gini = 0.198 nsamples = 9 nvalue = [1, 8]&#39;), Text(0.17647058823529413, 0.7307692307692307, &#39;X[1] &lt;= 60.5 ngini = 0.389 nsamples = 68 nvalue = [50, 18]&#39;), Text(0.14705882352941177, 0.6538461538461539, &#39;X[1] &lt;= 47.5 ngini = 0.42 nsamples = 60 nvalue = [42, 18]&#39;), Text(0.11764705882352941, 0.5769230769230769, &#39;X[1] &lt;= 45.25 ngini = 0.381 nsamples = 39 nvalue = [29, 10]&#39;), Text(0.08823529411764706, 0.5, &#39;X[1] &lt;= 41.0 ngini = 0.415 nsamples = 34 nvalue = [24, 10]&#39;), Text(0.058823529411764705, 0.4230769230769231, &#39;gini = 0.384 nsamples = 27 nvalue = [20, 7]&#39;), Text(0.11764705882352941, 0.4230769230769231, &#39;gini = 0.49 nsamples = 7 nvalue = [4, 3]&#39;), Text(0.14705882352941177, 0.5, &#39;gini = 0.0 nsamples = 5 nvalue = [5, 0]&#39;), Text(0.17647058823529413, 0.5769230769230769, &#39;gini = 0.472 nsamples = 21 nvalue = [13, 8]&#39;), Text(0.20588235294117646, 0.6538461538461539, &#39;gini = 0.0 nsamples = 8 nvalue = [8, 0]&#39;), Text(0.40441176470588236, 0.8076923076923077, &#39;X[1] &lt;= 3.5 ngini = 0.242 nsamples = 363 nvalue = [312, 51]&#39;), Text(0.375, 0.7307692307692307, &#39;gini = 0.444 nsamples = 15 nvalue = [5, 10]&#39;), Text(0.4338235294117647, 0.7307692307692307, &#39;X[1] &lt;= 32.5 ngini = 0.208 nsamples = 348 nvalue = [307, 41]&#39;), Text(0.27941176470588236, 0.6538461538461539, &#39;X[1] &lt;= 30.75 ngini = 0.239 nsamples = 260 nvalue = [224, 36]&#39;), Text(0.25, 0.5769230769230769, &#39;X[1] &lt;= 10.0 ngini = 0.215 nsamples = 245 nvalue = [215, 30]&#39;), Text(0.22058823529411764, 0.5, &#39;gini = 0.444 nsamples = 9 nvalue = [6, 3]&#39;), Text(0.27941176470588236, 0.5, &#39;X[1] &lt;= 25.57 ngini = 0.203 nsamples = 236 nvalue = [209, 27]&#39;), Text(0.17647058823529413, 0.4230769230769231, &#39;X[0] &lt;= 2.5 ngini = 0.177 nsamples = 183 nvalue = [165, 18]&#39;), Text(0.14705882352941177, 0.34615384615384615, &#39;gini = 0.095 nsamples = 20 nvalue = [19, 1]&#39;), Text(0.20588235294117646, 0.34615384615384615, &#39;X[1] &lt;= 19.5 ngini = 0.187 nsamples = 163 nvalue = [146, 17]&#39;), Text(0.14705882352941177, 0.2692307692307692, &#39;X[1] &lt;= 18.5 ngini = 0.117 nsamples = 32 nvalue = [30, 2]&#39;), Text(0.11764705882352941, 0.19230769230769232, &#39;gini = 0.159 nsamples = 23 nvalue = [21, 2]&#39;), Text(0.17647058823529413, 0.19230769230769232, &#39;gini = 0.0 nsamples = 9 nvalue = [9, 0]&#39;), Text(0.2647058823529412, 0.2692307692307692, &#39;X[1] &lt;= 20.25 ngini = 0.203 nsamples = 131 nvalue = [116, 15]&#39;), Text(0.23529411764705882, 0.19230769230769232, &#39;gini = 0.298 nsamples = 11 nvalue = [9, 2]&#39;), Text(0.29411764705882354, 0.19230769230769232, &#39;X[1] &lt;= 23.75 ngini = 0.193 nsamples = 120 nvalue = [107, 13]&#39;), Text(0.2647058823529412, 0.11538461538461539, &#39;gini = 0.147 nsamples = 25 nvalue = [23, 2]&#39;), Text(0.3235294117647059, 0.11538461538461539, &#39;X[1] &lt;= 25.07 ngini = 0.205 nsamples = 95 nvalue = [84, 11]&#39;), Text(0.29411764705882354, 0.038461538461538464, &#39;gini = 0.255 nsamples = 20 nvalue = [17, 3]&#39;), Text(0.35294117647058826, 0.038461538461538464, &#39;gini = 0.191 nsamples = 75 nvalue = [67, 8]&#39;), Text(0.38235294117647056, 0.4230769230769231, &#39;X[1] &lt;= 29.939 ngini = 0.282 nsamples = 53 nvalue = [44, 9]&#39;), Text(0.35294117647058826, 0.34615384615384615, &#39;X[0] &lt;= 2.5 ngini = 0.331 nsamples = 43 nvalue = [34, 9]&#39;), Text(0.3235294117647059, 0.2692307692307692, &#39;gini = 0.198 nsamples = 18 nvalue = [16, 2]&#39;), Text(0.38235294117647056, 0.2692307692307692, &#39;gini = 0.403 nsamples = 25 nvalue = [18, 7]&#39;), Text(0.4117647058823529, 0.34615384615384615, &#39;gini = 0.0 nsamples = 10 nvalue = [10, 0]&#39;), Text(0.3088235294117647, 0.5769230769230769, &#39;gini = 0.48 nsamples = 15 nvalue = [9, 6]&#39;), Text(0.5882352941176471, 0.6538461538461539, &#39;X[1] &lt;= 61.5 ngini = 0.107 nsamples = 88 nvalue = [83, 5]&#39;), Text(0.5588235294117647, 0.5769230769230769, &#39;X[1] &lt;= 44.5 ngini = 0.093 nsamples = 82 nvalue = [78, 4]&#39;), Text(0.5294117647058824, 0.5, &#39;X[1] &lt;= 38.5 ngini = 0.126 nsamples = 59 nvalue = [55, 4]&#39;), Text(0.5, 0.4230769230769231, &#39;X[0] &lt;= 2.5 ngini = 0.059 nsamples = 33 nvalue = [32, 1]&#39;), Text(0.47058823529411764, 0.34615384615384615, &#39;gini = 0.153 nsamples = 12 nvalue = [11, 1]&#39;), Text(0.5294117647058824, 0.34615384615384615, &#39;gini = 0.0 nsamples = 21 nvalue = [21, 0]&#39;), Text(0.5588235294117647, 0.4230769230769231, &#39;gini = 0.204 nsamples = 26 nvalue = [23, 3]&#39;), Text(0.5882352941176471, 0.5, &#39;gini = 0.0 nsamples = 23 nvalue = [23, 0]&#39;), Text(0.6176470588235294, 0.5769230769230769, &#39;gini = 0.278 nsamples = 6 nvalue = [5, 1]&#39;), Text(0.7941176470588235, 0.8846153846153846, &#39;X[0] &lt;= 2.5 ngini = 0.394 nsamples = 248 nvalue = [67, 181]&#39;), Text(0.7058823529411765, 0.8076923076923077, &#39;X[1] &lt;= 2.5 ngini = 0.087 nsamples = 131 nvalue = [6, 125]&#39;), Text(0.6764705882352942, 0.7307692307692307, &#39;gini = 0.0 nsamples = 1 nvalue = [1, 0]&#39;), Text(0.7352941176470589, 0.7307692307692307, &#39;X[1] &lt;= 43.5 ngini = 0.074 nsamples = 130 nvalue = [5, 125]&#39;), Text(0.7058823529411765, 0.6538461538461539, &#39;X[1] &lt;= 26.5 ngini = 0.037 nsamples = 106 nvalue = [2, 104]&#39;), Text(0.6764705882352942, 0.5769230769230769, &#39;X[1] &lt;= 25.5 ngini = 0.083 nsamples = 46 nvalue = [2, 44]&#39;), Text(0.6470588235294118, 0.5, &#39;X[1] &lt;= 23.5 ngini = 0.043 nsamples = 45 nvalue = [1, 44]&#39;), Text(0.6176470588235294, 0.4230769230769231, &#39;gini = 0.0 nsamples = 32 nvalue = [0, 32]&#39;), Text(0.6764705882352942, 0.4230769230769231, &#39;gini = 0.142 nsamples = 13 nvalue = [1, 12]&#39;), Text(0.7058823529411765, 0.5, &#39;gini = 0.0 nsamples = 1 nvalue = [1, 0]&#39;), Text(0.7352941176470589, 0.5769230769230769, &#39;gini = 0.0 nsamples = 60 nvalue = [0, 60]&#39;), Text(0.7647058823529411, 0.6538461538461539, &#39;gini = 0.219 nsamples = 24 nvalue = [3, 21]&#39;), Text(0.8823529411764706, 0.8076923076923077, &#39;X[1] &lt;= 27.5 ngini = 0.499 nsamples = 117 nvalue = [61, 56]&#39;), Text(0.8529411764705882, 0.7307692307692307, &#39;X[1] &lt;= 1.5 ngini = 0.497 nsamples = 93 nvalue = [43, 50]&#39;), Text(0.8235294117647058, 0.6538461538461539, &#39;gini = 0.0 nsamples = 4 nvalue = [0, 4]&#39;), Text(0.8823529411764706, 0.6538461538461539, &#39;X[1] &lt;= 21.5 ngini = 0.499 nsamples = 89 nvalue = [43, 46]&#39;), Text(0.8235294117647058, 0.5769230769230769, &#39;X[1] &lt;= 5.5 ngini = 0.473 nsamples = 39 nvalue = [24, 15]&#39;), Text(0.7941176470588235, 0.5, &#39;gini = 0.48 nsamples = 10 nvalue = [4, 6]&#39;), Text(0.8529411764705882, 0.5, &#39;gini = 0.428 nsamples = 29 nvalue = [20, 9]&#39;), Text(0.9411764705882353, 0.5769230769230769, &#39;X[1] &lt;= 26.5 ngini = 0.471 nsamples = 50 nvalue = [19, 31]&#39;), Text(0.9117647058823529, 0.5, &#39;X[1] &lt;= 25.07 ngini = 0.475 nsamples = 49 nvalue = [19, 30]&#39;), Text(0.8823529411764706, 0.4230769230769231, &#39;gini = 0.496 nsamples = 11 nvalue = [5, 6]&#39;), Text(0.9411764705882353, 0.4230769230769231, &#39;X[1] &lt;= 25.57 ngini = 0.465 nsamples = 38 nvalue = [14, 24]&#39;), Text(0.9117647058823529, 0.34615384615384615, &#39;gini = 0.467 nsamples = 35 nvalue = [13, 22]&#39;), Text(0.9705882352941176, 0.34615384615384615, &#39;gini = 0.444 nsamples = 3 nvalue = [1, 2]&#39;), Text(0.9705882352941176, 0.5, &#39;gini = 0.0 nsamples = 1 nvalue = [0, 1]&#39;), Text(0.9117647058823529, 0.7307692307692307, &#39;gini = 0.375 nsamples = 24 nvalue = [18, 6]&#39;)] . findfont: Font family [&#39;NanumBarunGothic&#39;] not found. Falling back to DejaVu Sans. . model_30 = DecisionTreeClassifier(min_samples_split=30) model_30.fit(X_train, y_train) Y_pred = model_30.predict(X_test) model_30.score(X_train, y_train) submission = pd.DataFrame({ &quot;PassengerId&quot;: test[&quot;PassengerId&quot;], &quot;Survived&quot;: Y_pred }) submission.to_csv(&#39;titanic.csv&#39;, index=False) . import os print(os.getcwd()) . /content .",
            "url": "https://shw9807.github.io/shw9807blog/%EB%8D%B0%EC%9D%B4%EC%BD%98/2022/01/20/%ED%83%80%EC%9D%B4%ED%83%80%EB%8B%89_%EB%8D%B0%EC%9D%B4%ED%84%B0_%EB%B6%84%EC%84%9D.html",
            "relUrl": "/%EB%8D%B0%EC%9D%B4%EC%BD%98/2022/01/20/%ED%83%80%EC%9D%B4%ED%83%80%EB%8B%89_%EB%8D%B0%EC%9D%B4%ED%84%B0_%EB%B6%84%EC%84%9D.html",
            "date": " • Jan 20, 2022"
        }
        
    
  
    
        ,"post6": {
            "title": "[Colab] 영화 관객수 예측모델",
            "content": "&#50689;&#54868; &#44288;&#44061;&#49688; &#50696;&#52769; &#47784;&#45944; . EDA를 위한 패키지 불러오기 . !sudo apt-get install -y fonts-nanum !sudo fc-cache -fv !rm ~/.cache/matplotlib -rf . Reading package lists... Done Building dependency tree Reading state information... Done The following NEW packages will be installed: fonts-nanum 0 upgraded, 1 newly installed, 0 to remove and 37 not upgraded. Need to get 9,604 kB of archives. After this operation, 29.5 MB of additional disk space will be used. Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 fonts-nanum all 20170925-1 [9,604 kB] Fetched 9,604 kB in 1s (8,421 kB/s) debconf: unable to initialize frontend: Dialog debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, &lt;&gt; line 1.) debconf: falling back to frontend: Readline debconf: unable to initialize frontend: Readline debconf: (This frontend requires a controlling tty.) debconf: falling back to frontend: Teletype dpkg-preconfigure: unable to re-open stdin: Selecting previously unselected package fonts-nanum. (Reading database ... 155229 files and directories currently installed.) Preparing to unpack .../fonts-nanum_20170925-1_all.deb ... Unpacking fonts-nanum (20170925-1) ... Setting up fonts-nanum (20170925-1) ... Processing triggers for fontconfig (2.12.6-0ubuntu2) ... /usr/share/fonts: caching, new cache contents: 0 fonts, 1 dirs /usr/share/fonts/truetype: caching, new cache contents: 0 fonts, 3 dirs /usr/share/fonts/truetype/humor-sans: caching, new cache contents: 1 fonts, 0 dirs /usr/share/fonts/truetype/liberation: caching, new cache contents: 16 fonts, 0 dirs /usr/share/fonts/truetype/nanum: caching, new cache contents: 10 fonts, 0 dirs /usr/local/share/fonts: caching, new cache contents: 0 fonts, 0 dirs /root/.local/share/fonts: skipping, no such directory /root/.fonts: skipping, no such directory /var/cache/fontconfig: cleaning cache directory /root/.cache/fontconfig: not cleaning non-existent cache directory /root/.fontconfig: not cleaning non-existent cache directory fc-cache: succeeded . import pandas as pd import seaborn as sns import matplotlib.pyplot as plt import lightgbm as lgb import warnings warnings.filterwarnings(&quot;ignore&quot;) # 필요한 패키지와 라이브러리를 가져옴 import matplotlib as mpl import matplotlib.pyplot as plt import matplotlib.font_manager as fm %matplotlib inline plt.rc(&#39;font&#39;, family=&#39;NanumBarunGothic&#39;) mpl.rcParams[&#39;axes.unicode_minus&#39;] = False . import numpy as np . movies_test = pd.read_csv(&quot;/content/drive/MyDrive/Colab Notebooks/영화관객수/movies_test.csv&quot;) movies_train = pd.read_csv(&quot;/content/drive/MyDrive/Colab Notebooks/영화관객수/movies_train.csv&quot;) . submission = pd.read_csv(&quot;/content/drive/MyDrive/Colab Notebooks/영화관객수/submission.csv&quot;) . movies_train.head() . title distributor genre release_time time screening_rat director dir_prev_bfnum dir_prev_num num_staff num_actor box_off_num . 0 개들의 전쟁 | 롯데엔터테인먼트 | 액션 | 2012-11-22 | 96 | 청소년 관람불가 | 조병옥 | NaN | 0 | 91 | 2 | 23398 | . 1 내부자들 | (주)쇼박스 | 느와르 | 2015-11-19 | 130 | 청소년 관람불가 | 우민호 | 1161602.50 | 2 | 387 | 3 | 7072501 | . 2 은밀하게 위대하게 | (주)쇼박스 | 액션 | 2013-06-05 | 123 | 15세 관람가 | 장철수 | 220775.25 | 4 | 343 | 4 | 6959083 | . 3 나는 공무원이다 | (주)NEW | 코미디 | 2012-07-12 | 101 | 전체 관람가 | 구자홍 | 23894.00 | 2 | 20 | 6 | 217866 | . 4 불량남녀 | 쇼박스(주)미디어플렉스 | 코미디 | 2010-11-04 | 108 | 15세 관람가 | 신근호 | 1.00 | 1 | 251 | 2 | 483387 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; title : 영화의 제목 distributor : 배급사 genre : 장르 release_time : 개봉일 time : 상영시간(분) screening_rat : 상영등급 director : 감독이름 dir_prev_bfnum : 해당 감독이 이 영화를 만들기 전 제작에 참여한 영화에서의 평균 관객수(단 관객수가 알려지지 않은 영화 제외) dir_prev_num : 해당 감독이 이 영화를 만들기 전 제작에 참여한 영화의 개수(단 관객수가 알려지지 않은 영화 제외) num_staff : 스텝수 num_actor : 주연배우수 box_off_num : 관객수 . movies_train.isnull().any() . title False distributor False genre False release_time False time False screening_rat False director False dir_prev_bfnum True dir_prev_num False num_staff False num_actor False box_off_num False dtype: bool . dir_prev_bfnum 항목에 결측치가 존재함을 알 수 있다. -&gt; 해당 영화가 그 감독의 첫 영화인 경우에 결측치를 가질 것이다. . movies_train[movies_train[&#39;dir_prev_bfnum&#39;].isnull() == True].head() . title distributor genre release_time time screening_rat director dir_prev_bfnum dir_prev_num num_staff num_actor box_off_num . 0 개들의 전쟁 | 롯데엔터테인먼트 | 액션 | 2012-11-22 | 96 | 청소년 관람불가 | 조병옥 | NaN | 0 | 91 | 2 | 23398 | . 6 길위에서 | 백두대간 | 다큐멘터리 | 2013-05-23 | 104 | 전체 관람가 | 이창재 | NaN | 0 | 32 | 5 | 53526 | . 8 1789, 바스티유의 연인들 | 유니버설픽쳐스인터내셔널코리아 | 뮤지컬 | 2014-09-18 | 129 | 전체 관람가 | 정성복 | NaN | 0 | 3 | 5 | 4778 | . 9 청춘그루브 | (주)두타연 | 드라마 | 2012-03-15 | 94 | 15세 관람가 | 변성현 | NaN | 0 | 138 | 3 | 868 | . 10 AV 아이돌 | (주) 케이알씨지 | 멜로/로맨스 | 2015-07-27 | 89 | 청소년 관람불가 | 조조 히데오 | NaN | 0 | 0 | 4 | 745 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; movies_train = movies_train.fillna(0) . movies_test = movies_test.fillna(0) . 결측치를 0으로 바꾸어 주었다. . movies_train.isnull().any() . title False distributor False genre False release_time False time False screening_rat False director False dir_prev_bfnum False dir_prev_num False num_staff False num_actor False box_off_num False dtype: bool . movies_test.isnull().any() . title False distributor False genre False release_time False time False screening_rat False director False dir_prev_bfnum False dir_prev_num False num_staff False num_actor False dtype: bool . 결측치가 없어진 것을 확인할 수 있다. . movies_train[&#39;month&#39;] = movies_train[&#39;release_time&#39;].str[5:7].astype(&#39;int&#39;) . plt.figure(figsize=[10,10]) sns.scatterplot(data=movies_train, x=&#39;month&#39;, y = &#39;box_off_num&#39;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fd8789c0510&gt; . 방학시즌에 개봉한 영화가 대체로 관객 수가 많은 것처럼 보인다. . plt.figure(figsize=[10,10]) sns.scatterplot(data=movies_train, x=&#39;num_staff&#39;, y = &#39;box_off_num&#39;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fd87898e490&gt; . 스태프 수가 많은 영화가 대체로 관객 수가 많은 것처럼 보인다. . plt.figure(figsize=[10,10]) sns.scatterplot(data=movies_train, x=&#39;screening_rat&#39;, y = &#39;box_off_num&#39;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fd8789735d0&gt; . 전체 관람가나 청소년 관람불가 영화보다는 12세관람가나 15세관람가 영화가 관객수가 많은 편인 것을 알 수 있다. . plt.figure(figsize=[10,10]) sns.scatterplot(data=movies_train, x=&#39;time&#39;, y = &#39;box_off_num&#39;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fd878917710&gt; . 상영시간이 지나치게 짧거나 2시간30분을 넘어가면 관객수가 대체로 적다는 것을 확인할 수 있다. . movies_train[[&#39;genre&#39;, &#39;box_off_num&#39;]].groupby(&#39;genre&#39;).mean().sort_values(&#39;box_off_num&#39;) . box_off_num . genre . 뮤지컬 6.627000e+03 | . 다큐멘터리 6.717226e+04 | . 서스펜스 8.261100e+04 | . 애니메이션 1.819267e+05 | . 멜로/로맨스 4.259680e+05 | . 미스터리 5.275482e+05 | . 공포 5.908325e+05 | . 드라마 6.256898e+05 | . 코미디 1.193914e+06 | . SF 1.788346e+06 | . 액션 2.203974e+06 | . 느와르 2.263695e+06 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; 장르별로 평균 관객수를 정렬해보았다. 액션과 느와르 장르가 대체적으로 관객수가 많은 것을 확인할 수 있다. . %matplotlib inline movies_train.hist(bins = 50, figsize = (20,15)) plt.show() . 변수별로 히스토그램을 그려보았다. . sns.heatmap(movies_train.corr(), annot = True) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fd86f758e50&gt; . 히트맵그림을 그려보았다.관객수와 상관관계가 있어보이는 변수는 스태프의 수와 상영시간 정도로 보인다. . &#48320;&#49688; &#49440;&#53469;&#44284; &#47784;&#45944; &#44396;&#52629; . lightGBM (base model) . model = lgb.LGBMRegressor(n_estimators=1000) . features = [&#39;time&#39;, &#39;dir_prev_num&#39;, &#39;num_staff&#39;, &#39;num_actor&#39;] target = [&#39;box_off_num&#39;] . X_train, X_test, y_train = movies_train[features], movies_test[features], movies_train[target] . model.fit(X_train, y_train) . LGBMRegressor(n_estimators=1000) . singleLGBM = submission.copy() . singleLGBM[&#39;box_off_num&#39;] = model.predict(X_test) . singleLGBM.head() . title box_off_num . 0 용서는 없다 | 2.817995e+06 | . 1 아빠가 여자를 좋아해 | 3.753772e+05 | . 2 하모니 | -5.693243e+05 | . 3 의형제 | 1.581189e+06 | . 4 평행 이론 | -5.277806e+05 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; 음수가 나오므로 잘못된 것 같다. . RMSE 점수를 확인하기 위해서 train 데이터로 학습시킨 모델을 다시 train 데이터에 적용하여 관객수를 구하고 RMSE를 구해보자. 그리고 변수는 어느정도 상관계수가 커보였던 상영시간과 스태프 수 만을 사용하기로 하자. . features = [&#39;time&#39;, &#39;num_staff&#39;] target = [&#39;box_off_num&#39;] . X_train, X_test, y_train = movies_train[features], movies_train[features], movies_train[target] . model.fit(X_train, y_train) . LGBMRegressor(n_estimators=1000) . singleLGBM = movies_train.copy() . singleLGBM[&#39;box_off_num&#39;] = model.predict(X_test) . singleLGBM.head() . title distributor genre release_time time screening_rat director dir_prev_bfnum dir_prev_num num_staff num_actor box_off_num month . 0 개들의 전쟁 | 롯데엔터테인먼트 | 액션 | 2012-11-22 | 96 | 청소년 관람불가 | 조병옥 | 0.00 | 0 | 91 | 2 | 4.245526e+03 | 11 | . 1 내부자들 | (주)쇼박스 | 느와르 | 2015-11-19 | 130 | 청소년 관람불가 | 우민호 | 1161602.50 | 2 | 387 | 3 | 6.805784e+06 | 11 | . 2 은밀하게 위대하게 | (주)쇼박스 | 액션 | 2013-06-05 | 123 | 15세 관람가 | 장철수 | 220775.25 | 4 | 343 | 4 | 6.577795e+06 | 6 | . 3 나는 공무원이다 | (주)NEW | 코미디 | 2012-07-12 | 101 | 전체 관람가 | 구자홍 | 23894.00 | 2 | 20 | 6 | 4.453083e+04 | 7 | . 4 불량남녀 | 쇼박스(주)미디어플렉스 | 코미디 | 2010-11-04 | 108 | 15세 관람가 | 신근호 | 1.00 | 1 | 251 | 2 | 8.484480e+05 | 11 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; def RMSE(true, pred): score = np.sqrt(np.mean(np.square(true-pred))) return score . RMSE(movies_train[&#39;box_off_num&#39;],singleLGBM[&#39;box_off_num&#39;]) . 776662.1566623428 . RMSE가 776,662 정도 나오는 것을 확인할 수 있다. 다른 모델도 사용해서 RMSE를 더 줄일 수 있는지 확인해보자. . from sklearn.model_selection import KFold . k_fold = KFold(n_splits=5, shuffle=True) . model = lgb.LGBMRegressor(n_estimators=1000) models = [] # 모델을 담을 바구니라 생각하자. for train_idx, val_idx in k_fold.split(X_train): x_t = X_train.iloc[train_idx] y_t = y_train.iloc[train_idx] x_val = X_train.iloc[val_idx] y_val = y_train.iloc[val_idx] models.append(model.fit(x_t, y_t, eval_set=(x_val, y_val), early_stopping_rounds=100, verbose = 100)) . Training until validation scores don&#39;t improve for 100 rounds. [100] valid_0&#39;s l2: 1.85132e+12 Early stopping, best iteration is: [7] valid_0&#39;s l2: 1.4462e+12 Training until validation scores don&#39;t improve for 100 rounds. [100] valid_0&#39;s l2: 3.34081e+12 Early stopping, best iteration is: [14] valid_0&#39;s l2: 2.88132e+12 Training until validation scores don&#39;t improve for 100 rounds. [100] valid_0&#39;s l2: 1.97832e+12 Early stopping, best iteration is: [23] valid_0&#39;s l2: 1.8693e+12 Training until validation scores don&#39;t improve for 100 rounds. [100] valid_0&#39;s l2: 1.72764e+12 Early stopping, best iteration is: [29] valid_0&#39;s l2: 1.60885e+12 Training until validation scores don&#39;t improve for 100 rounds. [100] valid_0&#39;s l2: 3.40884e+12 Early stopping, best iteration is: [26] valid_0&#39;s l2: 3.16161e+12 . models . [LGBMRegressor(n_estimators=1000), LGBMRegressor(n_estimators=1000), LGBMRegressor(n_estimators=1000), LGBMRegressor(n_estimators=1000), LGBMRegressor(n_estimators=1000)] . preds = [] for model in models: preds.append(model.predict(X_train)) len(preds) . 5 . pred에 만들어둔 모델들의 예측값들을 저장해준다. . kfoldLightGBM = movies_train.copy() # 답안지 복사. . kfoldLightGBM[&#39;box_off_num&#39;] = np.mean(preds, axis = 0) # mean 평균값함수,axis 축 . RMSE(movies_train[&#39;box_off_num&#39;],kfoldLightGBM[&#39;box_off_num&#39;]) . 1375351.354121486 . RMSE가 1,375,351로 매우 커졌다. 왜일까....?? . from sklearn import preprocessing le = preprocessing.LabelEncoder() movies_train[&#39;genre&#39;] = le.fit_transform(movies_train[&#39;genre&#39;]) . sklearn에서 제공하는 labelEncoder를 활용해서 문자열을 숫자로 변환했다. . features = [&#39;time&#39;, &#39;num_staff&#39;, &#39;dir_prev_bfnum&#39;, &#39;genre&#39;] . features에 전처리된 dir_prev_bfnum와 문자열을 숫자로 변환한 genre를 추가했다. . X_train, X_test, y_train = movies_train[features], movies_train[features], movies_train[target] . model = lgb.LGBMRegressor(random_state=777, n_estimators=1000) models = [] for train_idx, val_idx in k_fold.split(X_train): x_t = X_train.iloc[train_idx] y_t = y_train.iloc[train_idx] x_val = X_train.iloc[val_idx] y_val = y_train.iloc[val_idx] models.append(model.fit(x_t, y_t, eval_set=(x_val, y_val), early_stopping_rounds=100, verbose = 100)) . Training until validation scores don&#39;t improve for 100 rounds. [100] valid_0&#39;s l2: 2.0569e+12 Early stopping, best iteration is: [15] valid_0&#39;s l2: 1.87889e+12 Training until validation scores don&#39;t improve for 100 rounds. [100] valid_0&#39;s l2: 1.78541e+12 Early stopping, best iteration is: [9] valid_0&#39;s l2: 1.29506e+12 Training until validation scores don&#39;t improve for 100 rounds. [100] valid_0&#39;s l2: 2.30124e+12 Early stopping, best iteration is: [33] valid_0&#39;s l2: 2.12538e+12 Training until validation scores don&#39;t improve for 100 rounds. [100] valid_0&#39;s l2: 4.67408e+12 Early stopping, best iteration is: [22] valid_0&#39;s l2: 4.06655e+12 Training until validation scores don&#39;t improve for 100 rounds. [100] valid_0&#39;s l2: 2.30679e+12 Early stopping, best iteration is: [17] valid_0&#39;s l2: 1.76818e+12 . preds = [] for model in models: preds.append(model.predict(X_test)) len(preds) . 5 . feLightGBM = movies_train.copy() . feLightGBM[&#39;box_off_num&#39;] = np.mean(preds, axis = 0) . RMSE(movies_train[&#39;box_off_num&#39;],feLightGBM[&#39;box_off_num&#39;]) . 1360591.313400032 . 바로 위의 RMSE보다는 어느 정도 작아진 것을 알 수 있다. . Grid search &#47784;&#45944; . from sklearn.model_selection import GridSearchCV . model = lgb.LGBMRegressor(n_estimators=1000) . params = { &#39;learning_rate&#39;: [0.1, 0.01, 0.003], &#39;min_child_samples&#39;: [20, 30]} gs = GridSearchCV(estimator=model, param_grid=params, scoring= &#39;neg_mean_squared_error&#39;, cv = k_fold) . params에서 learning_rate는 모델링을 하는 간격으로, 값이 적을수록 점점 더 미세하게 모델의 변화가 이루어진다로 생각하면 된다. scoring을 rmse로 한 이후는 현재 이 대회의 평가지표가 rmse값이기 때문 . gs.fit(X_train, y_train) . GridSearchCV(cv=KFold(n_splits=5, random_state=None, shuffle=True), estimator=LGBMRegressor(n_estimators=1000), param_grid={&#39;learning_rate&#39;: [0.1, 0.01, 0.003], &#39;min_child_samples&#39;: [20, 30]}, scoring=&#39;neg_mean_squared_error&#39;) . gs.best_params_ . {&#39;learning_rate&#39;: 0.003, &#39;min_child_samples&#39;: 30} . model = lgb.LGBMRegressor(n_estimators=1000, learning_rate=0.003, min_child_samples=30) models = [] for train_idx, val_idx in k_fold.split(X_train): x_t = X_train.iloc[train_idx] y_t = y_train.iloc[train_idx] x_val = X_train.iloc[val_idx] y_val = y_train.iloc[val_idx] models.append(model.fit(x_t, y_t, eval_set=(x_val, y_val), early_stopping_rounds=100, verbose = 100)) . Training until validation scores don&#39;t improve for 100 rounds. [100] valid_0&#39;s l2: 4.88516e+12 [200] valid_0&#39;s l2: 4.42603e+12 [300] valid_0&#39;s l2: 4.13354e+12 [400] valid_0&#39;s l2: 3.97867e+12 [500] valid_0&#39;s l2: 3.87486e+12 [600] valid_0&#39;s l2: 3.80364e+12 [700] valid_0&#39;s l2: 3.75367e+12 [800] valid_0&#39;s l2: 3.73486e+12 [900] valid_0&#39;s l2: 3.73187e+12 Early stopping, best iteration is: [889] valid_0&#39;s l2: 3.73167e+12 Training until validation scores don&#39;t improve for 100 rounds. [100] valid_0&#39;s l2: 2.18551e+12 [200] valid_0&#39;s l2: 1.88159e+12 [300] valid_0&#39;s l2: 1.71548e+12 [400] valid_0&#39;s l2: 1.6308e+12 [500] valid_0&#39;s l2: 1.60349e+12 [600] valid_0&#39;s l2: 1.59885e+12 [700] valid_0&#39;s l2: 1.59966e+12 Early stopping, best iteration is: [628] valid_0&#39;s l2: 1.59791e+12 Training until validation scores don&#39;t improve for 100 rounds. [100] valid_0&#39;s l2: 1.63836e+12 [200] valid_0&#39;s l2: 1.49624e+12 [300] valid_0&#39;s l2: 1.37358e+12 [400] valid_0&#39;s l2: 1.28814e+12 [500] valid_0&#39;s l2: 1.23785e+12 [600] valid_0&#39;s l2: 1.2091e+12 [700] valid_0&#39;s l2: 1.18886e+12 [800] valid_0&#39;s l2: 1.17838e+12 [900] valid_0&#39;s l2: 1.15994e+12 [1000] valid_0&#39;s l2: 1.14697e+12 Did not meet early stopping. Best iteration is: [1000] valid_0&#39;s l2: 1.14697e+12 Training until validation scores don&#39;t improve for 100 rounds. [100] valid_0&#39;s l2: 1.54254e+12 [200] valid_0&#39;s l2: 1.32956e+12 [300] valid_0&#39;s l2: 1.26545e+12 [400] valid_0&#39;s l2: 1.26595e+12 Early stopping, best iteration is: [335] valid_0&#39;s l2: 1.2583e+12 Training until validation scores don&#39;t improve for 100 rounds. [100] valid_0&#39;s l2: 3.88743e+12 [200] valid_0&#39;s l2: 3.52199e+12 [300] valid_0&#39;s l2: 3.32668e+12 [400] valid_0&#39;s l2: 3.26764e+12 [500] valid_0&#39;s l2: 3.22996e+12 [600] valid_0&#39;s l2: 3.2196e+12 [700] valid_0&#39;s l2: 3.21357e+12 [800] valid_0&#39;s l2: 3.21046e+12 [900] valid_0&#39;s l2: 3.20925e+12 [1000] valid_0&#39;s l2: 3.2054e+12 Did not meet early stopping. Best iteration is: [999] valid_0&#39;s l2: 3.20518e+12 . preds = [] for model in models: preds.append(model.predict(X_test)) . gslgbm = movies_train.copy() . gslgbm[&#39;box_off_num&#39;] = np.mean(preds, axis=0) . RMSE(movies_train[&#39;box_off_num&#39;],gslgbm[&#39;box_off_num&#39;]) . 1371034.705755213 . 위보다는 조금 더 작아졌는데 왜이렇게 클까....?? . train 데이터를 train 과 test로 나눠서 확인을 했어야 하는데 train 데이터 전체로 모델을 만든 뒤 train데이터 전체를 사용해서 예측을 해서 이상해진 것 같다....?? . train 데이터로 모델을 만들고 정확히 같은 데이터를 예측하게 했는데 RMSE가 왜 저렇게 크게 나오는 걸까 .",
            "url": "https://shw9807.github.io/shw9807blog/%EB%8D%B0%EC%9D%B4%EC%BD%98/2022/01/13/%EC%98%81%ED%99%94%EA%B4%80%EA%B0%9D%EC%88%98_%EC%98%88%EC%B8%A1_%EB%AA%A8%EB%8D%B8.html",
            "relUrl": "/%EB%8D%B0%EC%9D%B4%EC%BD%98/2022/01/13/%EC%98%81%ED%99%94%EA%B4%80%EA%B0%9D%EC%88%98_%EC%98%88%EC%B8%A1_%EB%AA%A8%EB%8D%B8.html",
            "date": " • Jan 13, 2022"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "about",
          "content": "2022년부터 ML/DL 공부중 .",
          "url": "https://shw9807.github.io/shw9807blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://shw9807.github.io/shw9807blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}