{
  
    
        "post0": {
            "title": "[Colab] 영화 관객수 예측모델",
            "content": "&#50689;&#54868; &#44288;&#44061;&#49688; &#50696;&#52769; &#47784;&#45944; . EDA를 위한 패키지 불러오기 . !sudo apt-get install -y fonts-nanum !sudo fc-cache -fv !rm ~/.cache/matplotlib -rf . Reading package lists... Done Building dependency tree Reading state information... Done The following NEW packages will be installed: fonts-nanum 0 upgraded, 1 newly installed, 0 to remove and 37 not upgraded. Need to get 9,604 kB of archives. After this operation, 29.5 MB of additional disk space will be used. Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 fonts-nanum all 20170925-1 [9,604 kB] Fetched 9,604 kB in 1s (8,421 kB/s) debconf: unable to initialize frontend: Dialog debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, &lt;&gt; line 1.) debconf: falling back to frontend: Readline debconf: unable to initialize frontend: Readline debconf: (This frontend requires a controlling tty.) debconf: falling back to frontend: Teletype dpkg-preconfigure: unable to re-open stdin: Selecting previously unselected package fonts-nanum. (Reading database ... 155229 files and directories currently installed.) Preparing to unpack .../fonts-nanum_20170925-1_all.deb ... Unpacking fonts-nanum (20170925-1) ... Setting up fonts-nanum (20170925-1) ... Processing triggers for fontconfig (2.12.6-0ubuntu2) ... /usr/share/fonts: caching, new cache contents: 0 fonts, 1 dirs /usr/share/fonts/truetype: caching, new cache contents: 0 fonts, 3 dirs /usr/share/fonts/truetype/humor-sans: caching, new cache contents: 1 fonts, 0 dirs /usr/share/fonts/truetype/liberation: caching, new cache contents: 16 fonts, 0 dirs /usr/share/fonts/truetype/nanum: caching, new cache contents: 10 fonts, 0 dirs /usr/local/share/fonts: caching, new cache contents: 0 fonts, 0 dirs /root/.local/share/fonts: skipping, no such directory /root/.fonts: skipping, no such directory /var/cache/fontconfig: cleaning cache directory /root/.cache/fontconfig: not cleaning non-existent cache directory /root/.fontconfig: not cleaning non-existent cache directory fc-cache: succeeded . import pandas as pd import seaborn as sns import matplotlib.pyplot as plt import lightgbm as lgb import warnings warnings.filterwarnings(&quot;ignore&quot;) # 필요한 패키지와 라이브러리를 가져옴 import matplotlib as mpl import matplotlib.pyplot as plt import matplotlib.font_manager as fm %matplotlib inline plt.rc(&#39;font&#39;, family=&#39;NanumBarunGothic&#39;) mpl.rcParams[&#39;axes.unicode_minus&#39;] = False . import numpy as np . movies_test = pd.read_csv(&quot;/content/drive/MyDrive/Colab Notebooks/영화관객수/movies_test.csv&quot;) movies_train = pd.read_csv(&quot;/content/drive/MyDrive/Colab Notebooks/영화관객수/movies_train.csv&quot;) . submission = pd.read_csv(&quot;/content/drive/MyDrive/Colab Notebooks/영화관객수/submission.csv&quot;) . movies_train.head() . title distributor genre release_time time screening_rat director dir_prev_bfnum dir_prev_num num_staff num_actor box_off_num . 0 개들의 전쟁 | 롯데엔터테인먼트 | 액션 | 2012-11-22 | 96 | 청소년 관람불가 | 조병옥 | NaN | 0 | 91 | 2 | 23398 | . 1 내부자들 | (주)쇼박스 | 느와르 | 2015-11-19 | 130 | 청소년 관람불가 | 우민호 | 1161602.50 | 2 | 387 | 3 | 7072501 | . 2 은밀하게 위대하게 | (주)쇼박스 | 액션 | 2013-06-05 | 123 | 15세 관람가 | 장철수 | 220775.25 | 4 | 343 | 4 | 6959083 | . 3 나는 공무원이다 | (주)NEW | 코미디 | 2012-07-12 | 101 | 전체 관람가 | 구자홍 | 23894.00 | 2 | 20 | 6 | 217866 | . 4 불량남녀 | 쇼박스(주)미디어플렉스 | 코미디 | 2010-11-04 | 108 | 15세 관람가 | 신근호 | 1.00 | 1 | 251 | 2 | 483387 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; title : 영화의 제목 distributor : 배급사 genre : 장르 release_time : 개봉일 time : 상영시간(분) screening_rat : 상영등급 director : 감독이름 dir_prev_bfnum : 해당 감독이 이 영화를 만들기 전 제작에 참여한 영화에서의 평균 관객수(단 관객수가 알려지지 않은 영화 제외) dir_prev_num : 해당 감독이 이 영화를 만들기 전 제작에 참여한 영화의 개수(단 관객수가 알려지지 않은 영화 제외) num_staff : 스텝수 num_actor : 주연배우수 box_off_num : 관객수 . movies_train.isnull().any() . title False distributor False genre False release_time False time False screening_rat False director False dir_prev_bfnum True dir_prev_num False num_staff False num_actor False box_off_num False dtype: bool . dir_prev_bfnum 항목에 결측치가 존재함을 알 수 있다. -&gt; 해당 영화가 그 감독의 첫 영화인 경우에 결측치를 가질 것이다. . movies_train[movies_train[&#39;dir_prev_bfnum&#39;].isnull() == True].head() . title distributor genre release_time time screening_rat director dir_prev_bfnum dir_prev_num num_staff num_actor box_off_num . 0 개들의 전쟁 | 롯데엔터테인먼트 | 액션 | 2012-11-22 | 96 | 청소년 관람불가 | 조병옥 | NaN | 0 | 91 | 2 | 23398 | . 6 길위에서 | 백두대간 | 다큐멘터리 | 2013-05-23 | 104 | 전체 관람가 | 이창재 | NaN | 0 | 32 | 5 | 53526 | . 8 1789, 바스티유의 연인들 | 유니버설픽쳐스인터내셔널코리아 | 뮤지컬 | 2014-09-18 | 129 | 전체 관람가 | 정성복 | NaN | 0 | 3 | 5 | 4778 | . 9 청춘그루브 | (주)두타연 | 드라마 | 2012-03-15 | 94 | 15세 관람가 | 변성현 | NaN | 0 | 138 | 3 | 868 | . 10 AV 아이돌 | (주) 케이알씨지 | 멜로/로맨스 | 2015-07-27 | 89 | 청소년 관람불가 | 조조 히데오 | NaN | 0 | 0 | 4 | 745 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; movies_train = movies_train.fillna(0) . movies_test = movies_test.fillna(0) . 결측치를 0으로 바꾸어 주었다. . movies_train.isnull().any() . title False distributor False genre False release_time False time False screening_rat False director False dir_prev_bfnum False dir_prev_num False num_staff False num_actor False box_off_num False dtype: bool . movies_test.isnull().any() . title False distributor False genre False release_time False time False screening_rat False director False dir_prev_bfnum False dir_prev_num False num_staff False num_actor False dtype: bool . 결측치가 없어진 것을 확인할 수 있다. . movies_train[&#39;month&#39;] = movies_train[&#39;release_time&#39;].str[5:7].astype(&#39;int&#39;) . plt.figure(figsize=[10,10]) sns.scatterplot(data=movies_train, x=&#39;month&#39;, y = &#39;box_off_num&#39;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fd8789c0510&gt; . 방학시즌에 개봉한 영화가 대체로 관객 수가 많은 것처럼 보인다. . plt.figure(figsize=[10,10]) sns.scatterplot(data=movies_train, x=&#39;num_staff&#39;, y = &#39;box_off_num&#39;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fd87898e490&gt; . 스태프 수가 많은 영화가 대체로 관객 수가 많은 것처럼 보인다. . plt.figure(figsize=[10,10]) sns.scatterplot(data=movies_train, x=&#39;screening_rat&#39;, y = &#39;box_off_num&#39;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fd8789735d0&gt; . 전체 관람가나 청소년 관람불가 영화보다는 12세관람가나 15세관람가 영화가 관객수가 많은 편인 것을 알 수 있다. . plt.figure(figsize=[10,10]) sns.scatterplot(data=movies_train, x=&#39;time&#39;, y = &#39;box_off_num&#39;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fd878917710&gt; . 상영시간이 지나치게 짧거나 2시간30분을 넘어가면 관객수가 대체로 적다는 것을 확인할 수 있다. . movies_train[[&#39;genre&#39;, &#39;box_off_num&#39;]].groupby(&#39;genre&#39;).mean().sort_values(&#39;box_off_num&#39;) . box_off_num . genre . 뮤지컬 6.627000e+03 | . 다큐멘터리 6.717226e+04 | . 서스펜스 8.261100e+04 | . 애니메이션 1.819267e+05 | . 멜로/로맨스 4.259680e+05 | . 미스터리 5.275482e+05 | . 공포 5.908325e+05 | . 드라마 6.256898e+05 | . 코미디 1.193914e+06 | . SF 1.788346e+06 | . 액션 2.203974e+06 | . 느와르 2.263695e+06 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; 장르별로 평균 관객수를 정렬해보았다. 액션과 느와르 장르가 대체적으로 관객수가 많은 것을 확인할 수 있다. . %matplotlib inline movies_train.hist(bins = 50, figsize = (20,15)) plt.show() . 변수별로 히스토그램을 그려보았다. . sns.heatmap(movies_train.corr(), annot = True) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fd86f758e50&gt; . 히트맵그림을 그려보았다.관객수와 상관관계가 있어보이는 변수는 스태프의 수와 상영시간 정도로 보인다. . &#48320;&#49688; &#49440;&#53469;&#44284; &#47784;&#45944; &#44396;&#52629; . lightGBM (base model) . model = lgb.LGBMRegressor(n_estimators=1000) . features = [&#39;time&#39;, &#39;dir_prev_num&#39;, &#39;num_staff&#39;, &#39;num_actor&#39;] target = [&#39;box_off_num&#39;] . X_train, X_test, y_train = movies_train[features], movies_test[features], movies_train[target] . model.fit(X_train, y_train) . LGBMRegressor(n_estimators=1000) . singleLGBM = submission.copy() . singleLGBM[&#39;box_off_num&#39;] = model.predict(X_test) . singleLGBM.head() . title box_off_num . 0 용서는 없다 | 2.817995e+06 | . 1 아빠가 여자를 좋아해 | 3.753772e+05 | . 2 하모니 | -5.693243e+05 | . 3 의형제 | 1.581189e+06 | . 4 평행 이론 | -5.277806e+05 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; 음수가 나오므로 잘못된 것 같다. . RMSE 점수를 확인하기 위해서 train 데이터로 학습시킨 모델을 다시 train 데이터에 적용하여 관객수를 구하고 RMSE를 구해보자. 그리고 변수는 어느정도 상관계수가 커보였던 상영시간과 스태프 수 만을 사용하기로 하자. . features = [&#39;time&#39;, &#39;num_staff&#39;] target = [&#39;box_off_num&#39;] . X_train, X_test, y_train = movies_train[features], movies_train[features], movies_train[target] . model.fit(X_train, y_train) . LGBMRegressor(n_estimators=1000) . singleLGBM = movies_train.copy() . singleLGBM[&#39;box_off_num&#39;] = model.predict(X_test) . singleLGBM.head() . title distributor genre release_time time screening_rat director dir_prev_bfnum dir_prev_num num_staff num_actor box_off_num month . 0 개들의 전쟁 | 롯데엔터테인먼트 | 액션 | 2012-11-22 | 96 | 청소년 관람불가 | 조병옥 | 0.00 | 0 | 91 | 2 | 4.245526e+03 | 11 | . 1 내부자들 | (주)쇼박스 | 느와르 | 2015-11-19 | 130 | 청소년 관람불가 | 우민호 | 1161602.50 | 2 | 387 | 3 | 6.805784e+06 | 11 | . 2 은밀하게 위대하게 | (주)쇼박스 | 액션 | 2013-06-05 | 123 | 15세 관람가 | 장철수 | 220775.25 | 4 | 343 | 4 | 6.577795e+06 | 6 | . 3 나는 공무원이다 | (주)NEW | 코미디 | 2012-07-12 | 101 | 전체 관람가 | 구자홍 | 23894.00 | 2 | 20 | 6 | 4.453083e+04 | 7 | . 4 불량남녀 | 쇼박스(주)미디어플렉스 | 코미디 | 2010-11-04 | 108 | 15세 관람가 | 신근호 | 1.00 | 1 | 251 | 2 | 8.484480e+05 | 11 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; def RMSE(true, pred): score = np.sqrt(np.mean(np.square(true-pred))) return score . RMSE(movies_train[&#39;box_off_num&#39;],singleLGBM[&#39;box_off_num&#39;]) . 776662.1566623428 . RMSE가 776,662 정도 나오는 것을 확인할 수 있다. 다른 모델도 사용해서 RMSE를 더 줄일 수 있는지 확인해보자. . from sklearn.model_selection import KFold . k_fold = KFold(n_splits=5, shuffle=True) . model = lgb.LGBMRegressor(n_estimators=1000) models = [] # 모델을 담을 바구니라 생각하자. for train_idx, val_idx in k_fold.split(X_train): x_t = X_train.iloc[train_idx] y_t = y_train.iloc[train_idx] x_val = X_train.iloc[val_idx] y_val = y_train.iloc[val_idx] models.append(model.fit(x_t, y_t, eval_set=(x_val, y_val), early_stopping_rounds=100, verbose = 100)) . Training until validation scores don&#39;t improve for 100 rounds. [100] valid_0&#39;s l2: 1.85132e+12 Early stopping, best iteration is: [7] valid_0&#39;s l2: 1.4462e+12 Training until validation scores don&#39;t improve for 100 rounds. [100] valid_0&#39;s l2: 3.34081e+12 Early stopping, best iteration is: [14] valid_0&#39;s l2: 2.88132e+12 Training until validation scores don&#39;t improve for 100 rounds. [100] valid_0&#39;s l2: 1.97832e+12 Early stopping, best iteration is: [23] valid_0&#39;s l2: 1.8693e+12 Training until validation scores don&#39;t improve for 100 rounds. [100] valid_0&#39;s l2: 1.72764e+12 Early stopping, best iteration is: [29] valid_0&#39;s l2: 1.60885e+12 Training until validation scores don&#39;t improve for 100 rounds. [100] valid_0&#39;s l2: 3.40884e+12 Early stopping, best iteration is: [26] valid_0&#39;s l2: 3.16161e+12 . models . [LGBMRegressor(n_estimators=1000), LGBMRegressor(n_estimators=1000), LGBMRegressor(n_estimators=1000), LGBMRegressor(n_estimators=1000), LGBMRegressor(n_estimators=1000)] . preds = [] for model in models: preds.append(model.predict(X_train)) len(preds) . 5 . pred에 만들어둔 모델들의 예측값들을 저장해준다. . kfoldLightGBM = movies_train.copy() # 답안지 복사. . kfoldLightGBM[&#39;box_off_num&#39;] = np.mean(preds, axis = 0) # mean 평균값함수,axis 축 . RMSE(movies_train[&#39;box_off_num&#39;],kfoldLightGBM[&#39;box_off_num&#39;]) . 1375351.354121486 . RMSE가 1,375,351로 매우 커졌다. 왜일까....?? . from sklearn import preprocessing le = preprocessing.LabelEncoder() movies_train[&#39;genre&#39;] = le.fit_transform(movies_train[&#39;genre&#39;]) . sklearn에서 제공하는 labelEncoder를 활용해서 문자열을 숫자로 변환했다. . features = [&#39;time&#39;, &#39;num_staff&#39;, &#39;dir_prev_bfnum&#39;, &#39;genre&#39;] . features에 전처리된 dir_prev_bfnum와 문자열을 숫자로 변환한 genre를 추가했다. . X_train, X_test, y_train = movies_train[features], movies_train[features], movies_train[target] . model = lgb.LGBMRegressor(random_state=777, n_estimators=1000) models = [] for train_idx, val_idx in k_fold.split(X_train): x_t = X_train.iloc[train_idx] y_t = y_train.iloc[train_idx] x_val = X_train.iloc[val_idx] y_val = y_train.iloc[val_idx] models.append(model.fit(x_t, y_t, eval_set=(x_val, y_val), early_stopping_rounds=100, verbose = 100)) . Training until validation scores don&#39;t improve for 100 rounds. [100] valid_0&#39;s l2: 2.0569e+12 Early stopping, best iteration is: [15] valid_0&#39;s l2: 1.87889e+12 Training until validation scores don&#39;t improve for 100 rounds. [100] valid_0&#39;s l2: 1.78541e+12 Early stopping, best iteration is: [9] valid_0&#39;s l2: 1.29506e+12 Training until validation scores don&#39;t improve for 100 rounds. [100] valid_0&#39;s l2: 2.30124e+12 Early stopping, best iteration is: [33] valid_0&#39;s l2: 2.12538e+12 Training until validation scores don&#39;t improve for 100 rounds. [100] valid_0&#39;s l2: 4.67408e+12 Early stopping, best iteration is: [22] valid_0&#39;s l2: 4.06655e+12 Training until validation scores don&#39;t improve for 100 rounds. [100] valid_0&#39;s l2: 2.30679e+12 Early stopping, best iteration is: [17] valid_0&#39;s l2: 1.76818e+12 . preds = [] for model in models: preds.append(model.predict(X_test)) len(preds) . 5 . feLightGBM = movies_train.copy() . feLightGBM[&#39;box_off_num&#39;] = np.mean(preds, axis = 0) . RMSE(movies_train[&#39;box_off_num&#39;],feLightGBM[&#39;box_off_num&#39;]) . 1360591.313400032 . 바로 위의 RMSE보다는 어느 정도 작아진 것을 알 수 있다. . Grid search &#47784;&#45944; . from sklearn.model_selection import GridSearchCV . model = lgb.LGBMRegressor(n_estimators=1000) . params = { &#39;learning_rate&#39;: [0.1, 0.01, 0.003], &#39;min_child_samples&#39;: [20, 30]} gs = GridSearchCV(estimator=model, param_grid=params, scoring= &#39;neg_mean_squared_error&#39;, cv = k_fold) . params에서 learning_rate는 모델링을 하는 간격으로, 값이 적을수록 점점 더 미세하게 모델의 변화가 이루어진다로 생각하면 된다. scoring을 rmse로 한 이후는 현재 이 대회의 평가지표가 rmse값이기 때문 . gs.fit(X_train, y_train) . GridSearchCV(cv=KFold(n_splits=5, random_state=None, shuffle=True), estimator=LGBMRegressor(n_estimators=1000), param_grid={&#39;learning_rate&#39;: [0.1, 0.01, 0.003], &#39;min_child_samples&#39;: [20, 30]}, scoring=&#39;neg_mean_squared_error&#39;) . gs.best_params_ . {&#39;learning_rate&#39;: 0.003, &#39;min_child_samples&#39;: 30} . model = lgb.LGBMRegressor(n_estimators=1000, learning_rate=0.003, min_child_samples=30) models = [] for train_idx, val_idx in k_fold.split(X_train): x_t = X_train.iloc[train_idx] y_t = y_train.iloc[train_idx] x_val = X_train.iloc[val_idx] y_val = y_train.iloc[val_idx] models.append(model.fit(x_t, y_t, eval_set=(x_val, y_val), early_stopping_rounds=100, verbose = 100)) . Training until validation scores don&#39;t improve for 100 rounds. [100] valid_0&#39;s l2: 4.88516e+12 [200] valid_0&#39;s l2: 4.42603e+12 [300] valid_0&#39;s l2: 4.13354e+12 [400] valid_0&#39;s l2: 3.97867e+12 [500] valid_0&#39;s l2: 3.87486e+12 [600] valid_0&#39;s l2: 3.80364e+12 [700] valid_0&#39;s l2: 3.75367e+12 [800] valid_0&#39;s l2: 3.73486e+12 [900] valid_0&#39;s l2: 3.73187e+12 Early stopping, best iteration is: [889] valid_0&#39;s l2: 3.73167e+12 Training until validation scores don&#39;t improve for 100 rounds. [100] valid_0&#39;s l2: 2.18551e+12 [200] valid_0&#39;s l2: 1.88159e+12 [300] valid_0&#39;s l2: 1.71548e+12 [400] valid_0&#39;s l2: 1.6308e+12 [500] valid_0&#39;s l2: 1.60349e+12 [600] valid_0&#39;s l2: 1.59885e+12 [700] valid_0&#39;s l2: 1.59966e+12 Early stopping, best iteration is: [628] valid_0&#39;s l2: 1.59791e+12 Training until validation scores don&#39;t improve for 100 rounds. [100] valid_0&#39;s l2: 1.63836e+12 [200] valid_0&#39;s l2: 1.49624e+12 [300] valid_0&#39;s l2: 1.37358e+12 [400] valid_0&#39;s l2: 1.28814e+12 [500] valid_0&#39;s l2: 1.23785e+12 [600] valid_0&#39;s l2: 1.2091e+12 [700] valid_0&#39;s l2: 1.18886e+12 [800] valid_0&#39;s l2: 1.17838e+12 [900] valid_0&#39;s l2: 1.15994e+12 [1000] valid_0&#39;s l2: 1.14697e+12 Did not meet early stopping. Best iteration is: [1000] valid_0&#39;s l2: 1.14697e+12 Training until validation scores don&#39;t improve for 100 rounds. [100] valid_0&#39;s l2: 1.54254e+12 [200] valid_0&#39;s l2: 1.32956e+12 [300] valid_0&#39;s l2: 1.26545e+12 [400] valid_0&#39;s l2: 1.26595e+12 Early stopping, best iteration is: [335] valid_0&#39;s l2: 1.2583e+12 Training until validation scores don&#39;t improve for 100 rounds. [100] valid_0&#39;s l2: 3.88743e+12 [200] valid_0&#39;s l2: 3.52199e+12 [300] valid_0&#39;s l2: 3.32668e+12 [400] valid_0&#39;s l2: 3.26764e+12 [500] valid_0&#39;s l2: 3.22996e+12 [600] valid_0&#39;s l2: 3.2196e+12 [700] valid_0&#39;s l2: 3.21357e+12 [800] valid_0&#39;s l2: 3.21046e+12 [900] valid_0&#39;s l2: 3.20925e+12 [1000] valid_0&#39;s l2: 3.2054e+12 Did not meet early stopping. Best iteration is: [999] valid_0&#39;s l2: 3.20518e+12 . preds = [] for model in models: preds.append(model.predict(X_test)) . gslgbm = movies_train.copy() . gslgbm[&#39;box_off_num&#39;] = np.mean(preds, axis=0) . RMSE(movies_train[&#39;box_off_num&#39;],gslgbm[&#39;box_off_num&#39;]) . 1371034.705755213 . 위보다는 조금 더 작아졌는데 왜이렇게 클까....?? . train 데이터를 train 과 test로 나눠서 확인을 했어야 하는데 train 데이터 전체로 모델을 만든 뒤 train데이터 전체를 사용해서 예측을 해서 이상해진 것 같다....?? . train 데이터로 모델을 만들고 정확히 같은 데이터를 예측하게 했는데 RMSE가 왜 저렇게 크게 나오는 걸까 .",
            "url": "https://shw9807.github.io/shw9807blog/%EB%8D%B0%EC%9D%B4%EC%BD%98/2022/01/13/%EC%98%81%ED%99%94%EA%B4%80%EA%B0%9D%EC%88%98_%EC%98%88%EC%B8%A1_%EB%AA%A8%EB%8D%B8.html",
            "relUrl": "/%EB%8D%B0%EC%9D%B4%EC%BD%98/2022/01/13/%EC%98%81%ED%99%94%EA%B4%80%EA%B0%9D%EC%88%98_%EC%98%88%EC%B8%A1_%EB%AA%A8%EB%8D%B8.html",
            "date": " • Jan 13, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "모두를_위한_머신러닝_강의_정리",
            "content": "“모두를 위한 머신러닝 강의 정리 “ . toc:true | branch: master | badges: true | author: shw9807 | . 모두를 위한 머신러닝 . Lecture 1 : 기본적인 Machine Learning의 용어와 개념 설명 . Machine Learning 이란? . : 학습을 통해서 뭔가를 배우는 능력을 갖는 프로그램 . 1.Supervised learning - 이미 label들이 정해져 있는 데이터를 통해서 학습 2.Unsupervised learning - label을 정해주는 것이 아니라 데이터만을 보고 스스로 학습 . Supervised learning . 일반적으로 사용하는 방법 . 이미지 라벨링 | 이메일 스팸 필터 | 시험 성적 예측 | . Supervised learning의 종류 . 회귀모형 | 이항 분류 | multi-label 분류 | . Lecture 2 : Linear Regression . 세상에서 관측되는 여러 현상들은 선형 모델로 설명되는 경우가 굉장히 많음. . (Linear) Hypothesis (가설) . $ H(x) = Wx + b $ . Cost function (= Loss function) . 가설이 실제 데이터와 얼마나 일치하는가 $ (H(x) - y)^2 $ $ cost(W,b) = frac{1}{m} displaystyle sum_{i=1}^{m}{(H(x^{(i)} )-y^{(i)} )^2} $ . 목표 : Minimize cost . $ minimize ,cost(W,b) $ . Lecture 3 : How to minimize cost . Gradient descent algorithm . $cost(W,b)$에서 뿐만 아니라 많은 값을 가지는 cost function도 minimize 할 수 있다. . 아무 값에서든 시작한다. 2.cost(W,b)를 감소시키는 방향으로 W와 b를 조금 변화시킨다. | 더 이상 변화시킬 수 없을 때까지 반복한다. 이 알고리즘의 장점 어떤 점에서 시작하든 같은 최솟값을 얻을 수 있다.(몇몇 예외의 경우를 제외하고) | . | Formal definition . $ cost(W,b) = frac1m displaystyle sum_{i=1}^{m}{(Wx^{(i)}-y^{(i)} )^2} $ $ to cost(W,b) = frac{1}{2m} displaystyle sum_{i=1}^{m}{(Wx^{(i)}-y^{(i)} )^2} $ 미분을 쉽게 하기 위해 분모를 2m으로 변화시킨다. . $ W := W - alpha frac{d}{dW} cost(W)$ $ W := W - alpha frac{d}{dW} frac{1}{2m} displaystyle sum_{i=1}^{m}{(Wx^{(i)}-y^{(i)} )^2}$ $ W := W - alpha frac{1}{2m} displaystyle sum_{i=1}^{m}{2(Wx^{(i)}-y^{(i)} )x^{(i)}}$ $ W := W - alpha frac{1}{m} displaystyle sum_{i=1}^{m}{(Wx^{(i)}-y^{(i)} )x^{(i)}}$ . Gradient descent algorithm . $ W := W - alpha frac{1}{m} displaystyle sum_{i=1}^{m}{(Wx^{(i)}-y^{(i)} )x^{(i)}}$ . Gradient descent algorithm을 적용하기 전 cost function의 모양을 확인해봐야함 -&gt; 최소제곱법??? . Lecture 4 : Multivariable linear regression . Hypothesis . $ H(x_1,x_2,x_3) = w_1x_1 + w_2x_2 + w_3x_3 + b$ . cost function . $ cost(W,b) = frac1m displaystyle sum_{i=1}^{m}{(H(x_1^{(i)},x_2^{(i)},x_3^{(i)})-y^{(i)} )^2} $ . Hypothesis using matrix . $w_1x_1 + w_2x_2 + w_3x_3 +… +w_nx_n$ . $ (x_1 ,x_2 ,x_3) · begin{pmatrix}w_1 w_2 w_3 end{pmatrix} = (x_1w_1 + x_2w_2 + x_3w_3) H(X) = XW $ . WX vs XW . Lecture (theory) : $ H(x) = Wx + b $ | Implementation (TensorFlow) : $ H(X) = XW$ | . Lecture 5-1 : Logistic(regression) classification . classification algorithm 중에서 굉장히 정확도가 높은 알고리즘 | 실제 문제에도 바로 적용할 수 있음. | 수업의 궁극적 목표인 뉴럴 네트워크와 deep learning 에 굉장히 중요한 component | . Regression . Hypothesis: $H(X) = WX$ | Cost: $cost(W) = frac1m sum(WX-y)^2$ | Gradient decent: $W := W - alpha frac{d}{dW}cost(W)$ | . (Binary) Classification . Spam Detection: Spam or Ham | Facebook feed: show or hide | Credit Cart Fraudulent Transaction detection | . Linear regression . We know Y is 0 or 1 $ H(x) = Wx + b $ | Hypothesis 는 1보다 크거나 0보다 작은 값을 가질 수도 있다. | . Logistic Hypothesis . $ H(X) = frac1{1+e^{-W^TX}} $ . Lecture 5-2 : Logistic(regression) classification: cost function &amp; gradient decent . New cost function for logistic . $ cost(W) = frac1m sum c(H(x),y) $ $ c(H(x),y) = begin{cases} -log(H(x)) quad ; ;:y=1 -log(1-H(x)) , :y=0 end{cases}$ $C(H(x),y) = -ylog(H(x)) - (1-y)log(1-H(x))$ . Gradient decent algorithm . $W:= W- alpha frac d{dW}cost(W)$ . # cost function cost = tf.reduce_mean(-tf.reduce_sum(Y*tf.log(hypothesis) + (1-Y)*tf.log(1-hypothesis))) #Minimize a = tf.Variable(0.1) # learning rate, alpha optimizer = tf.train.GradientDescentOptimizer(a) train = optimizer.minimize(cost) . NameError Traceback (most recent call last) ~ AppData Local Temp/ipykernel_8704/1539947199.py in &lt;module&gt; 1 # cost function -&gt; 2 cost = tf.reduce_mean(-tf.reduce_sum(Y*tf.log(hypothesis) + (1-Y)*tf.log(1-hypothesis))) 3 4 #Minimize 5 a = tf.Variable(0.1) # learning rate, alpha NameError: name &#39;tf&#39; is not defined . Lecture 6-1 : Softmax classification : Multinomial classification . Multinomial classification . . . Lecture 6-2 : Softmax classifier의 cost function . .",
            "url": "https://shw9807.github.io/shw9807blog/2022/01/12/%EB%AA%A8%EB%91%90%EB%A5%BC_%EC%9C%84%ED%95%9C_%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D_%EA%B0%95%EC%9D%98_%EC%A0%95%EB%A6%AC.html",
            "relUrl": "/2022/01/12/%EB%AA%A8%EB%91%90%EB%A5%BC_%EC%9C%84%ED%95%9C_%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D_%EA%B0%95%EC%9D%98_%EC%A0%95%EB%A6%AC.html",
            "date": " • Jan 12, 2022"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://shw9807.github.io/shw9807blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://shw9807.github.io/shw9807blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}