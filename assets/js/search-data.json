{
  
    
        "post0": {
            "title": "[Colab] 물류 유통량 예측",
            "content": ". &#47932;&#47448; &#50976;&#53685;&#47049; &#50696;&#52769; . 1.Library &amp; Data Load . from google.colab import drive drive.mount(&#39;/content/drive&#39;) . Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(&#34;/content/drive&#34;, force_remount=True). . !pip install catboost !pip install optuna . Requirement already satisfied: catboost in /usr/local/lib/python3.7/dist-packages (1.0.4) Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from catboost) (1.15.0) Requirement already satisfied: pandas&gt;=0.24.0 in /usr/local/lib/python3.7/dist-packages (from catboost) (1.3.5) Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from catboost) (3.2.2) Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from catboost) (1.4.1) Requirement already satisfied: graphviz in /usr/local/lib/python3.7/dist-packages (from catboost) (0.10.1) Requirement already satisfied: numpy&gt;=1.16.0 in /usr/local/lib/python3.7/dist-packages (from catboost) (1.19.5) Requirement already satisfied: plotly in /usr/local/lib/python3.7/dist-packages (from catboost) (5.5.0) Requirement already satisfied: pytz&gt;=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas&gt;=0.24.0-&gt;catboost) (2018.9) Requirement already satisfied: python-dateutil&gt;=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas&gt;=0.24.0-&gt;catboost) (2.8.2) Requirement already satisfied: cycler&gt;=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib-&gt;catboost) (0.11.0) Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,&gt;=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib-&gt;catboost) (3.0.7) Requirement already satisfied: kiwisolver&gt;=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib-&gt;catboost) (1.3.2) Requirement already satisfied: tenacity&gt;=6.2.0 in /usr/local/lib/python3.7/dist-packages (from plotly-&gt;catboost) (8.0.1) Requirement already satisfied: optuna in /usr/local/lib/python3.7/dist-packages (2.10.0) Requirement already satisfied: scipy!=1.4.0 in /usr/local/lib/python3.7/dist-packages (from optuna) (1.4.1) Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from optuna) (4.62.3) Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from optuna) (3.13) Requirement already satisfied: cmaes&gt;=0.8.2 in /usr/local/lib/python3.7/dist-packages (from optuna) (0.8.2) Requirement already satisfied: alembic in /usr/local/lib/python3.7/dist-packages (from optuna) (1.7.6) Requirement already satisfied: colorlog in /usr/local/lib/python3.7/dist-packages (from optuna) (6.6.0) Requirement already satisfied: sqlalchemy&gt;=1.1.0 in /usr/local/lib/python3.7/dist-packages (from optuna) (1.4.31) Requirement already satisfied: cliff in /usr/local/lib/python3.7/dist-packages (from optuna) (3.10.0) Requirement already satisfied: packaging&gt;=20.0 in /usr/local/lib/python3.7/dist-packages (from optuna) (21.3) Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from optuna) (1.19.5) Requirement already satisfied: pyparsing!=3.0.5,&gt;=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging&gt;=20.0-&gt;optuna) (3.0.7) Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.7/dist-packages (from sqlalchemy&gt;=1.1.0-&gt;optuna) (1.1.2) Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from sqlalchemy&gt;=1.1.0-&gt;optuna) (4.10.1) Requirement already satisfied: Mako in /usr/local/lib/python3.7/dist-packages (from alembic-&gt;optuna) (1.1.6) Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from alembic-&gt;optuna) (5.4.0) Requirement already satisfied: pbr!=2.1.0,&gt;=2.0.0 in /usr/local/lib/python3.7/dist-packages (from cliff-&gt;optuna) (5.8.0) Requirement already satisfied: stevedore&gt;=2.0.1 in /usr/local/lib/python3.7/dist-packages (from cliff-&gt;optuna) (3.5.0) Requirement already satisfied: cmd2&gt;=1.0.0 in /usr/local/lib/python3.7/dist-packages (from cliff-&gt;optuna) (2.3.3) Requirement already satisfied: autopage&gt;=0.4.0 in /usr/local/lib/python3.7/dist-packages (from cliff-&gt;optuna) (0.5.0) Requirement already satisfied: PrettyTable&gt;=0.7.2 in /usr/local/lib/python3.7/dist-packages (from cliff-&gt;optuna) (3.0.0) Requirement already satisfied: attrs&gt;=16.3.0 in /usr/local/lib/python3.7/dist-packages (from cmd2&gt;=1.0.0-&gt;cliff-&gt;optuna) (21.4.0) Requirement already satisfied: pyperclip&gt;=1.6 in /usr/local/lib/python3.7/dist-packages (from cmd2&gt;=1.0.0-&gt;cliff-&gt;optuna) (1.8.2) Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from cmd2&gt;=1.0.0-&gt;cliff-&gt;optuna) (3.10.0.2) Requirement already satisfied: wcwidth&gt;=0.1.7 in /usr/local/lib/python3.7/dist-packages (from cmd2&gt;=1.0.0-&gt;cliff-&gt;optuna) (0.2.5) Requirement already satisfied: zipp&gt;=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata-&gt;sqlalchemy&gt;=1.1.0-&gt;optuna) (3.7.0) Requirement already satisfied: MarkupSafe&gt;=0.9.2 in /usr/local/lib/python3.7/dist-packages (from Mako-&gt;alembic-&gt;optuna) (2.0.1) . import pandas as pd import numpy as np from catboost import CatBoostRegressor from tqdm import tqdm from sklearn.model_selection import StratifiedKFold,train_test_split from sklearn.metrics import mean_squared_error import random import optuna from optuna.samplers import TPESampler . path=&#39;/content/drive/MyDrive/Colab Notebooks/물류유통량예측/&#39; . train = pd.read_csv(path+&#39;train_df.csv&#39;, encoding=&#39;CP949&#39;) test = pd.read_csv(path+&#39;test_df.csv&#39;, encoding=&#39;CP949&#39;) submission = pd.read_csv(path+&#39;sample_submission.csv&#39;) . index : 인덱스 | 송하인_격자공간고유번호 | 수하인 격자공간고유번호 | 택배_카테고리 | 운송장_건수 | . 2. Data Preprocess . train.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 32000 entries, 0 to 31999 Data columns (total 6 columns): # Column Non-Null Count Dtype -- -- 0 index 32000 non-null int64 1 SEND_SPG_INNB 32000 non-null int64 2 REC_SPG_INNB 32000 non-null int64 3 DL_GD_LCLS_NM 32000 non-null object 4 DL_GD_MCLS_NM 32000 non-null object 5 INVC_CONT 32000 non-null int64 dtypes: int64(4), object(2) memory usage: 1.5+ MB . train[&#39;SEND_SPG_INNB&#39;].head(10) . 0 1129000014045300 1 1135000009051200 2 1135000030093100 3 1154500002014200 4 1165000021008300 5 1168000013091300 6 1171000019003100 7 2623000012072300 8 2626000011052400 9 2726000034007300 Name: SEND_SPG_INNB, dtype: int64 . train.describe() . INVC_CONT . count 32000.000000 | . mean 4.767875 | . std 5.752122 | . min 3.000000 | . 25% 3.000000 | . 50% 3.000000 | . 75% 5.000000 | . max 239.000000 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; 송하인 격자공간고유번호와 수하인격자공간고유번호가 연속형 자료로 취급하면 안된다고 생각하였습니다. . 격자공간고유번호의 16자리 중 자릿수마다 담고 있는 정보가 다를 것이라 생각하였고, 데이터를 탐색해본 결과 1 ~ 5자리, 6 ~ 9자리, 10, 11 ~ 16자리가 가지고 있는 정보가 다를 것이라 생각했습니다. . 그리고 송하인 격자공간고유번호의 unique 수는 꽤 되지만 수하인 격자공간고유번호의 unique 수는 얼마 되지 않았습니다. . 따라서 송하인 격자공간고유번호는 1 ~ 5, 6 ~ 9, 10, 11 ~ 16자릿수로 나누고 수하인 격자공간고유번호는 자릿수 별로 변수를 생성하였습니다. . 총 22개의 설명변수로 이루어진 데이터로 변환하였습니다. . def numround(number, digit): num=[] while(number!=0): num.append(number % 10) number = number //10 return int(num[-digit]) . for i in tqdm(range(16)): train[f&#39;SEND_SPG_INNB_{i+1}&#39;] = 0 train[f&#39;REC_SPG_INNB_{i+1}&#39;] = 0 test[f&#39;SEND_SPG_INNB_{i+1}&#39;] = 0 test[f&#39;REC_SPG_INNB_{i+1}&#39;] = 0 for j in range(train.shape[0]): train.loc[j,f&#39;SEND_SPG_INNB_{i+1}&#39;]=numround(train.loc[j,&#39;SEND_SPG_INNB&#39;],i+1) train.loc[j,f&#39;REC_SPG_INNB_{i+1}&#39;]=numround(train.loc[j,&#39;REC_SPG_INNB&#39;],i+1) for j in range(test.shape[0]): test.loc[j,f&#39;SEND_SPG_INNB_{i+1}&#39;]=numround(test.loc[j,&#39;SEND_SPG_INNB&#39;],i+1) test.loc[j,f&#39;REC_SPG_INNB_{i+1}&#39;]=numround(test.loc[j,&#39;REC_SPG_INNB&#39;],i+1) . 100%|██████████| 16/16 [07:34&lt;00:00, 28.40s/it] . train[&#39;SEND_SPG_INNB_1~5&#39;]=train[&#39;SEND_SPG_INNB_1&#39;]+train[&#39;SEND_SPG_INNB_2&#39;]+train[&#39;SEND_SPG_INNB_3&#39;]+train[&#39;SEND_SPG_INNB_4&#39;]+train[&#39;SEND_SPG_INNB_5&#39;] train[&#39;SEND_SPG_INNB_6~9&#39;]=train[&#39;SEND_SPG_INNB_6&#39;]+train[&#39;SEND_SPG_INNB_7&#39;]+train[&#39;SEND_SPG_INNB_8&#39;]+train[&#39;SEND_SPG_INNB_9&#39;] train[&#39;SEND_SPG_INNB_10&#39;]=train[&#39;SEND_SPG_INNB_10&#39;] train[&#39;SEND_SPG_INNB_11~16&#39;]=train[&#39;SEND_SPG_INNB_11&#39;]+train[&#39;SEND_SPG_INNB_12&#39;]+train[&#39;SEND_SPG_INNB_13&#39;]+train[&#39;SEND_SPG_INNB_14&#39;]+train[&#39;SEND_SPG_INNB_15&#39;]+train[&#39;SEND_SPG_INNB_16&#39;] test[&#39;SEND_SPG_INNB_1~5&#39;]=test[&#39;SEND_SPG_INNB_1&#39;]+test[&#39;SEND_SPG_INNB_2&#39;]+test[&#39;SEND_SPG_INNB_3&#39;]+test[&#39;SEND_SPG_INNB_4&#39;]+test[&#39;SEND_SPG_INNB_5&#39;] test[&#39;SEND_SPG_INNB_6~9&#39;]=test[&#39;SEND_SPG_INNB_6&#39;]+test[&#39;SEND_SPG_INNB_7&#39;]+test[&#39;SEND_SPG_INNB_8&#39;]+test[&#39;SEND_SPG_INNB_9&#39;] test[&#39;SEND_SPG_INNB_10&#39;]=test[&#39;SEND_SPG_INNB_10&#39;] test[&#39;SEND_SPG_INNB_11~16&#39;]=test[&#39;SEND_SPG_INNB_11&#39;]+test[&#39;SEND_SPG_INNB_12&#39;]+test[&#39;SEND_SPG_INNB_13&#39;]+test[&#39;SEND_SPG_INNB_14&#39;]+test[&#39;SEND_SPG_INNB_15&#39;]+test[&#39;SEND_SPG_INNB_16&#39;] . train.index=train[&#39;index&#39;] test.index=test[&#39;index&#39;] train.drop([&#39;REC_SPG_INNB&#39;,&#39;SEND_SPG_INNB&#39;,&#39;SEND_SPG_INNB_1&#39;,&#39;SEND_SPG_INNB_2&#39;,&#39;SEND_SPG_INNB_3&#39;,&#39;SEND_SPG_INNB_4&#39;,&#39;SEND_SPG_INNB_5&#39;,&#39;SEND_SPG_INNB_6&#39;,&#39;SEND_SPG_INNB_7&#39;, &#39;SEND_SPG_INNB_8&#39;,&#39;SEND_SPG_INNB_9&#39;,&#39;SEND_SPG_INNB_11&#39;,&#39;SEND_SPG_INNB_12&#39;,&#39;SEND_SPG_INNB_13&#39;,&#39;SEND_SPG_INNB_14&#39;,&#39;SEND_SPG_INNB_15&#39;,&#39;SEND_SPG_INNB_16&#39;,&#39;index&#39;],axis=1,inplace=True) test.drop([&#39;REC_SPG_INNB&#39;,&#39;SEND_SPG_INNB&#39;,&#39;SEND_SPG_INNB_1&#39;,&#39;SEND_SPG_INNB_2&#39;,&#39;SEND_SPG_INNB_3&#39;,&#39;SEND_SPG_INNB_4&#39;,&#39;SEND_SPG_INNB_5&#39;,&#39;SEND_SPG_INNB_6&#39;,&#39;SEND_SPG_INNB_7&#39;, &#39;SEND_SPG_INNB_8&#39;,&#39;SEND_SPG_INNB_9&#39;,&#39;SEND_SPG_INNB_11&#39;,&#39;SEND_SPG_INNB_12&#39;,&#39;SEND_SPG_INNB_13&#39;,&#39;SEND_SPG_INNB_14&#39;,&#39;SEND_SPG_INNB_15&#39;,&#39;SEND_SPG_INNB_16&#39;,&#39;index&#39;],axis=1,inplace=True) . for col in test.columns: train[col]=train[col].astype(&#39;category&#39;) test[col]=test[col].astype(&#39;category&#39;) . X = train.drop([&#39;INVC_CONT&#39;],axis=1) y = train[&#39;INVC_CONT&#39;] X_test = test.copy() . X.head() . DL_GD_LCLS_NM DL_GD_MCLS_NM REC_SPG_INNB_1 REC_SPG_INNB_2 REC_SPG_INNB_3 REC_SPG_INNB_4 REC_SPG_INNB_5 REC_SPG_INNB_6 REC_SPG_INNB_7 REC_SPG_INNB_8 REC_SPG_INNB_9 SEND_SPG_INNB_10 REC_SPG_INNB_10 REC_SPG_INNB_11 REC_SPG_INNB_12 REC_SPG_INNB_13 REC_SPG_INNB_14 REC_SPG_INNB_15 REC_SPG_INNB_16 SEND_SPG_INNB_1~5 SEND_SPG_INNB_6~9 SEND_SPG_INNB_11~16 . index . 0 패션의류 | 상의 | 5 | 0 | 1 | 1 | 0 | 0 | 0 | 2 | 2 | 4 | 0 | 0 | 4 | 6 | 3 | 0 | 0 | 13 | 1 | 12 | . 1 생활/건강 | 반려동물 | 5 | 0 | 1 | 1 | 0 | 0 | 0 | 1 | 7 | 9 | 8 | 0 | 3 | 7 | 3 | 0 | 0 | 10 | 0 | 8 | . 2 패션의류 | 기타패션의류 | 5 | 0 | 1 | 1 | 0 | 0 | 0 | 2 | 6 | 0 | 5 | 0 | 9 | 1 | 4 | 0 | 0 | 10 | 3 | 13 | . 3 식품 | 농산물 | 5 | 0 | 1 | 1 | 0 | 0 | 0 | 3 | 1 | 2 | 5 | 0 | 8 | 7 | 4 | 0 | 0 | 16 | 0 | 7 | . 4 식품 | 가공식품 | 5 | 0 | 1 | 1 | 0 | 0 | 0 | 1 | 7 | 1 | 7 | 0 | 5 | 1 | 2 | 0 | 0 | 13 | 2 | 11 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; y.head() . index 0 3 1 3 2 9 3 10 4 3 Name: INVC_CONT, dtype: int64 . X_test.head() . DL_GD_LCLS_NM DL_GD_MCLS_NM REC_SPG_INNB_1 REC_SPG_INNB_2 REC_SPG_INNB_3 REC_SPG_INNB_4 REC_SPG_INNB_5 REC_SPG_INNB_6 REC_SPG_INNB_7 REC_SPG_INNB_8 REC_SPG_INNB_9 SEND_SPG_INNB_10 REC_SPG_INNB_10 REC_SPG_INNB_11 REC_SPG_INNB_12 REC_SPG_INNB_13 REC_SPG_INNB_14 REC_SPG_INNB_15 REC_SPG_INNB_16 SEND_SPG_INNB_1~5 SEND_SPG_INNB_6~9 SEND_SPG_INNB_11~16 . index . 32000 식품 | 농산물 | 1 | 1 | 6 | 5 | 0 | 0 | 0 | 0 | 2 | 3 | 1 | 0 | 9 | 7 | 2 | 0 | 0 | 9 | 4 | 14 | . 32001 식품 | 농산물 | 1 | 1 | 5 | 4 | 5 | 0 | 0 | 0 | 0 | 4 | 2 | 0 | 6 | 6 | 4 | 0 | 0 | 9 | 4 | 8 | . 32002 식품 | 농산물 | 4 | 1 | 3 | 9 | 0 | 0 | 0 | 1 | 0 | 5 | 2 | 0 | 1 | 3 | 2 | 0 | 0 | 9 | 2 | 5 | . 32003 식품 | 농산물 | 4 | 2 | 2 | 1 | 0 | 0 | 0 | 0 | 4 | 5 | 0 | 0 | 9 | 3 | 4 | 0 | 0 | 9 | 2 | 5 | . 32004 식품 | 농산물 | 2 | 7 | 2 | 6 | 0 | 0 | 0 | 0 | 0 | 8 | 4 | 0 | 1 | 7 | 1 | 0 | 0 | 9 | 8 | 6 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; train.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Int64Index: 32000 entries, 0 to 31999 Data columns (total 23 columns): # Column Non-Null Count Dtype -- -- 0 DL_GD_LCLS_NM 32000 non-null category 1 DL_GD_MCLS_NM 32000 non-null category 2 INVC_CONT 32000 non-null int64 3 REC_SPG_INNB_1 32000 non-null category 4 REC_SPG_INNB_2 32000 non-null category 5 REC_SPG_INNB_3 32000 non-null category 6 REC_SPG_INNB_4 32000 non-null category 7 REC_SPG_INNB_5 32000 non-null category 8 REC_SPG_INNB_6 32000 non-null category 9 REC_SPG_INNB_7 32000 non-null category 10 REC_SPG_INNB_8 32000 non-null category 11 REC_SPG_INNB_9 32000 non-null category 12 SEND_SPG_INNB_10 32000 non-null category 13 REC_SPG_INNB_10 32000 non-null category 14 REC_SPG_INNB_11 32000 non-null category 15 REC_SPG_INNB_12 32000 non-null category 16 REC_SPG_INNB_13 32000 non-null category 17 REC_SPG_INNB_14 32000 non-null category 18 REC_SPG_INNB_15 32000 non-null category 19 REC_SPG_INNB_16 32000 non-null category 20 SEND_SPG_INNB_1~5 32000 non-null category 21 SEND_SPG_INNB_6~9 32000 non-null category 22 SEND_SPG_INNB_11~16 32000 non-null category dtypes: category(22), int64(1) memory usage: 1.2 MB . 3.&#47784;&#45944;&#47553; . Optuna로 Random Search를 통해 Catboost 최적의 파라미터를 사용하였습니다. objective 함수의 param에 파라미터를 넣고, 구간을 넣으면 랜덤한 값으로 학습되며 rmse값이 반환되는 함수입니다. &quot;trial&quot;에 반복 횟수를 작성하면 됩니다. . Catboost 특성상 학습이 오래 걸리기 때문에 최적의 파라미터를 찾아 cat_param로 정의하였습니다. . (아래코드는 AIBoo님의 신용카드 사용자 연체 예측 AI 경진대회 [Private 8위 0.66203] | TYKIM | Catboost 코드를 참고하여 수정하였습니다.) . # param = { # &quot;random_state&quot;:42, # &#39;learning_rate&#39; : trial.suggest_loguniform(&#39;learning_rate&#39;, 0.01, 0.05), # &#39;bagging_temperature&#39; :trial.suggest_loguniform(&#39;bagging_temperature&#39;, 0.01, 100.00), # &quot;n_estimators&quot;:trial.suggest_int(&quot;n_estimators&quot;, 500, 5000), # &quot;max_depth&quot;:trial.suggest_int(&quot;max_depth&quot;, 4, 16), # &#39;random_strength&#39; :trial.suggest_int(&#39;random_strength&#39;, 0, 100), # &quot;colsample_bylevel&quot;:trial.suggest_float(&quot;colsample_bylevel&quot;, 0.4, 1.0), # &quot;l2_leaf_reg&quot;:trial.suggest_float(&quot;l2_leaf_reg&quot;,1e-8,3e-5), # &quot;min_child_samples&quot;: trial.suggest_int(&quot;min_child_samples&quot;, 5, 100), # &quot;max_bin&quot;: trial.suggest_int(&quot;max_bin&quot;, 200, 500), # &#39;od_type&#39;: trial.suggest_categorical(&#39;od_type&#39;, [&#39;IncToDec&#39;, &#39;Iter&#39;]), # } # X_train, X_valid, y_train, y_valid = train_test_split(X,y,test_size=0.2) # cat_features = range(X_test.shape[1]) # cat = CatBoostRegressor(**param) # cat.fit(X_train, y_train, # eval_set=[(X_train, y_train), (X_valid,y_valid)], # early_stopping_rounds=35,cat_features=cat_features, # verbose=100) # cat_pred = cat.predict(X_valid) # rmse = np.sqrt(mean_squared_error(y_valid, cat_pred)) # return rmse . # study = optuna.create_study( # study_name = &#39;cat_parameter_opt&#39;, # direction = &#39;minimize&#39;, # sampler = sampler, # ) # study.optimize(objective, n_trials=10) # print(&quot;Best Score:&quot;,study.best_value) # print(&quot;Best trial&quot;,study.best_trial.params) . cat_param={&#39;learning_rate&#39;: 0.018272261776066247, &#39;bagging_temperature&#39;: 63.512210106407046, &#39;n_estimators&#39;: 3794, &#39;max_depth&#39;: 11, &#39;random_strength&#39;: 15, &#39;colsample_bylevel&#39;: 0.49359671220172163, &#39;l2_leaf_reg&#39;: 1.7519275289243016e-06, &#39;min_child_samples&#39;: 88, &#39;max_bin&#39;: 380, &#39;od_type&#39;: &#39;IncToDec&#39; } . skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42) folds = [] for train_idx, valid_idx in skf.split(train, train[&#39;INVC_CONT&#39;]): folds.append((train_idx,valid_idx)) . /usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:680: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=10. UserWarning, . train 데이터를 K-Fold 하여 각 Fold의 학습 값을 가지고 test 예측값을 구한 후 평균을 구하였습니다. . skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42) folds = [] for train_idx, valid_idx in skf.split(train, train[&#39;INVC_CONT&#39;]): folds.append((train_idx,valid_idx)) random.seed(42) cat_models={} cat_features =range(X_test.shape[1]) for fold in range(10): print(f&#39;===================================={fold+1}============================================&#39;) train_idx, valid_idx = folds[fold] X_train = train.drop([&#39;INVC_CONT&#39;],axis=1).iloc[train_idx] X_valid = train.drop([&#39;INVC_CONT&#39;],axis=1).iloc[valid_idx] y_train = train[&#39;INVC_CONT&#39;][train_idx].values y_valid = train[&#39;INVC_CONT&#39;][valid_idx].values cat = CatBoostRegressor(**cat_param) cat.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_valid,y_valid)], early_stopping_rounds=35,cat_features=cat_features, verbose=100) cat_models[fold] = cat print(f&#39;================================================================================ n n&#39;) . ====================================1============================================ . /usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:680: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=10. UserWarning, . 0: learn: 5.6240726 test: 5.6244095 test1: 6.7835557 best: 6.7835557 (0) total: 498ms remaining: 31m 30s 100: learn: 5.4960942 test: 5.5301616 test1: 6.6675145 best: 6.6675145 (100) total: 24.7s remaining: 15m 2s 200: learn: 5.4405946 test: 5.4798163 test1: 6.6059558 best: 6.6059558 (198) total: 36.2s remaining: 10m 47s 300: learn: 5.4140537 test: 5.4605846 test1: 6.5849293 best: 6.5849293 (300) total: 47.3s remaining: 9m 9s 400: learn: 5.3916725 test: 5.4422328 test1: 6.5653535 best: 6.5653535 (400) total: 57.7s remaining: 8m 7s 500: learn: 5.3395210 test: 5.3956074 test1: 6.5185137 best: 6.5185137 (500) total: 1m 8s remaining: 7m 30s 600: learn: 5.2901206 test: 5.3652920 test1: 6.4707527 best: 6.4707527 (600) total: 1m 19s remaining: 7m 4s 700: learn: 5.0282278 test: 5.2082914 test1: 6.2761017 best: 6.2761017 (700) total: 1m 40s remaining: 7m 23s 800: learn: 4.7363885 test: 5.0484783 test1: 6.1402382 best: 6.1401487 (798) total: 2m 11s remaining: 8m 11s 900: learn: 4.5672517 test: 4.9696530 test1: 6.0793294 best: 6.0788567 (881) total: 2m 42s remaining: 8m 41s 1000: learn: 4.4406792 test: 4.9196033 test1: 6.0718066 best: 6.0716713 (990) total: 3m 12s remaining: 8m 58s Stopped by overfitting detector (35 iterations wait) bestTest = 6.068832186 bestIteration = 1044 Shrink model to first 1045 iterations. ================================================================================ ====================================2============================================ 0: learn: 5.7998015 test: 5.7997917 test1: 5.2928846 best: 5.2928846 (0) total: 65.4ms remaining: 4m 8s 100: learn: 5.6398961 test: 5.7026359 test1: 5.2344849 best: 5.2344849 (100) total: 13.9s remaining: 8m 26s 200: learn: 5.5727985 test: 5.6661956 test1: 5.2212126 best: 5.2212126 (200) total: 24.1s remaining: 7m 10s 300: learn: 5.4880491 test: 5.6112859 test1: 5.2099747 best: 5.2099747 (300) total: 36.2s remaining: 6m 59s 400: learn: 5.4288432 test: 5.5728402 test1: 5.2043505 best: 5.2043505 (400) total: 47.2s remaining: 6m 39s 500: learn: 5.3730200 test: 5.5308387 test1: 5.1973897 best: 5.1973897 (500) total: 57.8s remaining: 6m 19s 600: learn: 5.2682635 test: 5.4467884 test1: 5.1827241 best: 5.1823130 (598) total: 1m 11s remaining: 6m 17s 700: learn: 5.0329989 test: 5.2904110 test1: 5.1763991 best: 5.1754082 (686) total: 1m 40s remaining: 7m 25s 800: learn: 4.7760561 test: 5.1586431 test1: 5.1691799 best: 5.1689916 (799) total: 2m 17s remaining: 8m 35s 900: learn: 4.5718300 test: 5.0578540 test1: 5.1657950 best: 5.1656201 (881) total: 2m 48s remaining: 9m Stopped by overfitting detector (35 iterations wait) bestTest = 5.163419657 bestIteration = 958 Shrink model to first 959 iterations. ================================================================================ ====================================3============================================ 0: learn: 5.7954233 test: 5.7954087 test1: 5.3358658 best: 5.3358658 (0) total: 61ms remaining: 3m 51s 100: learn: 5.6510996 test: 5.6987296 test1: 5.2296969 best: 5.2296969 (100) total: 11.8s remaining: 7m 12s 200: learn: 5.5891938 test: 5.6632554 test1: 5.1978101 best: 5.1978101 (200) total: 25.6s remaining: 7m 36s 300: learn: 5.5205015 test: 5.6164968 test1: 5.1640026 best: 5.1639988 (294) total: 38.8s remaining: 7m 30s 400: learn: 5.4715385 test: 5.5996822 test1: 5.1517844 best: 5.1517844 (400) total: 51.3s remaining: 7m 14s 500: learn: 5.4126707 test: 5.5432101 test1: 5.1051460 best: 5.1051388 (499) total: 1m 1s remaining: 6m 42s 600: learn: 5.3683705 test: 5.5178964 test1: 5.0840610 best: 5.0840609 (599) total: 1m 11s remaining: 6m 20s 700: learn: 5.1067467 test: 5.3031105 test1: 4.9499417 best: 4.9499417 (700) total: 1m 33s remaining: 6m 51s 800: learn: 4.8336476 test: 5.1569749 test1: 4.8936226 best: 4.8936226 (800) total: 2m 3s remaining: 7m 40s 900: learn: 4.6463427 test: 5.0731393 test1: 4.8750691 best: 4.8749592 (899) total: 2m 33s remaining: 8m 14s 1000: learn: 4.4988646 test: 5.0119748 test1: 4.8621792 best: 4.8619472 (998) total: 3m 4s remaining: 8m 34s Stopped by overfitting detector (35 iterations wait) bestTest = 4.857927859 bestIteration = 1045 Shrink model to first 1046 iterations. ================================================================================ ====================================4============================================ 0: learn: 5.6797253 test: 5.6797053 test1: 6.3581339 best: 6.3581339 (0) total: 61.3ms remaining: 3m 52s 100: learn: 5.5764211 test: 5.5851445 test1: 6.2563079 best: 6.2563079 (100) total: 12.6s remaining: 7m 42s 200: learn: 5.5165005 test: 5.5369824 test1: 6.1989641 best: 6.1989641 (200) total: 25.9s remaining: 7m 42s 300: learn: 5.4783855 test: 5.4982155 test1: 6.1413561 best: 6.1413522 (295) total: 36.6s remaining: 7m 4s 400: learn: 5.4373504 test: 5.4615927 test1: 6.0793381 best: 6.0793381 (400) total: 45.6s remaining: 6m 25s 500: learn: 5.3895524 test: 5.4217469 test1: 6.0430049 best: 6.0429846 (497) total: 58.1s remaining: 6m 21s 600: learn: 5.3229585 test: 5.3672285 test1: 5.9601014 best: 5.9601014 (600) total: 1m 12s remaining: 6m 26s 700: learn: 5.0641650 test: 5.1685397 test1: 5.6283284 best: 5.6282761 (699) total: 1m 34s remaining: 6m 56s 800: learn: 4.8122561 test: 5.0117926 test1: 5.4345803 best: 5.4345803 (800) total: 2m 4s remaining: 7m 44s 900: learn: 4.5865353 test: 4.9277124 test1: 5.3770698 best: 5.3770698 (900) total: 2m 34s remaining: 8m 16s Stopped by overfitting detector (35 iterations wait) bestTest = 5.367815232 bestIteration = 948 Shrink model to first 949 iterations. ================================================================================ ====================================5============================================ 0: learn: 5.8127429 test: 5.8127445 test1: 5.1631856 best: 5.1631856 (0) total: 59.8ms remaining: 3m 46s 100: learn: 5.7174157 test: 5.7421257 test1: 5.1135334 best: 5.1135334 (100) total: 12.1s remaining: 7m 21s 200: learn: 5.6513823 test: 5.6897737 test1: 5.0990861 best: 5.0990861 (200) total: 23.2s remaining: 6m 54s 300: learn: 5.5762771 test: 5.6375500 test1: 5.0975761 best: 5.0945176 (267) total: 34.8s remaining: 6m 43s Stopped by overfitting detector (35 iterations wait) bestTest = 5.094517591 bestIteration = 267 Shrink model to first 268 iterations. ================================================================================ ====================================6============================================ 0: learn: 5.8718289 test: 5.8718225 test1: 4.5217283 best: 4.5217283 (0) total: 63.6ms remaining: 4m 1s 100: learn: 5.7675896 test: 5.7717752 test1: 4.4536227 best: 4.4536227 (100) total: 12.4s remaining: 7m 33s 200: learn: 5.7072065 test: 5.7210221 test1: 4.4333057 best: 4.4333057 (200) total: 24.5s remaining: 7m 18s 300: learn: 5.6531517 test: 5.6372884 test1: 4.4228929 best: 4.4228700 (299) total: 36.1s remaining: 6m 59s 400: learn: 5.5785283 test: 5.5630332 test1: 4.4138547 best: 4.4138547 (400) total: 48.7s remaining: 6m 52s 500: learn: 5.5368764 test: 5.5235730 test1: 4.4087818 best: 4.4087818 (500) total: 1m remaining: 6m 39s 600: learn: 5.4462576 test: 5.4463752 test1: 4.4044104 best: 4.4044104 (600) total: 1m 14s remaining: 6m 35s 700: learn: 5.1627216 test: 5.2381077 test1: 4.3989134 best: 4.3981901 (695) total: 1m 33s remaining: 6m 50s 800: learn: 4.8764121 test: 5.1291066 test1: 4.3813348 best: 4.3810835 (798) total: 2m 2s remaining: 7m 39s Stopped by overfitting detector (35 iterations wait) bestTest = 4.375711979 bestIteration = 855 Shrink model to first 856 iterations. ================================================================================ ====================================7============================================ 0: learn: 5.5896379 test: 5.5896739 test1: 7.0398490 best: 7.0398490 (0) total: 62.8ms remaining: 3m 58s 100: learn: 5.4895956 test: 5.5207092 test1: 6.9712621 best: 6.9712621 (100) total: 11.6s remaining: 7m 2s 200: learn: 5.4368356 test: 5.4905300 test1: 6.9470753 best: 6.9470753 (200) total: 22.4s remaining: 6m 41s 300: learn: 5.4108806 test: 5.4764698 test1: 6.9373469 best: 6.9373353 (298) total: 32.4s remaining: 6m 16s 400: learn: 5.3796268 test: 5.4556132 test1: 6.8839617 best: 6.8839617 (400) total: 43.7s remaining: 6m 9s 500: learn: 5.3317076 test: 5.4298767 test1: 6.8320513 best: 6.8320250 (497) total: 55.8s remaining: 6m 6s 600: learn: 5.2595996 test: 5.3902005 test1: 6.7159785 best: 6.7159785 (600) total: 1m 10s remaining: 6m 12s 700: learn: 5.1266059 test: 5.3415476 test1: 6.6241471 best: 6.6238443 (696) total: 1m 30s remaining: 6m 38s 800: learn: 4.8878021 test: 5.2295321 test1: 6.3647524 best: 6.3647524 (800) total: 2m remaining: 7m 30s 900: learn: 4.6774448 test: 5.1363369 test1: 6.2224111 best: 6.2201793 (898) total: 2m 30s remaining: 8m 4s 1000: learn: 4.5147885 test: 5.0836647 test1: 6.1634589 best: 6.1634589 (1000) total: 3m 1s remaining: 8m 25s 1100: learn: 4.3961307 test: 5.0453149 test1: 6.1036841 best: 6.1036841 (1100) total: 3m 31s remaining: 8m 38s 1200: learn: 4.2735652 test: 5.0100648 test1: 6.0608318 best: 6.0608318 (1200) total: 4m 2s remaining: 8m 43s 1300: learn: 4.1715964 test: 4.9854930 test1: 6.0428858 best: 6.0428858 (1300) total: 4m 32s remaining: 8m 42s 1400: learn: 4.0832148 test: 4.9626169 test1: 6.0374177 best: 6.0363832 (1389) total: 5m 3s remaining: 8m 38s Stopped by overfitting detector (35 iterations wait) bestTest = 6.036383207 bestIteration = 1389 Shrink model to first 1390 iterations. ================================================================================ ====================================8============================================ 0: learn: 5.7761549 test: 5.7761372 test1: 5.5206589 best: 5.5206589 (0) total: 60.1ms remaining: 3m 48s 100: learn: 5.6259120 test: 5.6818344 test1: 5.4627034 best: 5.4627034 (100) total: 12.7s remaining: 7m 45s 200: learn: 5.5214357 test: 5.6009615 test1: 5.4442310 best: 5.4442310 (200) total: 24.1s remaining: 7m 10s 300: learn: 5.4240376 test: 5.4945688 test1: 5.4326722 best: 5.4326722 (299) total: 35.9s remaining: 6m 56s 400: learn: 5.3796432 test: 5.4740359 test1: 5.4293208 best: 5.4293208 (400) total: 45.8s remaining: 6m 27s 500: learn: 5.3351087 test: 5.4464290 test1: 5.4225107 best: 5.4225107 (500) total: 57.3s remaining: 6m 16s Stopped by overfitting detector (35 iterations wait) bestTest = 5.42192434 bestIteration = 518 Shrink model to first 519 iterations. ================================================================================ ====================================9============================================ 0: learn: 5.8113326 test: 5.8113592 test1: 5.1774233 best: 5.1774233 (0) total: 66.7ms remaining: 4m 12s 100: learn: 5.6893174 test: 5.7115796 test1: 5.0706084 best: 5.0706084 (100) total: 13.9s remaining: 8m 28s 200: learn: 5.6380443 test: 5.6733641 test1: 5.0465860 best: 5.0465860 (200) total: 23s remaining: 6m 51s 300: learn: 5.5796243 test: 5.6231843 test1: 5.0204694 best: 5.0201020 (293) total: 33.9s remaining: 6m 33s 400: learn: 5.5339410 test: 5.6036245 test1: 5.0077692 best: 5.0077644 (399) total: 45.4s remaining: 6m 24s 500: learn: 5.4983008 test: 5.5786004 test1: 4.9919552 best: 4.9919552 (500) total: 58.6s remaining: 6m 25s 600: learn: 5.4141082 test: 5.5007961 test1: 4.9789318 best: 4.9789318 (600) total: 1m 12s remaining: 6m 24s 700: learn: 5.1832211 test: 5.3333448 test1: 4.9449670 best: 4.9445323 (696) total: 1m 34s remaining: 6m 57s 800: learn: 4.9239177 test: 5.1933113 test1: 4.9167434 best: 4.9167405 (799) total: 2m 3s remaining: 7m 42s 900: learn: 4.7601542 test: 5.1326880 test1: 4.9092309 best: 4.9091543 (899) total: 2m 34s remaining: 8m 15s 1000: learn: 4.6155194 test: 5.0813784 test1: 4.9029460 best: 4.9019464 (991) total: 3m 4s remaining: 8m 35s Stopped by overfitting detector (35 iterations wait) bestTest = 4.899096889 bestIteration = 1043 Shrink model to first 1044 iterations. ================================================================================ ====================================10============================================ 0: learn: 5.7433502 test: 5.7433425 test1: 5.8209412 best: 5.8209412 (0) total: 60ms remaining: 3m 47s 100: learn: 5.5948456 test: 5.6177444 test1: 5.7560805 best: 5.7560805 (100) total: 12.4s remaining: 7m 33s 200: learn: 5.5226384 test: 5.5328046 test1: 5.7386751 best: 5.7386742 (199) total: 23.8s remaining: 7m 5s 300: learn: 5.4668698 test: 5.4790270 test1: 5.7304271 best: 5.7304151 (299) total: 34.9s remaining: 6m 44s 400: learn: 5.4168370 test: 5.4306267 test1: 5.7260604 best: 5.7253147 (390) total: 45.9s remaining: 6m 28s Stopped by overfitting detector (35 iterations wait) bestTest = 5.725314711 bestIteration = 390 Shrink model to first 391 iterations. ================================================================================ . submission.loc[:,&#39;INVC_CONT&#39;]=0 for fold in range(10): submission.loc[:,&#39;INVC_CONT&#39;] += cat_models[fold].predict(test)/10 . train을 K-fold한 값의 평균을 구하다 보니 예측값의 극단값이 작아질 수 밖에 없습니다. . 따라서 INVC_CONT가 30 이상인 값에 적당한 값을 곱하여서 조정하였습니다. . submission.loc[submission.INVC_CONT&gt;30,&#39;INVC_CONT&#39;]=submission.loc[submission.INVC_CONT&gt;30,&#39;INVC_CONT&#39;]*4.8 .",
            "url": "https://shw9807.github.io/shw9807blog/%EB%8D%B0%EC%9D%B4%EC%BD%98/2022/02/03/%EB%AC%BC%EB%A5%98%EC%9C%A0%ED%86%B5%EB%9F%89_%EC%98%88%EC%B8%A1.html",
            "relUrl": "/%EB%8D%B0%EC%9D%B4%EC%BD%98/2022/02/03/%EB%AC%BC%EB%A5%98%EC%9C%A0%ED%86%B5%EB%9F%89_%EC%98%88%EC%B8%A1.html",
            "date": " • Feb 3, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "[Colab] 타이타닉 데이터 분석",
            "content": "&#53440;&#51060;&#53440;&#45769; &#45936;&#51060;&#53552; &#48516;&#49437; . !sudo apt-get install -y fonts-nanum !sudo fc-cache -fv !rm ~/.cache/matplotlib -rf . Reading package lists... Done Building dependency tree Reading state information... Done The following NEW packages will be installed: fonts-nanum 0 upgraded, 1 newly installed, 0 to remove and 37 not upgraded. Need to get 9,604 kB of archives. After this operation, 29.5 MB of additional disk space will be used. Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 fonts-nanum all 20170925-1 [9,604 kB] Fetched 9,604 kB in 2s (6,127 kB/s) debconf: unable to initialize frontend: Dialog debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, &lt;&gt; line 1.) debconf: falling back to frontend: Readline debconf: unable to initialize frontend: Readline debconf: (This frontend requires a controlling tty.) debconf: falling back to frontend: Teletype dpkg-preconfigure: unable to re-open stdin: Selecting previously unselected package fonts-nanum. (Reading database ... 155229 files and directories currently installed.) Preparing to unpack .../fonts-nanum_20170925-1_all.deb ... Unpacking fonts-nanum (20170925-1) ... Setting up fonts-nanum (20170925-1) ... Processing triggers for fontconfig (2.12.6-0ubuntu2) ... /usr/share/fonts: caching, new cache contents: 0 fonts, 1 dirs /usr/share/fonts/truetype: caching, new cache contents: 0 fonts, 3 dirs /usr/share/fonts/truetype/humor-sans: caching, new cache contents: 1 fonts, 0 dirs /usr/share/fonts/truetype/liberation: caching, new cache contents: 16 fonts, 0 dirs /usr/share/fonts/truetype/nanum: caching, new cache contents: 10 fonts, 0 dirs /usr/local/share/fonts: caching, new cache contents: 0 fonts, 0 dirs /root/.local/share/fonts: skipping, no such directory /root/.fonts: skipping, no such directory /var/cache/fontconfig: cleaning cache directory /root/.cache/fontconfig: not cleaning non-existent cache directory /root/.fontconfig: not cleaning non-existent cache directory fc-cache: succeeded . import warnings; warnings.filterwarnings(&#39;ignore&#39;) import matplotlib.pyplot as plt import matplotlib as mpl import matplotlib.font_manager as fm import numpy as npn import pandas as pd import seaborn as sns plt.rc(&#39;font&#39;, family=&#39;NanumBarunGothic&#39;) . !unzip -uq &quot;/content/drive/MyDrive/Colab Notebooks/타이타닉/타이타닉.zip&quot; -d &quot;/content/drive/MyDrive/Colab Notebooks/타이타닉&quot; . test = pd.read_csv(&quot;/content/drive/MyDrive/Colab Notebooks/타이타닉/test.csv&quot;) train = pd.read_csv(&quot;/content/drive/MyDrive/Colab Notebooks/타이타닉/train.csv&quot;) submission = pd.read_csv(&quot;/content/drive/MyDrive/Colab Notebooks/타이타닉/submission.csv&quot;) . train.head() . PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked . 0 1 | 0 | 3 | Braund, Mr. Owen Harris | male | 22.0 | 1 | 0 | A/5 21171 | 7.2500 | NaN | S | . 1 2 | 1 | 1 | Cumings, Mrs. John Bradley (Florence Briggs Th... | female | 38.0 | 1 | 0 | PC 17599 | 71.2833 | C85 | C | . 2 3 | 1 | 3 | Heikkinen, Miss. Laina | female | 26.0 | 0 | 0 | STON/O2. 3101282 | 7.9250 | NaN | S | . 3 4 | 1 | 1 | Futrelle, Mrs. Jacques Heath (Lily May Peel) | female | 35.0 | 1 | 0 | 113803 | 53.1000 | C123 | S | . 4 5 | 0 | 3 | Allen, Mr. William Henry | male | 35.0 | 0 | 0 | 373450 | 8.0500 | NaN | S | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; PassengerID : 탑승객 고유 아이디 Survival : 탑승객 생존 유무 (0: 사망, 1: 생존) Pclass : 등실의 등급 Name : 이름 Sex : 성별 Age : 나이 Sibsp : 함께 탐승한 형제자매, 아내, 남편의 수 Parch : 함께 탐승한 부모, 자식의 수 Ticket :티켓 번호 Fare : 티켓의 요금 Cabin : 객실번호 Embarked : 배에 탑승한 항구 이름 ( C = Cherbourn, Q = Queenstown, S = Southampton) . 일단 객실 등급과 성별, 나이, 티켓요금, 탑승한 항구 이름 정도가 영향이 있지 않았을까 싶다. . train.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 891 entries, 0 to 890 Data columns (total 12 columns): # Column Non-Null Count Dtype -- -- 0 PassengerId 891 non-null int64 1 Survived 891 non-null int64 2 Pclass 891 non-null int64 3 Name 891 non-null object 4 Sex 891 non-null object 5 Age 714 non-null float64 6 SibSp 891 non-null int64 7 Parch 891 non-null int64 8 Ticket 891 non-null object 9 Fare 891 non-null float64 10 Cabin 204 non-null object 11 Embarked 889 non-null object dtypes: float64(2), int64(5), object(5) memory usage: 83.7+ KB . 객실번호와 나이를 제외한 변수들은 결측치를 가지지 않음을 확인할 수 있다. . train.describe() . PassengerId Survived Pclass Age SibSp Parch Fare . count 891.000000 | 891.000000 | 891.000000 | 714.000000 | 891.000000 | 891.000000 | 891.000000 | . mean 446.000000 | 0.383838 | 2.308642 | 29.699118 | 0.523008 | 0.381594 | 32.204208 | . std 257.353842 | 0.486592 | 0.836071 | 14.526497 | 1.102743 | 0.806057 | 49.693429 | . min 1.000000 | 0.000000 | 1.000000 | 0.420000 | 0.000000 | 0.000000 | 0.000000 | . 25% 223.500000 | 0.000000 | 2.000000 | 20.125000 | 0.000000 | 0.000000 | 7.910400 | . 50% 446.000000 | 0.000000 | 3.000000 | 28.000000 | 0.000000 | 0.000000 | 14.454200 | . 75% 668.500000 | 1.000000 | 3.000000 | 38.000000 | 1.000000 | 0.000000 | 31.000000 | . max 891.000000 | 1.000000 | 3.000000 | 80.000000 | 8.000000 | 6.000000 | 512.329200 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; train.loc[train[&#39;Age&#39;].isna()] . PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked . 5 6 | 0 | 3 | Moran, Mr. James | male | NaN | 0 | 0 | 330877 | 8.4583 | NaN | Q | . 17 18 | 1 | 2 | Williams, Mr. Charles Eugene | male | NaN | 0 | 0 | 244373 | 13.0000 | NaN | S | . 19 20 | 1 | 3 | Masselmani, Mrs. Fatima | female | NaN | 0 | 0 | 2649 | 7.2250 | NaN | C | . 26 27 | 0 | 3 | Emir, Mr. Farred Chehab | male | NaN | 0 | 0 | 2631 | 7.2250 | NaN | C | . 28 29 | 1 | 3 | O&#39;Dwyer, Miss. Ellen &quot;Nellie&quot; | female | NaN | 0 | 0 | 330959 | 7.8792 | NaN | Q | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 859 860 | 0 | 3 | Razi, Mr. Raihed | male | NaN | 0 | 0 | 2629 | 7.2292 | NaN | C | . 863 864 | 0 | 3 | Sage, Miss. Dorothy Edith &quot;Dolly&quot; | female | NaN | 8 | 2 | CA. 2343 | 69.5500 | NaN | S | . 868 869 | 0 | 3 | van Melkebeke, Mr. Philemon | male | NaN | 0 | 0 | 345777 | 9.5000 | NaN | S | . 878 879 | 0 | 3 | Laleff, Mr. Kristo | male | NaN | 0 | 0 | 349217 | 7.8958 | NaN | S | . 888 889 | 0 | 3 | Johnston, Miss. Catherine Helen &quot;Carrie&quot; | female | NaN | 1 | 2 | W./C. 6607 | 23.4500 | NaN | S | . 177 rows × 12 columns . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; train.loc[(train[&#39;Age&#39;].isna()) &amp; (train[&#39;SibSp&#39;] == 0 ) &amp; (train[&#39;Parch&#39;] == 0)] . PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked . 5 6 | 0 | 3 | Moran, Mr. James | male | NaN | 0 | 0 | 330877 | 8.4583 | NaN | Q | . 17 18 | 1 | 2 | Williams, Mr. Charles Eugene | male | NaN | 0 | 0 | 244373 | 13.0000 | NaN | S | . 19 20 | 1 | 3 | Masselmani, Mrs. Fatima | female | NaN | 0 | 0 | 2649 | 7.2250 | NaN | C | . 26 27 | 0 | 3 | Emir, Mr. Farred Chehab | male | NaN | 0 | 0 | 2631 | 7.2250 | NaN | C | . 28 29 | 1 | 3 | O&#39;Dwyer, Miss. Ellen &quot;Nellie&quot; | female | NaN | 0 | 0 | 330959 | 7.8792 | NaN | Q | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 837 838 | 0 | 3 | Sirota, Mr. Maurice | male | NaN | 0 | 0 | 392092 | 8.0500 | NaN | S | . 839 840 | 1 | 1 | Marechal, Mr. Pierre | male | NaN | 0 | 0 | 11774 | 29.7000 | C47 | C | . 859 860 | 0 | 3 | Razi, Mr. Raihed | male | NaN | 0 | 0 | 2629 | 7.2292 | NaN | C | . 868 869 | 0 | 3 | van Melkebeke, Mr. Philemon | male | NaN | 0 | 0 | 345777 | 9.5000 | NaN | S | . 878 879 | 0 | 3 | Laleff, Mr. Kristo | male | NaN | 0 | 0 | 349217 | 7.8958 | NaN | S | . 133 rows × 12 columns . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; train.groupby(&#39;Pclass&#39;)[&#39;Age&#39;].mean() . Pclass 1 38.233441 2 29.877630 3 25.140620 Name: Age, dtype: float64 . plt.figure(figsize=[10,10]) sns.scatterplot(data=train, x=&#39;Pclass&#39;, y = &#39;Age&#39;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f7044f33410&gt; . findfont: Font family [&#39;NanumBarunGothic&#39;] not found. Falling back to DejaVu Sans. . &#44208;&#52769;&#52824; &#52376;&#47532; . 객실등급에 따라서 나이의 평균에 차이가 있는 것으로 보인다. 아무래도 비싼 객실일수록 평균 나이대가 높아지는 경향이 있는 것으로 보인다. 나이의 결측치들을 객실등급별로 평균값으로 채워주면 될 것 같다. . meanage = train.groupby(&#39;Pclass&#39;)[&#39;Age&#39;].mean() . train.loc[(train[&#39;Age&#39;].isna()) &amp; (train[&#39;Pclass&#39;] == 1), &#39;Age&#39;] = meanage[1] train.loc[(train[&#39;Age&#39;].isna()) &amp; (train[&#39;Pclass&#39;] == 2),&#39;Age&#39;] = meanage[2] train.loc[(train[&#39;Age&#39;].isna()) &amp; (train[&#39;Pclass&#39;] == 3),&#39;Age&#39;] = meanage[3] . train.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 891 entries, 0 to 890 Data columns (total 12 columns): # Column Non-Null Count Dtype -- -- 0 PassengerId 891 non-null int64 1 Survived 891 non-null int64 2 Pclass 891 non-null int64 3 Name 891 non-null object 4 Sex 891 non-null object 5 Age 891 non-null float64 6 SibSp 891 non-null int64 7 Parch 891 non-null int64 8 Ticket 891 non-null object 9 Fare 891 non-null float64 10 Cabin 204 non-null object 11 Embarked 889 non-null object dtypes: float64(2), int64(5), object(5) memory usage: 83.7+ KB . Cabin은 결측치도 많고 별로 의미가 없을까 싶다. 변수 자체를 없애는 것이 나아보인다. Embarked는 결측치가 몇 개 존재하지 않는다. 가장 자주 나온 변수로 채워주어도 괜찮을 것으로 보인다. . train[&#39;Embarked&#39;].value_counts() . S 644 C 168 Q 77 Name: Embarked, dtype: int64 . S 가 가장 많이 나타나므로 결측치는 S로 채워주도록 하자. . train[&#39;Embarked&#39;].fillna(value = &#39;S&#39;, inplace = True) . train[&#39;Embarked&#39;].isna().sum() . 0 . 테스트 데이터에도 똑같이 적용해준다. . test.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 418 entries, 0 to 417 Data columns (total 11 columns): # Column Non-Null Count Dtype -- -- 0 PassengerId 418 non-null int64 1 Pclass 418 non-null int64 2 Name 418 non-null object 3 Sex 418 non-null object 4 Age 332 non-null float64 5 SibSp 418 non-null int64 6 Parch 418 non-null int64 7 Ticket 418 non-null object 8 Fare 417 non-null float64 9 Cabin 91 non-null object 10 Embarked 418 non-null object dtypes: float64(2), int64(4), object(5) memory usage: 36.0+ KB . test.loc[(test[&#39;Age&#39;].isna()) &amp; (test[&#39;Pclass&#39;] == 1), &#39;Age&#39;] = meanage[1] test.loc[(test[&#39;Age&#39;].isna()) &amp; (test[&#39;Pclass&#39;] == 2),&#39;Age&#39;] = meanage[2] test.loc[(test[&#39;Age&#39;].isna()) &amp; (test[&#39;Pclass&#39;] == 3),&#39;Age&#39;] = meanage[3] . test.isna().sum() . PassengerId 0 Pclass 0 Name 0 Sex 0 Age 0 SibSp 0 Parch 0 Ticket 0 Fare 1 Cabin 327 Embarked 0 dtype: int64 . 성별이 &#39;male&#39;과 &#39;female&#39;로 설정되어 있는데 0과 1의 값을 가지는 변수로 변환해주자 . train[&#39;Sex&#39;] = train[&#39;Sex&#39;].map({&#39;male&#39;:0, &#39;female&#39;:1}) . test[&#39;Sex&#39;] = test[&#39;Sex&#39;].map({&#39;male&#39;:0, &#39;female&#39;:1}) . &#45936;&#51060;&#53552; &#49884;&#44033;&#54868; . plt.figure(figsize=[10,10]) sns.barplot(data=train, x=&#39;Pclass&#39;, y = &#39;Survived&#39;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f702f48df90&gt; . 확실히 객실의 등급이 높을수록 생존할 확률이 높아지는것을 확인할 수 있다. . plt.figure(figsize=[10,10]) sns.barplot(data=train, x=&#39;Sex&#39;, y = &#39;Survived&#39;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f70268cff50&gt; . 여자가 남자에 비해서 압도적으로 생존확률이 높은 것을 확인할 수 있다. . plt.figure(figsize=[10,10]) sns.barplot(data=train, x=&#39;Embarked&#39;, y = &#39;Survived&#39;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f70260125d0&gt; . sns.lmplot(data=train, x=&#39;Age&#39;, y = &#39;Survived&#39;) . &lt;seaborn.axisgrid.FacetGrid at 0x7f702f3c2e10&gt; . 나이별로 생존률에 차이가 있는지를 어떻게 확인해보려고 했는데 잘 안된다.... 큰차이는 없다..? 나이가 많아질수록 생존확률이 낮아진다? 정도로 볼 수 있을 것 같다. . %matplotlib inline train.hist(bins = 50, figsize = (20,15)) plt.show() . &#48320;&#49688; &#49440;&#53469; &#48143; &#47784;&#45944; &#44396;&#52629; . sklearn.linear_model.LogisticRegression() . 로지스틱 함수를 통해 0과 1사이의 값을 산출하여, 탑승객의 생존 여부를 파악해봅시다. . X_train = train[[&#39;Pclass&#39;, &#39;Age&#39;]] y_train = train[&#39;Survived&#39;] X_test = test[[&#39;Pclass&#39;, &#39;Age&#39;]] . from sklearn.linear_model import LogisticRegression model = LogisticRegression() . model.fit(X_train, y_train) . LogisticRegression() . y_pred = model.predict(X_test) . submission[&#39;Survived&#39;] = y_pred . submission.to_csv(&#39;lr_model_Pclass_Age.csv&#39;, index = False) . sklearn.tree.DecisionTreeClassifier() . 특징변수들로부터 타깃변수를 맞추기 위해 경우를 쪼개나가는 알고리즘입니다. (예) 과일이 사과, 딸기, 포도 중 무엇인지 맞추려합니다 . 주어진 특징은 과일 1개의 가로최대길이, 세로최대길이, 과일의색상이라고 합시다 | 사과를 맞추기 위해서 10 ~ 13cm의 가로, 세로 최대길이와 빨간색의 과일을 탐색하게 되겠죠 | 위의 &#39;길이가 10 ~ 13cm인가? 아닌가?&#39;, &#39;색깔이 빨간색인가? 아닌가?&#39;의 기준이 경우를 쪼개나가는 기준이 됩니다. | . from sklearn.tree import DecisionTreeClassifier dt_model = DecisionTreeClassifier() . dt_model.fit(X_train, y_train) . DecisionTreeClassifier() . submission[&#39;Survived&#39;] = dt_model.predict(X_test) . submission.to_csv(&#39;dt_model.csv&#39;, index = False) . &#47784;&#45944; &#54617;&#49845; &#48143; &#44160;&#51613; . submission[&#39;Survived&#39;] = model.predict_proba(X_test)[:,1] . submission.to_csv(&#39;lr_proba.csv&#39;, index = False) . submission[&#39;Survived&#39;] = dt_model.predict_proba(X_test)[:,1] . submission.to_csv(&#39;dt_proba.csv&#39;, index = False) . DecisionTreeClassifier() . DecisionTreeClassifier() . dt_model_new = DecisionTreeClassifier(min_samples_split=10) . dt_model_new.fit(X_train, y_train) . DecisionTreeClassifier(min_samples_split=10) . submission[&#39;Survived&#39;] = dt_model_new.predict_proba(X_test)[:, 1] . submission.to_csv(&#39;dt_min_samples_10_proba.csv&#39;, index = False) . train . PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked . 0 1 | 0 | 3 | Braund, Mr. Owen Harris | 0 | 22.00000 | 1 | 0 | A/5 21171 | 7.2500 | NaN | S | . 1 2 | 1 | 1 | Cumings, Mrs. John Bradley (Florence Briggs Th... | 1 | 38.00000 | 1 | 0 | PC 17599 | 71.2833 | C85 | C | . 2 3 | 1 | 3 | Heikkinen, Miss. Laina | 1 | 26.00000 | 0 | 0 | STON/O2. 3101282 | 7.9250 | NaN | S | . 3 4 | 1 | 1 | Futrelle, Mrs. Jacques Heath (Lily May Peel) | 1 | 35.00000 | 1 | 0 | 113803 | 53.1000 | C123 | S | . 4 5 | 0 | 3 | Allen, Mr. William Henry | 0 | 35.00000 | 0 | 0 | 373450 | 8.0500 | NaN | S | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 886 887 | 0 | 2 | Montvila, Rev. Juozas | 0 | 27.00000 | 0 | 0 | 211536 | 13.0000 | NaN | S | . 887 888 | 1 | 1 | Graham, Miss. Margaret Edith | 1 | 19.00000 | 0 | 0 | 112053 | 30.0000 | B42 | S | . 888 889 | 0 | 3 | Johnston, Miss. Catherine Helen &quot;Carrie&quot; | 1 | 25.14062 | 1 | 2 | W./C. 6607 | 23.4500 | NaN | S | . 889 890 | 1 | 1 | Behr, Mr. Karl Howell | 0 | 26.00000 | 0 | 0 | 111369 | 30.0000 | C148 | C | . 890 891 | 0 | 3 | Dooley, Mr. Patrick | 0 | 32.00000 | 0 | 0 | 370376 | 7.7500 | NaN | Q | . 891 rows × 12 columns . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; . from sklearn.metrics import confusion_matrix, classification_report from sklearn.metrics import precision_score from sklearn.metrics import recall_score from sklearn.metrics import accuracy_score from sklearn.metrics import roc_auc_score . X_train = train[[&#39;Pclass&#39;, &#39;Age&#39;]] y_train = train[&#39;Survived&#39;] X_test = test[[&#39;Pclass&#39;, &#39;Age&#39;]] . model = LogisticRegression() model.fit(X_train, y_train) y_pred = model.predict(X_train) . y_true = y_train.values . cf_matrix = confusion_matrix(y_true, y_pred) . cf_matrix . array([[466, 83], [183, 159]]) . accuracy_score(y_true, y_pred) . 0.7014590347923682 . (cf_matrix[0,0] + cf_matrix[1,1]) / 891 . 0.7014590347923682 . precision_score(y_true, y_pred) . 0.6570247933884298 . (cf_matrix[1,1]) / (83 + 160) . 0.654320987654321 . recall_score(y_true, y_pred) . 0.4649122807017544 . (cf_matrix[1,1]) / (182 + 160) . 0.4649122807017544 . print(classification_report(y_true, y_pred)) . precision recall f1-score support 0 0.72 0.85 0.78 549 1 0.66 0.46 0.54 342 accuracy 0.70 891 macro avg 0.69 0.66 0.66 891 weighted avg 0.69 0.70 0.69 891 . 데이콘 채점 기준은 auc 라는 지표를 사용합니다. | auc값을 측정하기 위해서는, 예측을 확률값으로 해주어야 합니다. | 그 중에서 1에 속할 확률을 선택해주어야 합니다. | . roc_auc_score(y_true, y_pred) . 0.6568641549228261 . &#44208;&#44284; &#48143; &#44208;&#50616; . import pandas as pd from sklearn.tree import DecisionTreeClassifier from sklearn.model_selection import KFold from sklearn.metrics import roc_auc_score . model_10 = DecisionTreeClassifier(min_samples_split=10) model_20 = DecisionTreeClassifier(min_samples_split=20) model_30 = DecisionTreeClassifier(min_samples_split=30) . kfold = KFold(n_splits=5, shuffle=True, random_state=10) . score_10 = [] for trn_idx, val_idx in kfold.split(X_train): X_trn, y_trn = X_train.iloc[trn_idx, :], y_train.iloc[trn_idx] X_val, y_val = X_train.iloc[val_idx, :], y_train.iloc[val_idx] model_10.fit(X_trn, y_trn) y_pred = model_10.predict_proba(X_val)[:, 1] print(&#39;예측 완료&#39;) score_10.append(roc_auc_score(y_val.values, y_pred)) . 예측 완료 예측 완료 예측 완료 예측 완료 예측 완료 . score_10 . [0.666528811690102, 0.6923701298701299, 0.6616221174004193, 0.58994708994709, 0.7008647798742138] . score_20 = [] for trn_idx, val_idx in kfold.split(X_train): X_trn, y_trn = X_train.iloc[trn_idx, :], y_train.iloc[trn_idx] X_val, y_val = X_train.iloc[val_idx, :], y_train.iloc[val_idx] model_20.fit(X_trn, y_trn) y_pred = model_20.predict_proba(X_val)[:, 1] print(&#39;예측 완료&#39;) score_20.append(roc_auc_score(y_val.values, y_pred)) . 예측 완료 예측 완료 예측 완료 예측 완료 예측 완료 . score_20 . [0.6767300799558864, 0.696766774891775, 0.6660770440251572, 0.608531746031746, 0.7118055555555556] . score_30 = [] for trn_idx, val_idx in kfold.split(X_train): X_trn, y_trn = X_train.iloc[trn_idx, :], y_train.iloc[trn_idx] X_val, y_val = X_train.iloc[val_idx, :], y_train.iloc[val_idx] model_30.fit(X_trn, y_trn) y_pred = model_30.predict_proba(X_val)[:, 1] print(&#39;예측 완료&#39;) score_30.append(roc_auc_score(y_val.values, y_pred)) . 예측 완료 예측 완료 예측 완료 예측 완료 예측 완료 . score_30 . [0.6944444444444444, 0.7076569264069265, 0.6582154088050315, 0.6217592592592592, 0.7237945492662475] . import numpy as np . np.mean(score_10), np.mean(score_20), np.mean(score_30) . (0.662266585756391, 0.671982240092024, 0.6811741176363818) . 추가로 성별까지 변수에 추가하여 점수를 확인해보자. . X_train = train[[&#39;Pclass&#39;, &#39;Age&#39;, &#39;Sex&#39;]] y_train = train[&#39;Survived&#39;] X_test = test[[&#39;Pclass&#39;, &#39;Age&#39;, &#39;Sex&#39;]] . score_10 = [] for trn_idx, val_idx in kfold.split(X_train): X_trn, y_trn = X_train.iloc[trn_idx, :], y_train.iloc[trn_idx] X_val, y_val = X_train.iloc[val_idx, :], y_train.iloc[val_idx] model_10.fit(X_trn, y_trn) y_pred = model_10.predict_proba(X_val)[:, 1] print(&#39;예측 완료&#39;) score_10.append(roc_auc_score(y_val.values, y_pred)) . 예측 완료 예측 완료 예측 완료 예측 완료 예측 완료 . score_20 = [] for trn_idx, val_idx in kfold.split(X_train): X_trn, y_trn = X_train.iloc[trn_idx, :], y_train.iloc[trn_idx] X_val, y_val = X_train.iloc[val_idx, :], y_train.iloc[val_idx] model_20.fit(X_trn, y_trn) y_pred = model_20.predict_proba(X_val)[:, 1] print(&#39;예측 완료&#39;) score_20.append(roc_auc_score(y_val.values, y_pred)) . 예측 완료 예측 완료 예측 완료 예측 완료 예측 완료 . score_30 = [] for trn_idx, val_idx in kfold.split(X_train): X_trn, y_trn = X_train.iloc[trn_idx, :], y_train.iloc[trn_idx] X_val, y_val = X_train.iloc[val_idx, :], y_train.iloc[val_idx] model_30.fit(X_trn, y_trn) y_pred = model_30.predict_proba(X_val)[:, 1] print(&#39;예측 완료&#39;) score_30.append(roc_auc_score(y_val.values, y_pred)) . 예측 완료 예측 완료 예측 완료 예측 완료 예측 완료 . np.mean(score_10), np.mean(score_20), np.mean(score_30) . (0.8173547973745784, 0.8224093714579868, 0.8311338310733472) . from sklearn import tree . plt.figure( figsize=(20,15) ) tree.plot_tree(model_30, impurity=True, filled=True, rounded=True) . [Text(0.5275735294117647, 0.9615384615384616, &#39;X[2] &lt;= 0.5 ngini = 0.471 nsamples = 713 nvalue = [443, 270]&#39;), Text(0.2610294117647059, 0.8846153846153846, &#39;X[0] &lt;= 1.5 ngini = 0.31 nsamples = 465 nvalue = [376, 89]&#39;), Text(0.11764705882352941, 0.8076923076923077, &#39;X[1] &lt;= 36.5 ngini = 0.468 nsamples = 102 nvalue = [64, 38]&#39;), Text(0.058823529411764705, 0.7307692307692307, &#39;X[1] &lt;= 33.5 ngini = 0.484 nsamples = 34 nvalue = [14, 20]&#39;), Text(0.029411764705882353, 0.6538461538461539, &#39;gini = 0.499 nsamples = 25 nvalue = [13, 12]&#39;), Text(0.08823529411764706, 0.6538461538461539, &#39;gini = 0.198 nsamples = 9 nvalue = [1, 8]&#39;), Text(0.17647058823529413, 0.7307692307692307, &#39;X[1] &lt;= 60.5 ngini = 0.389 nsamples = 68 nvalue = [50, 18]&#39;), Text(0.14705882352941177, 0.6538461538461539, &#39;X[1] &lt;= 47.5 ngini = 0.42 nsamples = 60 nvalue = [42, 18]&#39;), Text(0.11764705882352941, 0.5769230769230769, &#39;X[1] &lt;= 45.25 ngini = 0.381 nsamples = 39 nvalue = [29, 10]&#39;), Text(0.08823529411764706, 0.5, &#39;X[1] &lt;= 41.0 ngini = 0.415 nsamples = 34 nvalue = [24, 10]&#39;), Text(0.058823529411764705, 0.4230769230769231, &#39;gini = 0.384 nsamples = 27 nvalue = [20, 7]&#39;), Text(0.11764705882352941, 0.4230769230769231, &#39;gini = 0.49 nsamples = 7 nvalue = [4, 3]&#39;), Text(0.14705882352941177, 0.5, &#39;gini = 0.0 nsamples = 5 nvalue = [5, 0]&#39;), Text(0.17647058823529413, 0.5769230769230769, &#39;gini = 0.472 nsamples = 21 nvalue = [13, 8]&#39;), Text(0.20588235294117646, 0.6538461538461539, &#39;gini = 0.0 nsamples = 8 nvalue = [8, 0]&#39;), Text(0.40441176470588236, 0.8076923076923077, &#39;X[1] &lt;= 3.5 ngini = 0.242 nsamples = 363 nvalue = [312, 51]&#39;), Text(0.375, 0.7307692307692307, &#39;gini = 0.444 nsamples = 15 nvalue = [5, 10]&#39;), Text(0.4338235294117647, 0.7307692307692307, &#39;X[1] &lt;= 32.5 ngini = 0.208 nsamples = 348 nvalue = [307, 41]&#39;), Text(0.27941176470588236, 0.6538461538461539, &#39;X[1] &lt;= 30.75 ngini = 0.239 nsamples = 260 nvalue = [224, 36]&#39;), Text(0.25, 0.5769230769230769, &#39;X[1] &lt;= 10.0 ngini = 0.215 nsamples = 245 nvalue = [215, 30]&#39;), Text(0.22058823529411764, 0.5, &#39;gini = 0.444 nsamples = 9 nvalue = [6, 3]&#39;), Text(0.27941176470588236, 0.5, &#39;X[1] &lt;= 25.57 ngini = 0.203 nsamples = 236 nvalue = [209, 27]&#39;), Text(0.17647058823529413, 0.4230769230769231, &#39;X[0] &lt;= 2.5 ngini = 0.177 nsamples = 183 nvalue = [165, 18]&#39;), Text(0.14705882352941177, 0.34615384615384615, &#39;gini = 0.095 nsamples = 20 nvalue = [19, 1]&#39;), Text(0.20588235294117646, 0.34615384615384615, &#39;X[1] &lt;= 19.5 ngini = 0.187 nsamples = 163 nvalue = [146, 17]&#39;), Text(0.14705882352941177, 0.2692307692307692, &#39;X[1] &lt;= 18.5 ngini = 0.117 nsamples = 32 nvalue = [30, 2]&#39;), Text(0.11764705882352941, 0.19230769230769232, &#39;gini = 0.159 nsamples = 23 nvalue = [21, 2]&#39;), Text(0.17647058823529413, 0.19230769230769232, &#39;gini = 0.0 nsamples = 9 nvalue = [9, 0]&#39;), Text(0.2647058823529412, 0.2692307692307692, &#39;X[1] &lt;= 20.25 ngini = 0.203 nsamples = 131 nvalue = [116, 15]&#39;), Text(0.23529411764705882, 0.19230769230769232, &#39;gini = 0.298 nsamples = 11 nvalue = [9, 2]&#39;), Text(0.29411764705882354, 0.19230769230769232, &#39;X[1] &lt;= 23.75 ngini = 0.193 nsamples = 120 nvalue = [107, 13]&#39;), Text(0.2647058823529412, 0.11538461538461539, &#39;gini = 0.147 nsamples = 25 nvalue = [23, 2]&#39;), Text(0.3235294117647059, 0.11538461538461539, &#39;X[1] &lt;= 25.07 ngini = 0.205 nsamples = 95 nvalue = [84, 11]&#39;), Text(0.29411764705882354, 0.038461538461538464, &#39;gini = 0.255 nsamples = 20 nvalue = [17, 3]&#39;), Text(0.35294117647058826, 0.038461538461538464, &#39;gini = 0.191 nsamples = 75 nvalue = [67, 8]&#39;), Text(0.38235294117647056, 0.4230769230769231, &#39;X[1] &lt;= 29.939 ngini = 0.282 nsamples = 53 nvalue = [44, 9]&#39;), Text(0.35294117647058826, 0.34615384615384615, &#39;X[0] &lt;= 2.5 ngini = 0.331 nsamples = 43 nvalue = [34, 9]&#39;), Text(0.3235294117647059, 0.2692307692307692, &#39;gini = 0.198 nsamples = 18 nvalue = [16, 2]&#39;), Text(0.38235294117647056, 0.2692307692307692, &#39;gini = 0.403 nsamples = 25 nvalue = [18, 7]&#39;), Text(0.4117647058823529, 0.34615384615384615, &#39;gini = 0.0 nsamples = 10 nvalue = [10, 0]&#39;), Text(0.3088235294117647, 0.5769230769230769, &#39;gini = 0.48 nsamples = 15 nvalue = [9, 6]&#39;), Text(0.5882352941176471, 0.6538461538461539, &#39;X[1] &lt;= 61.5 ngini = 0.107 nsamples = 88 nvalue = [83, 5]&#39;), Text(0.5588235294117647, 0.5769230769230769, &#39;X[1] &lt;= 44.5 ngini = 0.093 nsamples = 82 nvalue = [78, 4]&#39;), Text(0.5294117647058824, 0.5, &#39;X[1] &lt;= 38.5 ngini = 0.126 nsamples = 59 nvalue = [55, 4]&#39;), Text(0.5, 0.4230769230769231, &#39;X[0] &lt;= 2.5 ngini = 0.059 nsamples = 33 nvalue = [32, 1]&#39;), Text(0.47058823529411764, 0.34615384615384615, &#39;gini = 0.153 nsamples = 12 nvalue = [11, 1]&#39;), Text(0.5294117647058824, 0.34615384615384615, &#39;gini = 0.0 nsamples = 21 nvalue = [21, 0]&#39;), Text(0.5588235294117647, 0.4230769230769231, &#39;gini = 0.204 nsamples = 26 nvalue = [23, 3]&#39;), Text(0.5882352941176471, 0.5, &#39;gini = 0.0 nsamples = 23 nvalue = [23, 0]&#39;), Text(0.6176470588235294, 0.5769230769230769, &#39;gini = 0.278 nsamples = 6 nvalue = [5, 1]&#39;), Text(0.7941176470588235, 0.8846153846153846, &#39;X[0] &lt;= 2.5 ngini = 0.394 nsamples = 248 nvalue = [67, 181]&#39;), Text(0.7058823529411765, 0.8076923076923077, &#39;X[1] &lt;= 2.5 ngini = 0.087 nsamples = 131 nvalue = [6, 125]&#39;), Text(0.6764705882352942, 0.7307692307692307, &#39;gini = 0.0 nsamples = 1 nvalue = [1, 0]&#39;), Text(0.7352941176470589, 0.7307692307692307, &#39;X[1] &lt;= 43.5 ngini = 0.074 nsamples = 130 nvalue = [5, 125]&#39;), Text(0.7058823529411765, 0.6538461538461539, &#39;X[1] &lt;= 26.5 ngini = 0.037 nsamples = 106 nvalue = [2, 104]&#39;), Text(0.6764705882352942, 0.5769230769230769, &#39;X[1] &lt;= 25.5 ngini = 0.083 nsamples = 46 nvalue = [2, 44]&#39;), Text(0.6470588235294118, 0.5, &#39;X[1] &lt;= 23.5 ngini = 0.043 nsamples = 45 nvalue = [1, 44]&#39;), Text(0.6176470588235294, 0.4230769230769231, &#39;gini = 0.0 nsamples = 32 nvalue = [0, 32]&#39;), Text(0.6764705882352942, 0.4230769230769231, &#39;gini = 0.142 nsamples = 13 nvalue = [1, 12]&#39;), Text(0.7058823529411765, 0.5, &#39;gini = 0.0 nsamples = 1 nvalue = [1, 0]&#39;), Text(0.7352941176470589, 0.5769230769230769, &#39;gini = 0.0 nsamples = 60 nvalue = [0, 60]&#39;), Text(0.7647058823529411, 0.6538461538461539, &#39;gini = 0.219 nsamples = 24 nvalue = [3, 21]&#39;), Text(0.8823529411764706, 0.8076923076923077, &#39;X[1] &lt;= 27.5 ngini = 0.499 nsamples = 117 nvalue = [61, 56]&#39;), Text(0.8529411764705882, 0.7307692307692307, &#39;X[1] &lt;= 1.5 ngini = 0.497 nsamples = 93 nvalue = [43, 50]&#39;), Text(0.8235294117647058, 0.6538461538461539, &#39;gini = 0.0 nsamples = 4 nvalue = [0, 4]&#39;), Text(0.8823529411764706, 0.6538461538461539, &#39;X[1] &lt;= 21.5 ngini = 0.499 nsamples = 89 nvalue = [43, 46]&#39;), Text(0.8235294117647058, 0.5769230769230769, &#39;X[1] &lt;= 5.5 ngini = 0.473 nsamples = 39 nvalue = [24, 15]&#39;), Text(0.7941176470588235, 0.5, &#39;gini = 0.48 nsamples = 10 nvalue = [4, 6]&#39;), Text(0.8529411764705882, 0.5, &#39;gini = 0.428 nsamples = 29 nvalue = [20, 9]&#39;), Text(0.9411764705882353, 0.5769230769230769, &#39;X[1] &lt;= 26.5 ngini = 0.471 nsamples = 50 nvalue = [19, 31]&#39;), Text(0.9117647058823529, 0.5, &#39;X[1] &lt;= 25.07 ngini = 0.475 nsamples = 49 nvalue = [19, 30]&#39;), Text(0.8823529411764706, 0.4230769230769231, &#39;gini = 0.496 nsamples = 11 nvalue = [5, 6]&#39;), Text(0.9411764705882353, 0.4230769230769231, &#39;X[1] &lt;= 25.57 ngini = 0.465 nsamples = 38 nvalue = [14, 24]&#39;), Text(0.9117647058823529, 0.34615384615384615, &#39;gini = 0.467 nsamples = 35 nvalue = [13, 22]&#39;), Text(0.9705882352941176, 0.34615384615384615, &#39;gini = 0.444 nsamples = 3 nvalue = [1, 2]&#39;), Text(0.9705882352941176, 0.5, &#39;gini = 0.0 nsamples = 1 nvalue = [0, 1]&#39;), Text(0.9117647058823529, 0.7307692307692307, &#39;gini = 0.375 nsamples = 24 nvalue = [18, 6]&#39;)] . findfont: Font family [&#39;NanumBarunGothic&#39;] not found. Falling back to DejaVu Sans. . model_30 = DecisionTreeClassifier(min_samples_split=30) model_30.fit(X_train, y_train) Y_pred = model_30.predict(X_test) model_30.score(X_train, y_train) submission = pd.DataFrame({ &quot;PassengerId&quot;: test[&quot;PassengerId&quot;], &quot;Survived&quot;: Y_pred }) submission.to_csv(&#39;titanic.csv&#39;, index=False) . import os print(os.getcwd()) . /content .",
            "url": "https://shw9807.github.io/shw9807blog/%EB%8D%B0%EC%9D%B4%EC%BD%98/2022/01/20/%ED%83%80%EC%9D%B4%ED%83%80%EB%8B%89_%EB%8D%B0%EC%9D%B4%ED%84%B0_%EB%B6%84%EC%84%9D.html",
            "relUrl": "/%EB%8D%B0%EC%9D%B4%EC%BD%98/2022/01/20/%ED%83%80%EC%9D%B4%ED%83%80%EB%8B%89_%EB%8D%B0%EC%9D%B4%ED%84%B0_%EB%B6%84%EC%84%9D.html",
            "date": " • Jan 20, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "[Colab] 영화 관객수 예측모델",
            "content": "&#50689;&#54868; &#44288;&#44061;&#49688; &#50696;&#52769; &#47784;&#45944; . EDA를 위한 패키지 불러오기 . !sudo apt-get install -y fonts-nanum !sudo fc-cache -fv !rm ~/.cache/matplotlib -rf . Reading package lists... Done Building dependency tree Reading state information... Done The following NEW packages will be installed: fonts-nanum 0 upgraded, 1 newly installed, 0 to remove and 37 not upgraded. Need to get 9,604 kB of archives. After this operation, 29.5 MB of additional disk space will be used. Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 fonts-nanum all 20170925-1 [9,604 kB] Fetched 9,604 kB in 1s (8,421 kB/s) debconf: unable to initialize frontend: Dialog debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, &lt;&gt; line 1.) debconf: falling back to frontend: Readline debconf: unable to initialize frontend: Readline debconf: (This frontend requires a controlling tty.) debconf: falling back to frontend: Teletype dpkg-preconfigure: unable to re-open stdin: Selecting previously unselected package fonts-nanum. (Reading database ... 155229 files and directories currently installed.) Preparing to unpack .../fonts-nanum_20170925-1_all.deb ... Unpacking fonts-nanum (20170925-1) ... Setting up fonts-nanum (20170925-1) ... Processing triggers for fontconfig (2.12.6-0ubuntu2) ... /usr/share/fonts: caching, new cache contents: 0 fonts, 1 dirs /usr/share/fonts/truetype: caching, new cache contents: 0 fonts, 3 dirs /usr/share/fonts/truetype/humor-sans: caching, new cache contents: 1 fonts, 0 dirs /usr/share/fonts/truetype/liberation: caching, new cache contents: 16 fonts, 0 dirs /usr/share/fonts/truetype/nanum: caching, new cache contents: 10 fonts, 0 dirs /usr/local/share/fonts: caching, new cache contents: 0 fonts, 0 dirs /root/.local/share/fonts: skipping, no such directory /root/.fonts: skipping, no such directory /var/cache/fontconfig: cleaning cache directory /root/.cache/fontconfig: not cleaning non-existent cache directory /root/.fontconfig: not cleaning non-existent cache directory fc-cache: succeeded . import pandas as pd import seaborn as sns import matplotlib.pyplot as plt import lightgbm as lgb import warnings warnings.filterwarnings(&quot;ignore&quot;) # 필요한 패키지와 라이브러리를 가져옴 import matplotlib as mpl import matplotlib.pyplot as plt import matplotlib.font_manager as fm %matplotlib inline plt.rc(&#39;font&#39;, family=&#39;NanumBarunGothic&#39;) mpl.rcParams[&#39;axes.unicode_minus&#39;] = False . import numpy as np . movies_test = pd.read_csv(&quot;/content/drive/MyDrive/Colab Notebooks/영화관객수/movies_test.csv&quot;) movies_train = pd.read_csv(&quot;/content/drive/MyDrive/Colab Notebooks/영화관객수/movies_train.csv&quot;) . submission = pd.read_csv(&quot;/content/drive/MyDrive/Colab Notebooks/영화관객수/submission.csv&quot;) . movies_train.head() . title distributor genre release_time time screening_rat director dir_prev_bfnum dir_prev_num num_staff num_actor box_off_num . 0 개들의 전쟁 | 롯데엔터테인먼트 | 액션 | 2012-11-22 | 96 | 청소년 관람불가 | 조병옥 | NaN | 0 | 91 | 2 | 23398 | . 1 내부자들 | (주)쇼박스 | 느와르 | 2015-11-19 | 130 | 청소년 관람불가 | 우민호 | 1161602.50 | 2 | 387 | 3 | 7072501 | . 2 은밀하게 위대하게 | (주)쇼박스 | 액션 | 2013-06-05 | 123 | 15세 관람가 | 장철수 | 220775.25 | 4 | 343 | 4 | 6959083 | . 3 나는 공무원이다 | (주)NEW | 코미디 | 2012-07-12 | 101 | 전체 관람가 | 구자홍 | 23894.00 | 2 | 20 | 6 | 217866 | . 4 불량남녀 | 쇼박스(주)미디어플렉스 | 코미디 | 2010-11-04 | 108 | 15세 관람가 | 신근호 | 1.00 | 1 | 251 | 2 | 483387 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; title : 영화의 제목 distributor : 배급사 genre : 장르 release_time : 개봉일 time : 상영시간(분) screening_rat : 상영등급 director : 감독이름 dir_prev_bfnum : 해당 감독이 이 영화를 만들기 전 제작에 참여한 영화에서의 평균 관객수(단 관객수가 알려지지 않은 영화 제외) dir_prev_num : 해당 감독이 이 영화를 만들기 전 제작에 참여한 영화의 개수(단 관객수가 알려지지 않은 영화 제외) num_staff : 스텝수 num_actor : 주연배우수 box_off_num : 관객수 . movies_train.isnull().any() . title False distributor False genre False release_time False time False screening_rat False director False dir_prev_bfnum True dir_prev_num False num_staff False num_actor False box_off_num False dtype: bool . dir_prev_bfnum 항목에 결측치가 존재함을 알 수 있다. -&gt; 해당 영화가 그 감독의 첫 영화인 경우에 결측치를 가질 것이다. . movies_train[movies_train[&#39;dir_prev_bfnum&#39;].isnull() == True].head() . title distributor genre release_time time screening_rat director dir_prev_bfnum dir_prev_num num_staff num_actor box_off_num . 0 개들의 전쟁 | 롯데엔터테인먼트 | 액션 | 2012-11-22 | 96 | 청소년 관람불가 | 조병옥 | NaN | 0 | 91 | 2 | 23398 | . 6 길위에서 | 백두대간 | 다큐멘터리 | 2013-05-23 | 104 | 전체 관람가 | 이창재 | NaN | 0 | 32 | 5 | 53526 | . 8 1789, 바스티유의 연인들 | 유니버설픽쳐스인터내셔널코리아 | 뮤지컬 | 2014-09-18 | 129 | 전체 관람가 | 정성복 | NaN | 0 | 3 | 5 | 4778 | . 9 청춘그루브 | (주)두타연 | 드라마 | 2012-03-15 | 94 | 15세 관람가 | 변성현 | NaN | 0 | 138 | 3 | 868 | . 10 AV 아이돌 | (주) 케이알씨지 | 멜로/로맨스 | 2015-07-27 | 89 | 청소년 관람불가 | 조조 히데오 | NaN | 0 | 0 | 4 | 745 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; movies_train = movies_train.fillna(0) . movies_test = movies_test.fillna(0) . 결측치를 0으로 바꾸어 주었다. . movies_train.isnull().any() . title False distributor False genre False release_time False time False screening_rat False director False dir_prev_bfnum False dir_prev_num False num_staff False num_actor False box_off_num False dtype: bool . movies_test.isnull().any() . title False distributor False genre False release_time False time False screening_rat False director False dir_prev_bfnum False dir_prev_num False num_staff False num_actor False dtype: bool . 결측치가 없어진 것을 확인할 수 있다. . movies_train[&#39;month&#39;] = movies_train[&#39;release_time&#39;].str[5:7].astype(&#39;int&#39;) . plt.figure(figsize=[10,10]) sns.scatterplot(data=movies_train, x=&#39;month&#39;, y = &#39;box_off_num&#39;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fd8789c0510&gt; . 방학시즌에 개봉한 영화가 대체로 관객 수가 많은 것처럼 보인다. . plt.figure(figsize=[10,10]) sns.scatterplot(data=movies_train, x=&#39;num_staff&#39;, y = &#39;box_off_num&#39;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fd87898e490&gt; . 스태프 수가 많은 영화가 대체로 관객 수가 많은 것처럼 보인다. . plt.figure(figsize=[10,10]) sns.scatterplot(data=movies_train, x=&#39;screening_rat&#39;, y = &#39;box_off_num&#39;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fd8789735d0&gt; . 전체 관람가나 청소년 관람불가 영화보다는 12세관람가나 15세관람가 영화가 관객수가 많은 편인 것을 알 수 있다. . plt.figure(figsize=[10,10]) sns.scatterplot(data=movies_train, x=&#39;time&#39;, y = &#39;box_off_num&#39;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fd878917710&gt; . 상영시간이 지나치게 짧거나 2시간30분을 넘어가면 관객수가 대체로 적다는 것을 확인할 수 있다. . movies_train[[&#39;genre&#39;, &#39;box_off_num&#39;]].groupby(&#39;genre&#39;).mean().sort_values(&#39;box_off_num&#39;) . box_off_num . genre . 뮤지컬 6.627000e+03 | . 다큐멘터리 6.717226e+04 | . 서스펜스 8.261100e+04 | . 애니메이션 1.819267e+05 | . 멜로/로맨스 4.259680e+05 | . 미스터리 5.275482e+05 | . 공포 5.908325e+05 | . 드라마 6.256898e+05 | . 코미디 1.193914e+06 | . SF 1.788346e+06 | . 액션 2.203974e+06 | . 느와르 2.263695e+06 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; 장르별로 평균 관객수를 정렬해보았다. 액션과 느와르 장르가 대체적으로 관객수가 많은 것을 확인할 수 있다. . %matplotlib inline movies_train.hist(bins = 50, figsize = (20,15)) plt.show() . 변수별로 히스토그램을 그려보았다. . sns.heatmap(movies_train.corr(), annot = True) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fd86f758e50&gt; . 히트맵그림을 그려보았다.관객수와 상관관계가 있어보이는 변수는 스태프의 수와 상영시간 정도로 보인다. . &#48320;&#49688; &#49440;&#53469;&#44284; &#47784;&#45944; &#44396;&#52629; . lightGBM (base model) . model = lgb.LGBMRegressor(n_estimators=1000) . features = [&#39;time&#39;, &#39;dir_prev_num&#39;, &#39;num_staff&#39;, &#39;num_actor&#39;] target = [&#39;box_off_num&#39;] . X_train, X_test, y_train = movies_train[features], movies_test[features], movies_train[target] . model.fit(X_train, y_train) . LGBMRegressor(n_estimators=1000) . singleLGBM = submission.copy() . singleLGBM[&#39;box_off_num&#39;] = model.predict(X_test) . singleLGBM.head() . title box_off_num . 0 용서는 없다 | 2.817995e+06 | . 1 아빠가 여자를 좋아해 | 3.753772e+05 | . 2 하모니 | -5.693243e+05 | . 3 의형제 | 1.581189e+06 | . 4 평행 이론 | -5.277806e+05 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; 음수가 나오므로 잘못된 것 같다. . RMSE 점수를 확인하기 위해서 train 데이터로 학습시킨 모델을 다시 train 데이터에 적용하여 관객수를 구하고 RMSE를 구해보자. 그리고 변수는 어느정도 상관계수가 커보였던 상영시간과 스태프 수 만을 사용하기로 하자. . features = [&#39;time&#39;, &#39;num_staff&#39;] target = [&#39;box_off_num&#39;] . X_train, X_test, y_train = movies_train[features], movies_train[features], movies_train[target] . model.fit(X_train, y_train) . LGBMRegressor(n_estimators=1000) . singleLGBM = movies_train.copy() . singleLGBM[&#39;box_off_num&#39;] = model.predict(X_test) . singleLGBM.head() . title distributor genre release_time time screening_rat director dir_prev_bfnum dir_prev_num num_staff num_actor box_off_num month . 0 개들의 전쟁 | 롯데엔터테인먼트 | 액션 | 2012-11-22 | 96 | 청소년 관람불가 | 조병옥 | 0.00 | 0 | 91 | 2 | 4.245526e+03 | 11 | . 1 내부자들 | (주)쇼박스 | 느와르 | 2015-11-19 | 130 | 청소년 관람불가 | 우민호 | 1161602.50 | 2 | 387 | 3 | 6.805784e+06 | 11 | . 2 은밀하게 위대하게 | (주)쇼박스 | 액션 | 2013-06-05 | 123 | 15세 관람가 | 장철수 | 220775.25 | 4 | 343 | 4 | 6.577795e+06 | 6 | . 3 나는 공무원이다 | (주)NEW | 코미디 | 2012-07-12 | 101 | 전체 관람가 | 구자홍 | 23894.00 | 2 | 20 | 6 | 4.453083e+04 | 7 | . 4 불량남녀 | 쇼박스(주)미디어플렉스 | 코미디 | 2010-11-04 | 108 | 15세 관람가 | 신근호 | 1.00 | 1 | 251 | 2 | 8.484480e+05 | 11 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; def RMSE(true, pred): score = np.sqrt(np.mean(np.square(true-pred))) return score . RMSE(movies_train[&#39;box_off_num&#39;],singleLGBM[&#39;box_off_num&#39;]) . 776662.1566623428 . RMSE가 776,662 정도 나오는 것을 확인할 수 있다. 다른 모델도 사용해서 RMSE를 더 줄일 수 있는지 확인해보자. . from sklearn.model_selection import KFold . k_fold = KFold(n_splits=5, shuffle=True) . model = lgb.LGBMRegressor(n_estimators=1000) models = [] # 모델을 담을 바구니라 생각하자. for train_idx, val_idx in k_fold.split(X_train): x_t = X_train.iloc[train_idx] y_t = y_train.iloc[train_idx] x_val = X_train.iloc[val_idx] y_val = y_train.iloc[val_idx] models.append(model.fit(x_t, y_t, eval_set=(x_val, y_val), early_stopping_rounds=100, verbose = 100)) . Training until validation scores don&#39;t improve for 100 rounds. [100] valid_0&#39;s l2: 1.85132e+12 Early stopping, best iteration is: [7] valid_0&#39;s l2: 1.4462e+12 Training until validation scores don&#39;t improve for 100 rounds. [100] valid_0&#39;s l2: 3.34081e+12 Early stopping, best iteration is: [14] valid_0&#39;s l2: 2.88132e+12 Training until validation scores don&#39;t improve for 100 rounds. [100] valid_0&#39;s l2: 1.97832e+12 Early stopping, best iteration is: [23] valid_0&#39;s l2: 1.8693e+12 Training until validation scores don&#39;t improve for 100 rounds. [100] valid_0&#39;s l2: 1.72764e+12 Early stopping, best iteration is: [29] valid_0&#39;s l2: 1.60885e+12 Training until validation scores don&#39;t improve for 100 rounds. [100] valid_0&#39;s l2: 3.40884e+12 Early stopping, best iteration is: [26] valid_0&#39;s l2: 3.16161e+12 . models . [LGBMRegressor(n_estimators=1000), LGBMRegressor(n_estimators=1000), LGBMRegressor(n_estimators=1000), LGBMRegressor(n_estimators=1000), LGBMRegressor(n_estimators=1000)] . preds = [] for model in models: preds.append(model.predict(X_train)) len(preds) . 5 . pred에 만들어둔 모델들의 예측값들을 저장해준다. . kfoldLightGBM = movies_train.copy() # 답안지 복사. . kfoldLightGBM[&#39;box_off_num&#39;] = np.mean(preds, axis = 0) # mean 평균값함수,axis 축 . RMSE(movies_train[&#39;box_off_num&#39;],kfoldLightGBM[&#39;box_off_num&#39;]) . 1375351.354121486 . RMSE가 1,375,351로 매우 커졌다. 왜일까....?? . from sklearn import preprocessing le = preprocessing.LabelEncoder() movies_train[&#39;genre&#39;] = le.fit_transform(movies_train[&#39;genre&#39;]) . sklearn에서 제공하는 labelEncoder를 활용해서 문자열을 숫자로 변환했다. . features = [&#39;time&#39;, &#39;num_staff&#39;, &#39;dir_prev_bfnum&#39;, &#39;genre&#39;] . features에 전처리된 dir_prev_bfnum와 문자열을 숫자로 변환한 genre를 추가했다. . X_train, X_test, y_train = movies_train[features], movies_train[features], movies_train[target] . model = lgb.LGBMRegressor(random_state=777, n_estimators=1000) models = [] for train_idx, val_idx in k_fold.split(X_train): x_t = X_train.iloc[train_idx] y_t = y_train.iloc[train_idx] x_val = X_train.iloc[val_idx] y_val = y_train.iloc[val_idx] models.append(model.fit(x_t, y_t, eval_set=(x_val, y_val), early_stopping_rounds=100, verbose = 100)) . Training until validation scores don&#39;t improve for 100 rounds. [100] valid_0&#39;s l2: 2.0569e+12 Early stopping, best iteration is: [15] valid_0&#39;s l2: 1.87889e+12 Training until validation scores don&#39;t improve for 100 rounds. [100] valid_0&#39;s l2: 1.78541e+12 Early stopping, best iteration is: [9] valid_0&#39;s l2: 1.29506e+12 Training until validation scores don&#39;t improve for 100 rounds. [100] valid_0&#39;s l2: 2.30124e+12 Early stopping, best iteration is: [33] valid_0&#39;s l2: 2.12538e+12 Training until validation scores don&#39;t improve for 100 rounds. [100] valid_0&#39;s l2: 4.67408e+12 Early stopping, best iteration is: [22] valid_0&#39;s l2: 4.06655e+12 Training until validation scores don&#39;t improve for 100 rounds. [100] valid_0&#39;s l2: 2.30679e+12 Early stopping, best iteration is: [17] valid_0&#39;s l2: 1.76818e+12 . preds = [] for model in models: preds.append(model.predict(X_test)) len(preds) . 5 . feLightGBM = movies_train.copy() . feLightGBM[&#39;box_off_num&#39;] = np.mean(preds, axis = 0) . RMSE(movies_train[&#39;box_off_num&#39;],feLightGBM[&#39;box_off_num&#39;]) . 1360591.313400032 . 바로 위의 RMSE보다는 어느 정도 작아진 것을 알 수 있다. . Grid search &#47784;&#45944; . from sklearn.model_selection import GridSearchCV . model = lgb.LGBMRegressor(n_estimators=1000) . params = { &#39;learning_rate&#39;: [0.1, 0.01, 0.003], &#39;min_child_samples&#39;: [20, 30]} gs = GridSearchCV(estimator=model, param_grid=params, scoring= &#39;neg_mean_squared_error&#39;, cv = k_fold) . params에서 learning_rate는 모델링을 하는 간격으로, 값이 적을수록 점점 더 미세하게 모델의 변화가 이루어진다로 생각하면 된다. scoring을 rmse로 한 이후는 현재 이 대회의 평가지표가 rmse값이기 때문 . gs.fit(X_train, y_train) . GridSearchCV(cv=KFold(n_splits=5, random_state=None, shuffle=True), estimator=LGBMRegressor(n_estimators=1000), param_grid={&#39;learning_rate&#39;: [0.1, 0.01, 0.003], &#39;min_child_samples&#39;: [20, 30]}, scoring=&#39;neg_mean_squared_error&#39;) . gs.best_params_ . {&#39;learning_rate&#39;: 0.003, &#39;min_child_samples&#39;: 30} . model = lgb.LGBMRegressor(n_estimators=1000, learning_rate=0.003, min_child_samples=30) models = [] for train_idx, val_idx in k_fold.split(X_train): x_t = X_train.iloc[train_idx] y_t = y_train.iloc[train_idx] x_val = X_train.iloc[val_idx] y_val = y_train.iloc[val_idx] models.append(model.fit(x_t, y_t, eval_set=(x_val, y_val), early_stopping_rounds=100, verbose = 100)) . Training until validation scores don&#39;t improve for 100 rounds. [100] valid_0&#39;s l2: 4.88516e+12 [200] valid_0&#39;s l2: 4.42603e+12 [300] valid_0&#39;s l2: 4.13354e+12 [400] valid_0&#39;s l2: 3.97867e+12 [500] valid_0&#39;s l2: 3.87486e+12 [600] valid_0&#39;s l2: 3.80364e+12 [700] valid_0&#39;s l2: 3.75367e+12 [800] valid_0&#39;s l2: 3.73486e+12 [900] valid_0&#39;s l2: 3.73187e+12 Early stopping, best iteration is: [889] valid_0&#39;s l2: 3.73167e+12 Training until validation scores don&#39;t improve for 100 rounds. [100] valid_0&#39;s l2: 2.18551e+12 [200] valid_0&#39;s l2: 1.88159e+12 [300] valid_0&#39;s l2: 1.71548e+12 [400] valid_0&#39;s l2: 1.6308e+12 [500] valid_0&#39;s l2: 1.60349e+12 [600] valid_0&#39;s l2: 1.59885e+12 [700] valid_0&#39;s l2: 1.59966e+12 Early stopping, best iteration is: [628] valid_0&#39;s l2: 1.59791e+12 Training until validation scores don&#39;t improve for 100 rounds. [100] valid_0&#39;s l2: 1.63836e+12 [200] valid_0&#39;s l2: 1.49624e+12 [300] valid_0&#39;s l2: 1.37358e+12 [400] valid_0&#39;s l2: 1.28814e+12 [500] valid_0&#39;s l2: 1.23785e+12 [600] valid_0&#39;s l2: 1.2091e+12 [700] valid_0&#39;s l2: 1.18886e+12 [800] valid_0&#39;s l2: 1.17838e+12 [900] valid_0&#39;s l2: 1.15994e+12 [1000] valid_0&#39;s l2: 1.14697e+12 Did not meet early stopping. Best iteration is: [1000] valid_0&#39;s l2: 1.14697e+12 Training until validation scores don&#39;t improve for 100 rounds. [100] valid_0&#39;s l2: 1.54254e+12 [200] valid_0&#39;s l2: 1.32956e+12 [300] valid_0&#39;s l2: 1.26545e+12 [400] valid_0&#39;s l2: 1.26595e+12 Early stopping, best iteration is: [335] valid_0&#39;s l2: 1.2583e+12 Training until validation scores don&#39;t improve for 100 rounds. [100] valid_0&#39;s l2: 3.88743e+12 [200] valid_0&#39;s l2: 3.52199e+12 [300] valid_0&#39;s l2: 3.32668e+12 [400] valid_0&#39;s l2: 3.26764e+12 [500] valid_0&#39;s l2: 3.22996e+12 [600] valid_0&#39;s l2: 3.2196e+12 [700] valid_0&#39;s l2: 3.21357e+12 [800] valid_0&#39;s l2: 3.21046e+12 [900] valid_0&#39;s l2: 3.20925e+12 [1000] valid_0&#39;s l2: 3.2054e+12 Did not meet early stopping. Best iteration is: [999] valid_0&#39;s l2: 3.20518e+12 . preds = [] for model in models: preds.append(model.predict(X_test)) . gslgbm = movies_train.copy() . gslgbm[&#39;box_off_num&#39;] = np.mean(preds, axis=0) . RMSE(movies_train[&#39;box_off_num&#39;],gslgbm[&#39;box_off_num&#39;]) . 1371034.705755213 . 위보다는 조금 더 작아졌는데 왜이렇게 클까....?? . train 데이터를 train 과 test로 나눠서 확인을 했어야 하는데 train 데이터 전체로 모델을 만든 뒤 train데이터 전체를 사용해서 예측을 해서 이상해진 것 같다....?? . train 데이터로 모델을 만들고 정확히 같은 데이터를 예측하게 했는데 RMSE가 왜 저렇게 크게 나오는 걸까 .",
            "url": "https://shw9807.github.io/shw9807blog/%EB%8D%B0%EC%9D%B4%EC%BD%98/2022/01/13/%EC%98%81%ED%99%94%EA%B4%80%EA%B0%9D%EC%88%98_%EC%98%88%EC%B8%A1_%EB%AA%A8%EB%8D%B8.html",
            "relUrl": "/%EB%8D%B0%EC%9D%B4%EC%BD%98/2022/01/13/%EC%98%81%ED%99%94%EA%B4%80%EA%B0%9D%EC%88%98_%EC%98%88%EC%B8%A1_%EB%AA%A8%EB%8D%B8.html",
            "date": " • Jan 13, 2022"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "about",
          "content": "2022년부터 ML/DL 공부중 .",
          "url": "https://shw9807.github.io/shw9807blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://shw9807.github.io/shw9807blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}